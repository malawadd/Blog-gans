{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN-GP Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0003'\n",
    "DATA_NAME = 'celeb'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_celeb(data_name, image_size, batch_size):\n",
    "    data_folder = os.path.join(\"./data\", data_name)\n",
    "\n",
    "    data_gen = ImageDataGenerator(preprocessing_function=lambda x: (x.astype('float32') - 127.5) / 127.5)\n",
    "\n",
    "    x_train = data_gen.flow_from_directory(data_folder\n",
    "                                            , target_size = (image_size,image_size)\n",
    "                                            , batch_size = batch_size\n",
    "                                            , shuffle = True\n",
    "                                            , class_mode = 'input'\n",
    "                                            , subset = \"training\"\n",
    "                                                )\n",
    "\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_celeb(DATA_NAME, IMAGE_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((x_train[0][0][0]+1)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def grad(y, x):\n",
    "    V = Lambda(lambda z: K.gradients(\n",
    "        z[0], z[1]), output_shape=[1])([y, x])\n",
    "    return V\n",
    "\n",
    "class RandomWeightedAverage(Layer):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def call(self, inputs):\n",
    "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class WGANGP():\n",
    "    def __init__(self\n",
    "        , input_dim\n",
    "        , critic_conv_filters\n",
    "        , critic_conv_kernel_size\n",
    "        , critic_conv_strides\n",
    "        , critic_batch_norm_momentum\n",
    "        , critic_activation\n",
    "        , critic_dropout_rate\n",
    "        , critic_learning_rate\n",
    "        , generator_initial_dense_layer_size\n",
    "        , generator_upsample\n",
    "        , generator_conv_filters\n",
    "        , generator_conv_kernel_size\n",
    "        , generator_conv_strides\n",
    "        , generator_batch_norm_momentum\n",
    "        , generator_activation\n",
    "        , generator_dropout_rate\n",
    "        , generator_learning_rate\n",
    "        , optimiser\n",
    "        , grad_weight\n",
    "        , z_dim\n",
    "        , batch_size\n",
    "        ):\n",
    "\n",
    "        self.name = 'gan'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.critic_conv_filters = critic_conv_filters\n",
    "        self.critic_conv_kernel_size = critic_conv_kernel_size\n",
    "        self.critic_conv_strides = critic_conv_strides\n",
    "        self.critic_batch_norm_momentum = critic_batch_norm_momentum\n",
    "        self.critic_activation = critic_activation\n",
    "        self.critic_dropout_rate = critic_dropout_rate\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "\n",
    "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
    "        self.generator_upsample = generator_upsample\n",
    "        self.generator_conv_filters = generator_conv_filters\n",
    "        self.generator_conv_kernel_size = generator_conv_kernel_size\n",
    "        self.generator_conv_strides = generator_conv_strides\n",
    "        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n",
    "        self.generator_activation = generator_activation\n",
    "        self.generator_dropout_rate = generator_dropout_rate\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "        \n",
    "        self.optimiser = optimiser\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.n_layers_critic = len(critic_conv_filters)\n",
    "        self.n_layers_generator = len(generator_conv_filters)\n",
    "\n",
    "        self.weight_init = RandomNormal(mean=0., stddev=0.02) # 'he_normal' #RandomNormal(mean=0., stddev=0.02)\n",
    "        self.grad_weight = grad_weight\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        self.epoch = 0\n",
    "\n",
    "        self._build_critic()\n",
    "        self._build_generator()\n",
    "\n",
    "        self._build_adversarial()\n",
    "\n",
    "            \n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, interpolated_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = grad(y_pred, interpolated_samples)[0]\n",
    "\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "    def wasserstein(self, y_true, y_pred):\n",
    "        return -K.mean(y_true * y_pred)\n",
    "\n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'leaky_relu':\n",
    "            layer = LeakyReLU(alpha = 0.2)\n",
    "        else:\n",
    "            layer = Activation(activation)\n",
    "        return layer\n",
    "\n",
    "    def _build_critic(self):\n",
    "\n",
    "        ### THE critic\n",
    "        critic_input = Input(shape=self.input_dim, name='critic_input')\n",
    "\n",
    "        x = critic_input\n",
    "\n",
    "        for i in range(self.n_layers_critic):\n",
    "\n",
    "            x = Conv2D(\n",
    "                filters = self.critic_conv_filters[i]\n",
    "                , kernel_size = self.critic_conv_kernel_size[i]\n",
    "                , strides = self.critic_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'critic_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "\n",
    "            if self.critic_batch_norm_momentum and i > 0:\n",
    "                x = BatchNormalization(momentum = self.critic_batch_norm_momentum)(x)\n",
    "\n",
    "            x = self.get_activation(self.critic_activation)(x)\n",
    "\n",
    "            if self.critic_dropout_rate:\n",
    "                x = Dropout(rate = self.critic_dropout_rate)(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        # x = Dense(512, kernel_initializer = self.weight_init)(x)\n",
    "\n",
    "        # x = self.get_activation(self.critic_activation)(x)\n",
    "        \n",
    "        critic_output = Dense(1, activation=None\n",
    "        , kernel_initializer = self.weight_init\n",
    "        )(x)\n",
    "\n",
    "        self.critic = Model(critic_input, critic_output)\n",
    "\n",
    "    def _build_generator(self):\n",
    "\n",
    "        ### THE generator\n",
    "\n",
    "        generator_input = Input(shape=(self.z_dim,), name='generator_input')\n",
    "\n",
    "        x = generator_input\n",
    "\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
    "        if self.generator_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "        \n",
    "        x = self.get_activation(self.generator_activation)(x)\n",
    "\n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "\n",
    "        if self.generator_dropout_rate:\n",
    "            x = Dropout(rate = self.generator_dropout_rate)(x)\n",
    "\n",
    "        for i in range(self.n_layers_generator):\n",
    "\n",
    "            if self.generator_upsample[i] == 2:\n",
    "                x = UpSampling2D()(x)\n",
    "                x = Conv2D(\n",
    "                filters = self.generator_conv_filters[i]\n",
    "                , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'generator_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "            else:\n",
    "\n",
    "                x = Conv2DTranspose(\n",
    "                    filters = self.generator_conv_filters[i]\n",
    "                    , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                    , padding = 'same'\n",
    "                    , strides = self.generator_conv_strides[i]\n",
    "                    , name = 'generator_conv_' + str(i)\n",
    "                    , kernel_initializer = self.weight_init\n",
    "                    )(x)\n",
    "\n",
    "            if i < self.n_layers_generator - 1:\n",
    "\n",
    "                if self.generator_batch_norm_momentum:\n",
    "                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "\n",
    "                x = self.get_activation(self.generator_activation)(x)\n",
    "                \n",
    "            else:\n",
    "                x = Activation('tanh')(x)\n",
    "\n",
    "        generator_output = x\n",
    "        self.generator = Model(generator_input, generator_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_opti(self, lr):\n",
    "        if self.optimiser == 'adam':\n",
    "            opti = Adam(lr=lr, beta_1=0.5)\n",
    "        elif self.optimiser == 'rmsprop':\n",
    "            opti = RMSprop(lr=lr)\n",
    "        else:\n",
    "            opti = Adam(lr=lr)\n",
    "\n",
    "        return opti\n",
    "\n",
    "\n",
    "    def set_trainable(self, m, val):\n",
    "        m.trainable = val\n",
    "        for l in m.layers:\n",
    "            l.trainable = val\n",
    "\n",
    "    def _build_adversarial(self):\n",
    "                \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.set_trainable(self.generator, False)\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.input_dim)\n",
    "\n",
    "        # Fake image\n",
    "        z_disc = Input(shape=(self.z_dim,))\n",
    "        fake_img = self.generator(z_disc)\n",
    "\n",
    "        # critic determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage(self.batch_size)([real_img, fake_img])\n",
    "        \n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'interpolated_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          interpolated_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, z_disc],\n",
    "                            outputs=[valid, fake, validity_interpolated])\n",
    "\n",
    "        self.critic_model.compile(\n",
    "            loss=[self.wasserstein,self.wasserstein, partial_gp_loss]\n",
    "            ,optimizer=self.get_opti(self.critic_learning_rate)\n",
    "            ,loss_weights=[1, 1, self.grad_weight]\n",
    "            )\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.set_trainable(self.critic, False)\n",
    "        self.set_trainable(self.generator, True)\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        model_input = Input(shape=(self.z_dim,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator(model_input)\n",
    "        # Discriminator determines validity\n",
    "        model_output = self.critic(img)\n",
    "        # Defines generator model\n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "        self.model.compile(optimizer=self.get_opti(self.generator_learning_rate)\n",
    "        , loss=self.wasserstein\n",
    "        )\n",
    "\n",
    "        self.set_trainable(self.critic, True)\n",
    "\n",
    "    def train_critic(self, x_train, batch_size, using_generator):\n",
    "\n",
    "        valid = np.ones((batch_size,1), dtype=np.float32)\n",
    "        fake = -np.ones((batch_size,1), dtype=np.float32)\n",
    "        dummy = np.zeros((batch_size, 1), dtype=np.float32) # Dummy gt for gradient penalty\n",
    "\n",
    "        if using_generator:\n",
    "            true_imgs = next(x_train)[0]\n",
    "            if true_imgs.shape[0] != batch_size:\n",
    "                true_imgs = next(x_train)[0]\n",
    "        else:\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            true_imgs = x_train[idx]\n",
    "    \n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "\n",
    "        d_loss = self.critic_model.train_on_batch([true_imgs, noise], [valid, fake, dummy])\n",
    "        return d_loss\n",
    "\n",
    "    def train_generator(self, batch_size):\n",
    "        valid = np.ones((batch_size,1), dtype=np.float32)\n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        return self.model.train_on_batch(noise, valid)\n",
    "\n",
    "\n",
    "    def train(self, x_train, batch_size, epochs, run_folder, print_every_n_batches = 10\n",
    "    , n_critic = 5\n",
    "    , using_generator = False):\n",
    "\n",
    "        for epoch in range(self.epoch, self.epoch + epochs):\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                critic_loops = 5\n",
    "            else:\n",
    "                critic_loops = n_critic\n",
    "\n",
    "            for _ in range(critic_loops):\n",
    "                d_loss = self.train_critic(x_train, batch_size, using_generator)\n",
    "\n",
    "            g_loss = self.train_generator(batch_size)\n",
    "\n",
    "            \n",
    "            print (\"%d (%d, %d) [D loss: (%.1f)(R %.1f, F %.1f, G %.1f)] [G loss: %.1f]\" % (epoch, critic_loops, 1, d_loss[0], d_loss[1],d_loss[2],d_loss[3],g_loss))\n",
    "            \n",
    "\n",
    "\n",
    "            self.d_losses.append(d_loss)\n",
    "            self.g_losses.append(g_loss)\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % print_every_n_batches == 0:\n",
    "                self.sample_images(run_folder)\n",
    "                self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n",
    "                self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
    "                self.save_model(run_folder)\n",
    "                \n",
    "\n",
    "            self.epoch+=1\n",
    "\n",
    "\n",
    "    def sample_images(self, run_folder):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        #Rescale images 0 - 1\n",
    "\n",
    "        gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "        cnt = 0\n",
    "\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray_r')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.critic, to_file=os.path.join(run_folder ,'viz/critic.png'), show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.generator, to_file=os.path.join(run_folder ,'viz/generator.png'), show_shapes = True, show_layer_names = True)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    def save(self, folder):\n",
    "\n",
    "            with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "                pickle.dump([\n",
    "                    self.input_dim\n",
    "                    , self.critic_conv_filters\n",
    "                    , self.critic_conv_kernel_size\n",
    "                    , self.critic_conv_strides\n",
    "                    , self.critic_batch_norm_momentum\n",
    "                    , self.critic_activation\n",
    "                    , self.critic_dropout_rate\n",
    "                    , self.critic_learning_rate\n",
    "                    , self.generator_initial_dense_layer_size\n",
    "                    , self.generator_upsample\n",
    "                    , self.generator_conv_filters\n",
    "                    , self.generator_conv_kernel_size\n",
    "                    , self.generator_conv_strides\n",
    "                    , self.generator_batch_norm_momentum\n",
    "                    , self.generator_activation\n",
    "                    , self.generator_dropout_rate\n",
    "                    , self.generator_learning_rate\n",
    "                    , self.optimiser\n",
    "                    , self.grad_weight\n",
    "                    , self.z_dim\n",
    "                    , self.batch_size\n",
    "                    ], f)\n",
    "\n",
    "            self.plot_model(folder)\n",
    "\n",
    "    def save_model(self, run_folder):\n",
    "        self.model.save(os.path.join(run_folder, 'model.h5'))\n",
    "        self.critic.save(os.path.join(run_folder, 'critic.h5'))\n",
    "        self.generator.save(os.path.join(run_folder, 'generator.h5'))\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = WGANGP(input_dim = (IMAGE_SIZE,IMAGE_SIZE,3)\n",
    "        , critic_conv_filters = [64,128,256,512]\n",
    "        , critic_conv_kernel_size = [5,5,5,5]\n",
    "        , critic_conv_strides = [2,2,2,2]\n",
    "        , critic_batch_norm_momentum = None\n",
    "        , critic_activation = 'leaky_relu'\n",
    "        , critic_dropout_rate = None\n",
    "        , critic_learning_rate = 0.0002\n",
    "        , generator_initial_dense_layer_size = (4, 4, 512)\n",
    "        , generator_upsample = [1,1,1,1]\n",
    "        , generator_conv_filters = [256,128,64,3]\n",
    "        , generator_conv_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_strides = [2,2,2,2]\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'leaky_relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0002\n",
    "        , optimiser = 'adam'\n",
    "        , grad_weight = 10\n",
    "        , z_dim = 100\n",
    "        , batch_size = BATCH_SIZE\n",
    "        )\n",
    "\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 5\n",
    "N_CRITIC = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , n_critic = N_CRITIC\n",
    "    , using_generator = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot(gan.g_losses, color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "# plt.ylim(0, 2)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
