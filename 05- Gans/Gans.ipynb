{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# تدريب شبكات الخصومة التوليدية"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## إستيراد المكتبات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os import walk, getcwd\n",
    "import json\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'camel'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.makedirs(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## البيانات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_safari(folder):\n",
    "\n",
    "    mypath = os.path.join(\"./data\", folder)\n",
    "    txt_name_list = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        for f in filenames:\n",
    "            if f != '.DS_Store':\n",
    "                txt_name_list.append(f)\n",
    "                break\n",
    "\n",
    "    slice_train = int(80000/len(txt_name_list))  ###Setting value to be 80000 for the final dataset\n",
    "    i = 0\n",
    "    seed = np.random.randint(1, 10e6)\n",
    "\n",
    "    for txt_name in txt_name_list:\n",
    "        txt_path = os.path.join(mypath,txt_name)\n",
    "        x = np.load(txt_path)\n",
    "        x = (x.astype('float32') - 127.5) / 127.5\n",
    "        # x = x.astype('float32') / 255.0\n",
    "        \n",
    "        x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "        \n",
    "        y = [i] * len(x)  \n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(x)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(y)\n",
    "        x = x[:slice_train]\n",
    "        y = y[:slice_train]\n",
    "        if i != 0: \n",
    "            xtotal = np.concatenate((x,xtotal), axis=0)\n",
    "            ytotal = np.concatenate((y,ytotal), axis=0)\n",
    "        else:\n",
    "            xtotal = x\n",
    "            ytotal = y\n",
    "        i += 1\n",
    "        \n",
    "    return xtotal, ytotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari(DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x8149788>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## بناء شبكة الخصومة التوليدية"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self\n",
    "        , input_dim\n",
    "        , discriminator_conv_filters\n",
    "        , discriminator_conv_kernel_size\n",
    "        , discriminator_conv_strides\n",
    "        , discriminator_batch_norm_momentum\n",
    "        , discriminator_activation\n",
    "        , discriminator_dropout_rate\n",
    "        , discriminator_learning_rate\n",
    "        , generator_initial_dense_layer_size\n",
    "        , generator_upsample\n",
    "        , generator_conv_filters\n",
    "        , generator_conv_kernel_size\n",
    "        , generator_conv_strides\n",
    "        , generator_batch_norm_momentum\n",
    "        , generator_activation\n",
    "        , generator_dropout_rate\n",
    "        , generator_learning_rate\n",
    "        , optimiser\n",
    "        , z_dim\n",
    "        ):\n",
    "\n",
    "        self.name = 'gan'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.discriminator_conv_filters = discriminator_conv_filters\n",
    "        self.discriminator_conv_kernel_size = discriminator_conv_kernel_size\n",
    "        self.discriminator_conv_strides = discriminator_conv_strides\n",
    "        self.discriminator_batch_norm_momentum = discriminator_batch_norm_momentum\n",
    "        self.discriminator_activation = discriminator_activation\n",
    "        self.discriminator_dropout_rate = discriminator_dropout_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "\n",
    "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
    "        self.generator_upsample = generator_upsample\n",
    "        self.generator_conv_filters = generator_conv_filters\n",
    "        self.generator_conv_kernel_size = generator_conv_kernel_size\n",
    "        self.generator_conv_strides = generator_conv_strides\n",
    "        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n",
    "        self.generator_activation = generator_activation\n",
    "        self.generator_dropout_rate = generator_dropout_rate\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "        \n",
    "        self.optimiser = optimiser\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.n_layers_discriminator = len(discriminator_conv_filters)\n",
    "        self.n_layers_generator = len(generator_conv_filters)\n",
    "\n",
    "        self.weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "\n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "        self._build_discriminator()\n",
    "        self._build_generator()\n",
    "\n",
    "        self._build_adversarial()\n",
    "\n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'leaky_relu':\n",
    "            layer = LeakyReLU(alpha = 0.2)\n",
    "        else:\n",
    "            layer = Activation(activation)\n",
    "        return layer\n",
    "\n",
    "    def _build_discriminator(self):\n",
    "\n",
    "        ### المميز\n",
    "        # حدد المدخلات للمميز (الصورة).\n",
    "        discriminator_input = Input(shape=self.input_dim, name='discriminator_input')\n",
    "\n",
    "        x = discriminator_input\n",
    "        \n",
    "        # كدس طبقات تلافيفية فوق بعضها البعض\n",
    "        for i in range(self.n_layers_discriminator):\n",
    "\n",
    "            x = Conv2D(\n",
    "                filters = self.discriminator_conv_filters[i]\n",
    "                , kernel_size = self.discriminator_conv_kernel_size[i]\n",
    "                , strides = self.discriminator_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'discriminator_conv_' + str(i)\n",
    "                , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "\n",
    "            if self.discriminator_batch_norm_momentum and i > 0:\n",
    "                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n",
    "\n",
    "            x = self.get_activation(self.discriminator_activation)(x)\n",
    "\n",
    "            if self.discriminator_dropout_rate:\n",
    "                x = Dropout(rate = self.discriminator_dropout_rate)(x)\n",
    "        \n",
    "        #قم بتسطيح آخر طبقة لف لرياضي ( طبقة تلافيفية ) إلى متجه.\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        #طبقة كثيفة من وحدة واحدة ،\n",
    "        #مع دالة التنشيط سيغمويد التي تحول الناتج من الطبقة الكثيفة إلى النطاق [0 ، 1].\n",
    "        discriminator_output = Dense(1, activation='sigmoid', kernel_initializer = self.weight_init)(x)\n",
    "        \n",
    "        #نموذج Keras الذي يحدد شبكة المميز وهو  نموذج يأخذ صورة إدخال ويخرج رقمًا واحدًا بين 0 و \n",
    "        self.discriminator = Model(discriminator_input, discriminator_output)\n",
    "\n",
    "\n",
    "    def _build_generator(self):\n",
    "\n",
    "        ### المولد\n",
    "        \n",
    "        #تحديد المدخلات للمولد - متجه طوله 100.\n",
    "        generator_input = Input(shape=(self.z_dim,), name='generator_input')\n",
    "\n",
    "        x = generator_input\n",
    "        \n",
    "        #طبقة كثيفة تتكون من 3136 وحدة\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
    "        \n",
    "        # تطبيق تسوية الحزم\n",
    "        if self.generator_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "        # تطبيق دالة التنشيط\n",
    "        x = self.get_activation(self.generator_activation)(x)\n",
    "        \n",
    "        # إعادة تشكيل التنسور إلى  7 × 7 × 64\n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "\n",
    "        if self.generator_dropout_rate:\n",
    "            x = Dropout(rate = self.generator_dropout_rate)(x)\n",
    "\n",
    "        #######\n",
    "        #Conv2D قوم بتمرير هذا من خلال أربع طبقات   \n",
    "        # Upsampling2D أول طبقتين مسبوقتين بطبقات\n",
    "        # لإعادة تشكيل التنسور إلى 14 × 14 ، ثم 28 × 28 (حجم الصورة الأصلي)\n",
    "        # في الكل ما عدا الطبقة الأخيرة ، نستخدم تسوية الحزم\n",
    "        # Relu  وتنشيط \n",
    "        for i in range(self.n_layers_generator):\n",
    "\n",
    "            if self.generator_upsample[i] == 2:\n",
    "                x = UpSampling2D()(x)\n",
    "                x = Conv2D(\n",
    "                    filters = self.generator_conv_filters[i]\n",
    "                    , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                    , padding = 'same'\n",
    "                    , name = 'generator_conv_' + str(i)\n",
    "                    , kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "            else:\n",
    "\n",
    "                x = Conv2DTranspose(\n",
    "                    filters = self.generator_conv_filters[i]\n",
    "                    , kernel_size = self.generator_conv_kernel_size[i]\n",
    "                    , padding = 'same'\n",
    "                    , strides = self.generator_conv_strides[i]\n",
    "                    , name = 'generator_conv_' + str(i)\n",
    "                    , kernel_initializer = self.weight_init\n",
    "                    )(x)\n",
    "            \n",
    "            # tanh بعد الطبقة اللف الرياضي الأخيرة ، نستخدم تنشيط\n",
    "            # لتحويل الإخراج إلى النطاق [–1 ، 1] ، لمطابقة مجال الصورة الأصلي.\n",
    "\n",
    "            if i < self.n_layers_generator - 1:\n",
    "\n",
    "                if self.generator_batch_norm_momentum:\n",
    "                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "\n",
    "                x = self.get_activation(self.generator_activation)(x)\n",
    "                    \n",
    "                \n",
    "            else:\n",
    "\n",
    "                x = Activation('tanh')(x)\n",
    "\n",
    "        # النموذج لذي يحدد المولد - نموذج يقبل متجهًا بطول 100 ويخرج موترًا للشكل [28 ، 28 ، 1].\n",
    "\n",
    "        generator_output = x\n",
    "\n",
    "        self.generator = Model(generator_input, generator_output)\n",
    "\n",
    "       \n",
    "    def get_opti(self, lr):\n",
    "        if self.optimiser == 'adam':\n",
    "            opti = Adam(lr=lr, beta_1=0.5)\n",
    "        elif self.optimiser == 'rmsprop':\n",
    "            opti = RMSprop(lr=lr)\n",
    "        else:\n",
    "            opti = Adam(lr=lr)\n",
    "\n",
    "        return opti\n",
    "\n",
    "    def set_trainable(self, m, val):\n",
    "        m.trainable = val\n",
    "        for l in m.layers:\n",
    "            l.trainable = val\n",
    "\n",
    "\n",
    "    def _build_adversarial(self):\n",
    "        \n",
    "        ### تحميع المميز \n",
    "        \n",
    "        #يتم تجميع المميّز مع خسارة الانتروبيا المتقاطعة الثنائية \n",
    "        #، حيث أن الاستجابة ثنائية ولدينا مخرج واحد  مع دالة تنشيط سيغمويد \n",
    "        self.discriminator.compile(\n",
    "        optimizer=self.get_opti(self.discriminator_learning_rate)  \n",
    "        , loss = 'binary_crossentropy'\n",
    "        ,  metrics = ['accuracy']\n",
    "        )\n",
    "        \n",
    "        ### تجميع شبكة الخصومة التوليدية\n",
    "\n",
    "        #  نقوم بتجميد أوزان المميز - وهذا لا يؤثر على نموذج المميز الحالي الذي قمنا بتجميعه مسبقاً.\n",
    "        self.set_trainable(self.discriminator, False)\n",
    "\n",
    "        model_input = Input(shape=(self.z_dim,), name='model_input')\n",
    "        model_output = self.discriminator(self.generator(model_input))\n",
    "        \n",
    "        #نحدد نموذجًا جديدًا يكون مدخله عبارة عن متجه كامنه لديها 100 بعد ؛\n",
    "        #يتم تمريرها من خلال المولد والمميز الذي تم تجميده لإنتاج المخرج كإحتمال. \n",
    "\n",
    "        self.model = Model(model_input, model_output)\n",
    "        \n",
    "        # مرة أخرى ، نستخدم خسارة الانتروبيا المتقاطعة الثنائية ا للنموذج المشترك \n",
    "        self.model.compile(optimizer=self.get_opti(self.generator_learning_rate) , loss='binary_crossentropy', metrics=['accuracy']\n",
    "        , experimental_run_tf_function=False\n",
    "        )\n",
    "\n",
    "        self.set_trainable(self.discriminator, True)\n",
    "\n",
    "\n",
    "\n",
    "    # تدريب المميز\n",
    "    def train_discriminator(self, x_train, batch_size, using_generator):\n",
    "\n",
    "        valid = np.ones((batch_size,1))\n",
    "        fake = np.zeros((batch_size,1))\n",
    "\n",
    "        if using_generator:\n",
    "            true_imgs = next(x_train)[0]\n",
    "            if true_imgs.shape[0] != batch_size:\n",
    "                true_imgs = next(x_train)[0]\n",
    "        else:\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            true_imgs = x_train[idx]\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        d_loss_real, d_acc_real =   self.discriminator.train_on_batch(true_imgs, valid)\n",
    "        d_loss_fake, d_acc_fake =   self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss =  0.5 * (d_loss_real + d_loss_fake)\n",
    "        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "        return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n",
    "\n",
    "    def train_generator(self, batch_size):\n",
    "        valid = np.ones((batch_size,1))\n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        return self.model.train_on_batch(noise, valid)\n",
    "\n",
    "\n",
    "    def train(self, x_train, batch_size, epochs, run_folder\n",
    "    , print_every_n_batches = 50\n",
    "    , using_generator = False):\n",
    "\n",
    "        for epoch in range(self.epoch, self.epoch + epochs):\n",
    "\n",
    "            d = self.train_discriminator(x_train, batch_size, using_generator)\n",
    "            g = self.train_generator(batch_size)\n",
    "\n",
    "            print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n",
    "\n",
    "            self.d_losses.append(d)\n",
    "            self.g_losses.append(g)\n",
    "\n",
    "            if epoch % print_every_n_batches == 0:\n",
    "                self.sample_images(run_folder)\n",
    "                self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n",
    "                self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
    "                self.save_model(run_folder)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    \n",
    "    def sample_images(self, run_folder):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "        cnt = 0\n",
    "\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.discriminator, to_file=os.path.join(run_folder ,'viz/discriminator.png'), show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.generator, to_file=os.path.join(run_folder ,'viz/generator.png'), show_shapes = True, show_layer_names = True)\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, folder):\n",
    "\n",
    "        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "            pkl.dump([\n",
    "                self.input_dim\n",
    "                , self.discriminator_conv_filters\n",
    "                , self.discriminator_conv_kernel_size\n",
    "                , self.discriminator_conv_strides\n",
    "                , self.discriminator_batch_norm_momentum\n",
    "                , self.discriminator_activation\n",
    "                , self.discriminator_dropout_rate\n",
    "                , self.discriminator_learning_rate\n",
    "                , self.generator_initial_dense_layer_size\n",
    "                , self.generator_upsample\n",
    "                , self.generator_conv_filters\n",
    "                , self.generator_conv_kernel_size\n",
    "                , self.generator_conv_strides\n",
    "                , self.generator_batch_norm_momentum\n",
    "                , self.generator_activation\n",
    "                , self.generator_dropout_rate\n",
    "                , self.generator_learning_rate\n",
    "                , self.optimiser\n",
    "                , self.z_dim\n",
    "                ], f)\n",
    "\n",
    "        self.plot_model(folder)\n",
    "\n",
    "    def save_model(self, run_folder):\n",
    "        self.model.save(os.path.join(run_folder, 'model.h5'))\n",
    "        self.discriminator.save(os.path.join(run_folder, 'discriminator.h5'))\n",
    "        self.generator.save(os.path.join(run_folder, 'generator.h5'))\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(input_dim = (28,28,1)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.0008\n",
    "        , generator_initial_dense_layer_size = (7, 7, 64)\n",
    "        , generator_upsample = [2,2, 1, 1]\n",
    "        , generator_conv_filters = [128,64, 64,1]\n",
    "        , generator_conv_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_strides = [1,1, 1, 1]\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2D)    (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2D)    (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2DTran (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2DTran (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## التدريب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "0 [D loss: (0.738)(R 0.684, F 0.791)] [D acc: (0.422)(0.844, 0.000)] [G loss: 0.680] [G acc: 1.000]\n",
      "1 [D loss: (1.014)(R 0.625, F 1.403)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.669] [G acc: 1.000]\n",
      "2 [D loss: (0.684)(R 0.659, F 0.709)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.651] [G acc: 1.000]\n",
      "3 [D loss: (0.671)(R 0.648, F 0.695)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.605] [G acc: 1.000]\n",
      "4 [D loss: (0.669)(R 0.607, F 0.730)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.537] [G acc: 1.000]\n",
      "5 [D loss: (0.744)(R 0.546, F 0.941)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.603] [G acc: 1.000]\n",
      "6 [D loss: (0.674)(R 0.596, F 0.751)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.580] [G acc: 1.000]\n",
      "7 [D loss: (0.712)(R 0.563, F 0.862)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.626] [G acc: 1.000]\n",
      "8 [D loss: (0.688)(R 0.589, F 0.788)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.646] [G acc: 1.000]\n",
      "9 [D loss: (0.681)(R 0.565, F 0.796)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.666] [G acc: 1.000]\n",
      "10 [D loss: (0.650)(R 0.526, F 0.774)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.679] [G acc: 1.000]\n",
      "11 [D loss: (0.593)(R 0.413, F 0.773)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.688] [G acc: 1.000]\n",
      "12 [D loss: (0.473)(R 0.212, F 0.734)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.691] [G acc: 0.891]\n",
      "13 [D loss: (0.367)(R 0.040, F 0.695)] [D acc: (0.531)(1.000, 0.062)] [G loss: 0.695] [G acc: 0.062]\n",
      "14 [D loss: (0.347)(R 0.004, F 0.691)] [D acc: (0.969)(1.000, 0.938)] [G loss: 0.701] [G acc: 0.000]\n",
      "15 [D loss: (0.344)(R 0.003, F 0.685)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.721] [G acc: 0.000]\n",
      "16 [D loss: (0.334)(R 0.002, F 0.666)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.817] [G acc: 0.000]\n",
      "17 [D loss: (0.294)(R 0.002, F 0.586)] [D acc: (1.000)(1.000, 1.000)] [G loss: 1.450] [G acc: 0.000]\n",
      "18 [D loss: (0.145)(R 0.002, F 0.289)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.263] [G acc: 0.000]\n",
      "19 [D loss: (9.903)(R 1.110, F 18.697)] [D acc: (0.297)(0.594, 0.000)] [G loss: 1.106] [G acc: 0.000]\n",
      "20 [D loss: (0.178)(R 0.026, F 0.329)] [D acc: (1.000)(1.000, 1.000)] [G loss: 1.149] [G acc: 0.000]\n",
      "21 [D loss: (0.130)(R 0.033, F 0.227)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.624] [G acc: 0.750]\n",
      "22 [D loss: (0.088)(R 0.027, F 0.149)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.012] [G acc: 1.000]\n",
      "23 [D loss: (0.063)(R 0.031, F 0.096)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.003] [G acc: 1.000]\n",
      "24 [D loss: (0.063)(R 0.019, F 0.108)] [D acc: (0.992)(1.000, 0.984)] [G loss: 0.003] [G acc: 1.000]\n",
      "25 [D loss: (0.202)(R 0.086, F 0.318)] [D acc: (0.953)(0.984, 0.922)] [G loss: 0.005] [G acc: 1.000]\n",
      "26 [D loss: (0.403)(R 0.126, F 0.679)] [D acc: (0.844)(0.953, 0.734)] [G loss: 0.026] [G acc: 1.000]\n",
      "27 [D loss: (0.756)(R 0.256, F 1.256)] [D acc: (0.695)(0.891, 0.500)] [G loss: 0.140] [G acc: 0.984]\n",
      "28 [D loss: (0.617)(R 0.321, F 0.913)] [D acc: (0.719)(0.828, 0.609)] [G loss: 0.172] [G acc: 1.000]\n",
      "29 [D loss: (0.966)(R 0.234, F 1.697)] [D acc: (0.562)(0.938, 0.188)] [G loss: 0.640] [G acc: 0.703]\n",
      "30 [D loss: (1.276)(R 0.621, F 1.932)] [D acc: (0.359)(0.719, 0.000)] [G loss: 0.809] [G acc: 0.047]\n",
      "31 [D loss: (0.790)(R 0.585, F 0.996)] [D acc: (0.383)(0.766, 0.000)] [G loss: 0.718] [G acc: 0.266]\n",
      "32 [D loss: (0.661)(R 0.454, F 0.867)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.733] [G acc: 0.156]\n",
      "33 [D loss: (0.601)(R 0.383, F 0.819)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.766] [G acc: 0.047]\n",
      "34 [D loss: (0.516)(R 0.294, F 0.738)] [D acc: (0.602)(1.000, 0.203)] [G loss: 0.831] [G acc: 0.000]\n",
      "35 [D loss: (0.454)(R 0.225, F 0.683)] [D acc: (0.836)(1.000, 0.672)] [G loss: 0.946] [G acc: 0.000]\n",
      "36 [D loss: (0.356)(R 0.140, F 0.572)] [D acc: (0.977)(1.000, 0.953)] [G loss: 1.137] [G acc: 0.000]\n",
      "37 [D loss: (0.299)(R 0.059, F 0.539)] [D acc: (0.961)(1.000, 0.922)] [G loss: 1.266] [G acc: 0.000]\n",
      "38 [D loss: (0.380)(R 0.126, F 0.634)] [D acc: (0.883)(0.969, 0.797)] [G loss: 1.214] [G acc: 0.016]\n",
      "39 [D loss: (0.633)(R 0.101, F 1.166)] [D acc: (0.562)(1.000, 0.125)] [G loss: 1.334] [G acc: 0.000]\n",
      "40 [D loss: (0.586)(R 0.457, F 0.716)] [D acc: (0.617)(0.781, 0.453)] [G loss: 2.088] [G acc: 0.000]\n",
      "41 [D loss: (0.473)(R 0.407, F 0.539)] [D acc: (0.820)(0.844, 0.797)] [G loss: 1.223] [G acc: 0.000]\n",
      "42 [D loss: (0.324)(R 0.135, F 0.512)] [D acc: (0.883)(0.969, 0.797)] [G loss: 1.278] [G acc: 0.000]\n",
      "43 [D loss: (0.645)(R 0.231, F 1.058)] [D acc: (0.492)(0.922, 0.062)] [G loss: 0.960] [G acc: 0.000]\n",
      "44 [D loss: (0.406)(R 0.202, F 0.610)] [D acc: (0.836)(0.953, 0.719)] [G loss: 1.107] [G acc: 0.000]\n",
      "45 [D loss: (0.328)(R 0.271, F 0.385)] [D acc: (0.953)(0.922, 0.984)] [G loss: 1.540] [G acc: 0.000]\n",
      "46 [D loss: (0.251)(R 0.076, F 0.425)] [D acc: (0.992)(1.000, 0.984)] [G loss: 1.761] [G acc: 0.000]\n",
      "47 [D loss: (0.145)(R 0.052, F 0.238)] [D acc: (1.000)(1.000, 1.000)] [G loss: 2.353] [G acc: 0.000]\n",
      "48 [D loss: (0.334)(R 0.178, F 0.489)] [D acc: (0.891)(0.969, 0.812)] [G loss: 2.571] [G acc: 0.000]\n",
      "49 [D loss: (0.521)(R 0.173, F 0.869)] [D acc: (0.688)(0.938, 0.438)] [G loss: 3.203] [G acc: 0.000]\n",
      "50 [D loss: (0.354)(R 0.500, F 0.207)] [D acc: (0.906)(0.812, 1.000)] [G loss: 2.077] [G acc: 0.047]\n",
      "51 [D loss: (1.666)(R 0.013, F 3.320)] [D acc: (0.500)(1.000, 0.000)] [G loss: 1.056] [G acc: 0.047]\n",
      "52 [D loss: (0.618)(R 0.601, F 0.635)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.920] [G acc: 0.000]\n",
      "53 [D loss: (0.534)(R 0.584, F 0.484)] [D acc: (0.789)(0.750, 0.828)] [G loss: 2.693] [G acc: 0.000]\n",
      "54 [D loss: (0.667)(R 0.516, F 0.819)] [D acc: (0.562)(0.812, 0.312)] [G loss: 1.211] [G acc: 0.031]\n",
      "55 [D loss: (0.526)(R 0.409, F 0.642)] [D acc: (0.789)(0.938, 0.641)] [G loss: 1.581] [G acc: 0.000]\n",
      "56 [D loss: (0.507)(R 0.442, F 0.572)] [D acc: (0.734)(0.797, 0.672)] [G loss: 1.754] [G acc: 0.000]\n",
      "57 [D loss: (0.426)(R 0.345, F 0.507)] [D acc: (0.852)(0.938, 0.766)] [G loss: 2.464] [G acc: 0.000]\n",
      "58 [D loss: (0.906)(R 0.898, F 0.913)] [D acc: (0.383)(0.547, 0.219)] [G loss: 2.277] [G acc: 0.000]\n",
      "59 [D loss: (0.539)(R 0.784, F 0.293)] [D acc: (0.742)(0.500, 0.984)] [G loss: 0.805] [G acc: 0.359]\n",
      "60 [D loss: (0.607)(R 0.210, F 1.003)] [D acc: (0.625)(1.000, 0.250)] [G loss: 1.055] [G acc: 0.016]\n",
      "61 [D loss: (0.651)(R 0.607, F 0.695)] [D acc: (0.594)(0.766, 0.422)] [G loss: 0.702] [G acc: 0.562]\n",
      "62 [D loss: (0.484)(R 0.401, F 0.567)] [D acc: (0.812)(0.875, 0.750)] [G loss: 0.719] [G acc: 0.422]\n",
      "63 [D loss: (0.871)(R 0.620, F 1.123)] [D acc: (0.391)(0.719, 0.062)] [G loss: 0.828] [G acc: 0.156]\n",
      "64 [D loss: (0.591)(R 0.486, F 0.696)] [D acc: (0.656)(0.906, 0.406)] [G loss: 0.753] [G acc: 0.344]\n",
      "65 [D loss: (0.547)(R 0.421, F 0.672)] [D acc: (0.727)(0.906, 0.547)] [G loss: 0.904] [G acc: 0.094]\n",
      "66 [D loss: (0.685)(R 0.477, F 0.892)] [D acc: (0.508)(0.844, 0.172)] [G loss: 0.995] [G acc: 0.031]\n",
      "67 [D loss: (0.551)(R 0.464, F 0.638)] [D acc: (0.758)(0.859, 0.656)] [G loss: 1.107] [G acc: 0.031]\n",
      "68 [D loss: (0.690)(R 0.501, F 0.879)] [D acc: (0.547)(0.828, 0.266)] [G loss: 1.017] [G acc: 0.000]\n",
      "69 [D loss: (0.579)(R 0.438, F 0.720)] [D acc: (0.703)(0.938, 0.469)] [G loss: 1.107] [G acc: 0.031]\n",
      "70 [D loss: (0.694)(R 0.463, F 0.924)] [D acc: (0.484)(0.828, 0.141)] [G loss: 0.987] [G acc: 0.000]\n",
      "71 [D loss: (0.614)(R 0.581, F 0.647)] [D acc: (0.656)(0.766, 0.547)] [G loss: 0.812] [G acc: 0.125]\n",
      "72 [D loss: (0.838)(R 0.460, F 1.215)] [D acc: (0.453)(0.875, 0.031)] [G loss: 1.236] [G acc: 0.000]\n",
      "73 [D loss: (0.625)(R 0.642, F 0.609)] [D acc: (0.703)(0.734, 0.672)] [G loss: 1.105] [G acc: 0.000]\n",
      "74 [D loss: (0.588)(R 0.588, F 0.588)] [D acc: (0.812)(0.750, 0.875)] [G loss: 1.383] [G acc: 0.000]\n",
      "75 [D loss: (0.598)(R 0.412, F 0.784)] [D acc: (0.633)(0.938, 0.328)] [G loss: 0.943] [G acc: 0.016]\n",
      "76 [D loss: (0.710)(R 0.447, F 0.973)] [D acc: (0.508)(0.969, 0.047)] [G loss: 0.859] [G acc: 0.031]\n",
      "77 [D loss: (0.650)(R 0.562, F 0.738)] [D acc: (0.547)(0.797, 0.297)] [G loss: 0.806] [G acc: 0.109]\n",
      "78 [D loss: (0.647)(R 0.488, F 0.805)] [D acc: (0.578)(0.938, 0.219)] [G loss: 0.852] [G acc: 0.031]\n",
      "79 [D loss: (0.621)(R 0.489, F 0.753)] [D acc: (0.672)(0.969, 0.375)] [G loss: 0.893] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 [D loss: (0.622)(R 0.516, F 0.728)] [D acc: (0.680)(0.938, 0.422)] [G loss: 0.902] [G acc: 0.000]\n",
      "81 [D loss: (0.617)(R 0.476, F 0.759)] [D acc: (0.547)(0.922, 0.172)] [G loss: 1.002] [G acc: 0.000]\n",
      "82 [D loss: (0.572)(R 0.471, F 0.673)] [D acc: (0.742)(0.859, 0.625)] [G loss: 1.329] [G acc: 0.000]\n",
      "83 [D loss: (0.656)(R 0.458, F 0.854)] [D acc: (0.539)(0.859, 0.219)] [G loss: 1.111] [G acc: 0.000]\n",
      "84 [D loss: (0.593)(R 0.482, F 0.704)] [D acc: (0.680)(0.844, 0.516)] [G loss: 1.125] [G acc: 0.000]\n",
      "85 [D loss: (0.522)(R 0.439, F 0.604)] [D acc: (0.852)(0.922, 0.781)] [G loss: 1.353] [G acc: 0.000]\n",
      "86 [D loss: (0.534)(R 0.328, F 0.740)] [D acc: (0.695)(0.906, 0.484)] [G loss: 2.007] [G acc: 0.000]\n",
      "87 [D loss: (0.606)(R 0.518, F 0.694)] [D acc: (0.672)(0.781, 0.562)] [G loss: 1.119] [G acc: 0.062]\n",
      "88 [D loss: (0.496)(R 0.222, F 0.771)] [D acc: (0.695)(0.969, 0.422)] [G loss: 1.283] [G acc: 0.000]\n",
      "89 [D loss: (0.563)(R 0.181, F 0.945)] [D acc: (0.641)(1.000, 0.281)] [G loss: 1.713] [G acc: 0.000]\n",
      "90 [D loss: (0.775)(R 0.987, F 0.563)] [D acc: (0.617)(0.250, 0.984)] [G loss: 0.953] [G acc: 0.000]\n",
      "91 [D loss: (0.427)(R 0.275, F 0.579)] [D acc: (0.938)(0.969, 0.906)] [G loss: 1.030] [G acc: 0.016]\n",
      "92 [D loss: (0.443)(R 0.182, F 0.703)] [D acc: (0.781)(0.984, 0.578)] [G loss: 1.218] [G acc: 0.000]\n",
      "93 [D loss: (0.503)(R 0.432, F 0.573)] [D acc: (0.844)(0.844, 0.844)] [G loss: 1.179] [G acc: 0.000]\n",
      "94 [D loss: (0.551)(R 0.215, F 0.887)] [D acc: (0.664)(0.953, 0.375)] [G loss: 1.581] [G acc: 0.000]\n",
      "95 [D loss: (0.621)(R 0.630, F 0.612)] [D acc: (0.625)(0.609, 0.641)] [G loss: 1.336] [G acc: 0.016]\n",
      "96 [D loss: (0.501)(R 0.336, F 0.666)] [D acc: (0.789)(0.891, 0.688)] [G loss: 1.241] [G acc: 0.062]\n",
      "97 [D loss: (1.348)(R 0.383, F 2.313)] [D acc: (0.500)(0.828, 0.172)] [G loss: 1.042] [G acc: 0.031]\n",
      "98 [D loss: (0.918)(R 0.831, F 1.004)] [D acc: (0.281)(0.203, 0.359)] [G loss: 0.782] [G acc: 0.203]\n",
      "99 [D loss: (0.895)(R 0.652, F 1.138)] [D acc: (0.359)(0.609, 0.109)] [G loss: 0.764] [G acc: 0.141]\n",
      "100 [D loss: (0.717)(R 0.635, F 0.799)] [D acc: (0.453)(0.625, 0.281)] [G loss: 0.763] [G acc: 0.109]\n",
      "101 [D loss: (0.629)(R 0.557, F 0.701)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.739] [G acc: 0.203]\n",
      "102 [D loss: (0.647)(R 0.515, F 0.780)] [D acc: (0.555)(0.766, 0.344)] [G loss: 0.730] [G acc: 0.234]\n",
      "103 [D loss: (0.698)(R 0.440, F 0.956)] [D acc: (0.570)(0.906, 0.234)] [G loss: 0.762] [G acc: 0.125]\n",
      "104 [D loss: (0.708)(R 0.578, F 0.838)] [D acc: (0.516)(0.688, 0.344)] [G loss: 0.774] [G acc: 0.203]\n",
      "105 [D loss: (0.680)(R 0.615, F 0.746)] [D acc: (0.562)(0.656, 0.469)] [G loss: 0.839] [G acc: 0.031]\n",
      "106 [D loss: (0.690)(R 0.631, F 0.749)] [D acc: (0.531)(0.578, 0.484)] [G loss: 0.796] [G acc: 0.047]\n",
      "107 [D loss: (0.671)(R 0.656, F 0.687)] [D acc: (0.594)(0.547, 0.641)] [G loss: 0.805] [G acc: 0.109]\n",
      "108 [D loss: (0.682)(R 0.664, F 0.701)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.843] [G acc: 0.016]\n",
      "109 [D loss: (0.661)(R 0.670, F 0.653)] [D acc: (0.625)(0.531, 0.719)] [G loss: 0.898] [G acc: 0.016]\n",
      "110 [D loss: (0.710)(R 0.709, F 0.711)] [D acc: (0.555)(0.531, 0.578)] [G loss: 0.804] [G acc: 0.047]\n",
      "111 [D loss: (0.689)(R 0.631, F 0.746)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.803] [G acc: 0.078]\n",
      "112 [D loss: (0.692)(R 0.673, F 0.711)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.788] [G acc: 0.062]\n",
      "113 [D loss: (0.672)(R 0.628, F 0.716)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.797] [G acc: 0.047]\n",
      "114 [D loss: (0.671)(R 0.621, F 0.720)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.815] [G acc: 0.156]\n",
      "115 [D loss: (0.719)(R 0.691, F 0.747)] [D acc: (0.430)(0.484, 0.375)] [G loss: 0.760] [G acc: 0.125]\n",
      "116 [D loss: (0.699)(R 0.651, F 0.746)] [D acc: (0.562)(0.688, 0.438)] [G loss: 0.770] [G acc: 0.203]\n",
      "117 [D loss: (0.654)(R 0.657, F 0.651)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.812] [G acc: 0.172]\n",
      "118 [D loss: (0.716)(R 0.681, F 0.751)] [D acc: (0.523)(0.609, 0.438)] [G loss: 0.800] [G acc: 0.062]\n",
      "119 [D loss: (0.649)(R 0.641, F 0.657)] [D acc: (0.656)(0.750, 0.562)] [G loss: 0.779] [G acc: 0.141]\n",
      "120 [D loss: (0.436)(R 0.604, F 0.268)] [D acc: (0.883)(0.781, 0.984)] [G loss: 0.771] [G acc: 0.453]\n",
      "121 [D loss: (0.648)(R 0.663, F 0.633)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.510] [G acc: 0.000]\n",
      "122 [D loss: (0.698)(R 0.373, F 1.023)] [D acc: (0.586)(0.969, 0.203)] [G loss: 0.811] [G acc: 0.172]\n",
      "123 [D loss: (0.476)(R 0.366, F 0.586)] [D acc: (0.797)(0.938, 0.656)] [G loss: 0.773] [G acc: 0.375]\n",
      "124 [D loss: (0.844)(R 0.612, F 1.075)] [D acc: (0.492)(0.734, 0.250)] [G loss: 0.976] [G acc: 0.016]\n",
      "125 [D loss: (0.680)(R 0.511, F 0.850)] [D acc: (0.523)(0.844, 0.203)] [G loss: 0.849] [G acc: 0.000]\n",
      "126 [D loss: (0.667)(R 0.548, F 0.786)] [D acc: (0.594)(0.828, 0.359)] [G loss: 0.814] [G acc: 0.062]\n",
      "127 [D loss: (0.624)(R 0.545, F 0.702)] [D acc: (0.648)(0.844, 0.453)] [G loss: 0.858] [G acc: 0.062]\n",
      "128 [D loss: (0.671)(R 0.585, F 0.758)] [D acc: (0.547)(0.781, 0.312)] [G loss: 0.764] [G acc: 0.234]\n",
      "129 [D loss: (0.666)(R 0.535, F 0.797)] [D acc: (0.594)(0.828, 0.359)] [G loss: 0.778] [G acc: 0.203]\n",
      "130 [D loss: (0.645)(R 0.544, F 0.747)] [D acc: (0.602)(0.812, 0.391)] [G loss: 0.803] [G acc: 0.156]\n",
      "131 [D loss: (0.657)(R 0.579, F 0.735)] [D acc: (0.656)(0.797, 0.516)] [G loss: 0.763] [G acc: 0.266]\n",
      "132 [D loss: (0.604)(R 0.526, F 0.683)] [D acc: (0.711)(0.844, 0.578)] [G loss: 0.744] [G acc: 0.344]\n",
      "133 [D loss: (0.787)(R 0.673, F 0.901)] [D acc: (0.430)(0.641, 0.219)] [G loss: 1.033] [G acc: 0.000]\n",
      "134 [D loss: (0.704)(R 0.644, F 0.765)] [D acc: (0.469)(0.656, 0.281)] [G loss: 0.790] [G acc: 0.047]\n",
      "135 [D loss: (0.621)(R 0.574, F 0.667)] [D acc: (0.688)(0.766, 0.609)] [G loss: 0.781] [G acc: 0.281]\n",
      "136 [D loss: (0.701)(R 0.596, F 0.806)] [D acc: (0.438)(0.672, 0.203)] [G loss: 0.805] [G acc: 0.062]\n",
      "137 [D loss: (0.679)(R 0.600, F 0.757)] [D acc: (0.531)(0.719, 0.344)] [G loss: 0.799] [G acc: 0.047]\n",
      "138 [D loss: (0.668)(R 0.617, F 0.719)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.817] [G acc: 0.062]\n",
      "139 [D loss: (0.660)(R 0.628, F 0.693)] [D acc: (0.609)(0.672, 0.547)] [G loss: 0.814] [G acc: 0.062]\n",
      "140 [D loss: (0.671)(R 0.569, F 0.772)] [D acc: (0.547)(0.797, 0.297)] [G loss: 0.844] [G acc: 0.031]\n",
      "141 [D loss: (0.661)(R 0.631, F 0.692)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.815] [G acc: 0.078]\n",
      "142 [D loss: (0.665)(R 0.593, F 0.737)] [D acc: (0.594)(0.797, 0.391)] [G loss: 0.858] [G acc: 0.016]\n",
      "143 [D loss: (0.676)(R 0.613, F 0.739)] [D acc: (0.570)(0.672, 0.469)] [G loss: 0.847] [G acc: 0.000]\n",
      "144 [D loss: (0.653)(R 0.587, F 0.719)] [D acc: (0.672)(0.844, 0.500)] [G loss: 0.860] [G acc: 0.031]\n",
      "145 [D loss: (0.687)(R 0.603, F 0.772)] [D acc: (0.539)(0.703, 0.375)] [G loss: 0.812] [G acc: 0.047]\n",
      "146 [D loss: (0.685)(R 0.623, F 0.748)] [D acc: (0.508)(0.609, 0.406)] [G loss: 0.822] [G acc: 0.016]\n",
      "147 [D loss: (0.673)(R 0.631, F 0.715)] [D acc: (0.570)(0.641, 0.500)] [G loss: 0.811] [G acc: 0.047]\n",
      "148 [D loss: (0.689)(R 0.659, F 0.720)] [D acc: (0.523)(0.547, 0.500)] [G loss: 0.805] [G acc: 0.016]\n",
      "149 [D loss: (0.680)(R 0.655, F 0.705)] [D acc: (0.539)(0.531, 0.547)] [G loss: 0.802] [G acc: 0.047]\n",
      "150 [D loss: (0.682)(R 0.638, F 0.725)] [D acc: (0.523)(0.516, 0.531)] [G loss: 0.792] [G acc: 0.078]\n",
      "151 [D loss: (0.675)(R 0.603, F 0.748)] [D acc: (0.547)(0.688, 0.406)] [G loss: 0.803] [G acc: 0.016]\n",
      "152 [D loss: (0.663)(R 0.616, F 0.709)] [D acc: (0.648)(0.672, 0.625)] [G loss: 0.787] [G acc: 0.062]\n",
      "153 [D loss: (0.740)(R 0.626, F 0.853)] [D acc: (0.469)(0.672, 0.266)] [G loss: 0.749] [G acc: 0.172]\n",
      "154 [D loss: (0.729)(R 0.690, F 0.768)] [D acc: (0.359)(0.422, 0.297)] [G loss: 0.733] [G acc: 0.203]\n",
      "155 [D loss: (0.704)(R 0.705, F 0.702)] [D acc: (0.461)(0.406, 0.516)] [G loss: 0.756] [G acc: 0.078]\n",
      "156 [D loss: (0.696)(R 0.715, F 0.677)] [D acc: (0.555)(0.312, 0.797)] [G loss: 0.742] [G acc: 0.047]\n",
      "157 [D loss: (0.694)(R 0.686, F 0.701)] [D acc: (0.562)(0.500, 0.625)] [G loss: 0.746] [G acc: 0.016]\n",
      "158 [D loss: (0.682)(R 0.689, F 0.674)] [D acc: (0.586)(0.422, 0.750)] [G loss: 0.737] [G acc: 0.109]\n",
      "159 [D loss: (0.694)(R 0.639, F 0.749)] [D acc: (0.492)(0.656, 0.328)] [G loss: 0.744] [G acc: 0.047]\n",
      "160 [D loss: (0.680)(R 0.676, F 0.684)] [D acc: (0.633)(0.531, 0.734)] [G loss: 0.731] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 [D loss: (0.718)(R 0.648, F 0.788)] [D acc: (0.484)(0.594, 0.375)] [G loss: 0.731] [G acc: 0.109]\n",
      "162 [D loss: (0.674)(R 0.666, F 0.682)] [D acc: (0.617)(0.562, 0.672)] [G loss: 0.730] [G acc: 0.141]\n",
      "163 [D loss: (0.684)(R 0.634, F 0.733)] [D acc: (0.453)(0.594, 0.312)] [G loss: 0.751] [G acc: 0.078]\n",
      "164 [D loss: (0.678)(R 0.676, F 0.680)] [D acc: (0.586)(0.469, 0.703)] [G loss: 0.753] [G acc: 0.094]\n",
      "165 [D loss: (0.776)(R 0.612, F 0.941)] [D acc: (0.414)(0.688, 0.141)] [G loss: 0.755] [G acc: 0.016]\n",
      "166 [D loss: (0.693)(R 0.721, F 0.666)] [D acc: (0.547)(0.219, 0.875)] [G loss: 0.743] [G acc: 0.047]\n",
      "167 [D loss: (0.679)(R 0.695, F 0.664)] [D acc: (0.648)(0.438, 0.859)] [G loss: 0.757] [G acc: 0.031]\n",
      "168 [D loss: (0.671)(R 0.660, F 0.682)] [D acc: (0.609)(0.531, 0.688)] [G loss: 0.754] [G acc: 0.078]\n",
      "169 [D loss: (0.695)(R 0.678, F 0.711)] [D acc: (0.523)(0.500, 0.547)] [G loss: 0.757] [G acc: 0.031]\n",
      "170 [D loss: (0.683)(R 0.668, F 0.699)] [D acc: (0.586)(0.547, 0.625)] [G loss: 0.755] [G acc: 0.000]\n",
      "171 [D loss: (0.707)(R 0.661, F 0.753)] [D acc: (0.461)(0.547, 0.375)] [G loss: 0.754] [G acc: 0.047]\n",
      "172 [D loss: (0.687)(R 0.697, F 0.677)] [D acc: (0.531)(0.422, 0.641)] [G loss: 0.746] [G acc: 0.047]\n",
      "173 [D loss: (0.685)(R 0.675, F 0.695)] [D acc: (0.562)(0.500, 0.625)] [G loss: 0.742] [G acc: 0.016]\n",
      "174 [D loss: (0.717)(R 0.678, F 0.756)] [D acc: (0.445)(0.500, 0.391)] [G loss: 0.743] [G acc: 0.031]\n",
      "175 [D loss: (0.689)(R 0.706, F 0.672)] [D acc: (0.570)(0.375, 0.766)] [G loss: 0.745] [G acc: 0.031]\n",
      "176 [D loss: (0.682)(R 0.695, F 0.669)] [D acc: (0.586)(0.359, 0.812)] [G loss: 0.747] [G acc: 0.016]\n",
      "177 [D loss: (0.698)(R 0.676, F 0.721)] [D acc: (0.484)(0.500, 0.469)] [G loss: 0.748] [G acc: 0.016]\n",
      "178 [D loss: (0.686)(R 0.695, F 0.677)] [D acc: (0.594)(0.422, 0.766)] [G loss: 0.749] [G acc: 0.000]\n",
      "179 [D loss: (0.689)(R 0.708, F 0.671)] [D acc: (0.617)(0.391, 0.844)] [G loss: 0.748] [G acc: 0.031]\n",
      "180 [D loss: (0.689)(R 0.661, F 0.716)] [D acc: (0.516)(0.609, 0.422)] [G loss: 0.748] [G acc: 0.062]\n",
      "181 [D loss: (0.702)(R 0.718, F 0.685)] [D acc: (0.469)(0.281, 0.656)] [G loss: 0.743] [G acc: 0.000]\n",
      "182 [D loss: (0.700)(R 0.660, F 0.741)] [D acc: (0.531)(0.641, 0.422)] [G loss: 0.739] [G acc: 0.031]\n",
      "183 [D loss: (0.685)(R 0.684, F 0.685)] [D acc: (0.570)(0.469, 0.672)] [G loss: 0.742] [G acc: 0.047]\n",
      "184 [D loss: (0.705)(R 0.676, F 0.734)] [D acc: (0.477)(0.562, 0.391)] [G loss: 0.740] [G acc: 0.000]\n",
      "185 [D loss: (0.682)(R 0.692, F 0.673)] [D acc: (0.625)(0.438, 0.812)] [G loss: 0.747] [G acc: 0.047]\n",
      "186 [D loss: (0.687)(R 0.679, F 0.694)] [D acc: (0.555)(0.500, 0.609)] [G loss: 0.749] [G acc: 0.094]\n",
      "187 [D loss: (0.679)(R 0.678, F 0.679)] [D acc: (0.664)(0.547, 0.781)] [G loss: 0.768] [G acc: 0.016]\n",
      "188 [D loss: (0.724)(R 0.643, F 0.805)] [D acc: (0.492)(0.656, 0.328)] [G loss: 0.792] [G acc: 0.125]\n",
      "189 [D loss: (0.670)(R 0.729, F 0.611)] [D acc: (0.734)(0.469, 1.000)] [G loss: 0.983] [G acc: 0.000]\n",
      "190 [D loss: (0.772)(R 0.600, F 0.944)] [D acc: (0.398)(0.719, 0.078)] [G loss: 0.746] [G acc: 0.078]\n",
      "191 [D loss: (0.686)(R 0.659, F 0.713)] [D acc: (0.578)(0.750, 0.406)] [G loss: 0.740] [G acc: 0.141]\n",
      "192 [D loss: (0.689)(R 0.648, F 0.731)] [D acc: (0.500)(0.656, 0.344)] [G loss: 0.747] [G acc: 0.094]\n",
      "193 [D loss: (0.693)(R 0.682, F 0.704)] [D acc: (0.508)(0.516, 0.500)] [G loss: 0.741] [G acc: 0.094]\n",
      "194 [D loss: (0.688)(R 0.683, F 0.693)] [D acc: (0.523)(0.531, 0.516)] [G loss: 0.746] [G acc: 0.109]\n",
      "195 [D loss: (0.689)(R 0.669, F 0.710)] [D acc: (0.539)(0.578, 0.500)] [G loss: 0.734] [G acc: 0.203]\n",
      "196 [D loss: (0.699)(R 0.686, F 0.713)] [D acc: (0.445)(0.500, 0.391)] [G loss: 0.748] [G acc: 0.078]\n",
      "197 [D loss: (0.690)(R 0.674, F 0.706)] [D acc: (0.523)(0.578, 0.469)] [G loss: 0.745] [G acc: 0.125]\n",
      "198 [D loss: (0.692)(R 0.688, F 0.697)] [D acc: (0.523)(0.469, 0.578)] [G loss: 0.743] [G acc: 0.094]\n",
      "199 [D loss: (0.700)(R 0.690, F 0.710)] [D acc: (0.492)(0.500, 0.484)] [G loss: 0.741] [G acc: 0.109]\n",
      "200 [D loss: (0.706)(R 0.705, F 0.706)] [D acc: (0.445)(0.375, 0.516)] [G loss: 0.737] [G acc: 0.094]\n",
      "201 [D loss: (0.685)(R 0.699, F 0.670)] [D acc: (0.609)(0.406, 0.812)] [G loss: 0.737] [G acc: 0.094]\n",
      "202 [D loss: (0.690)(R 0.683, F 0.697)] [D acc: (0.531)(0.547, 0.516)] [G loss: 0.728] [G acc: 0.234]\n",
      "203 [D loss: (0.690)(R 0.682, F 0.697)] [D acc: (0.523)(0.531, 0.516)] [G loss: 0.727] [G acc: 0.156]\n",
      "204 [D loss: (0.686)(R 0.668, F 0.703)] [D acc: (0.570)(0.609, 0.531)] [G loss: 0.737] [G acc: 0.156]\n",
      "205 [D loss: (0.690)(R 0.675, F 0.706)] [D acc: (0.539)(0.594, 0.484)] [G loss: 0.769] [G acc: 0.078]\n",
      "206 [D loss: (0.689)(R 0.699, F 0.678)] [D acc: (0.508)(0.422, 0.594)] [G loss: 0.790] [G acc: 0.078]\n",
      "207 [D loss: (0.690)(R 0.677, F 0.702)] [D acc: (0.547)(0.531, 0.562)] [G loss: 0.772] [G acc: 0.031]\n",
      "208 [D loss: (0.700)(R 0.672, F 0.728)] [D acc: (0.508)(0.594, 0.422)] [G loss: 0.770] [G acc: 0.031]\n",
      "209 [D loss: (0.691)(R 0.691, F 0.691)] [D acc: (0.539)(0.469, 0.609)] [G loss: 0.732] [G acc: 0.141]\n",
      "210 [D loss: (0.689)(R 0.682, F 0.695)] [D acc: (0.555)(0.516, 0.594)] [G loss: 0.731] [G acc: 0.156]\n",
      "211 [D loss: (0.686)(R 0.672, F 0.700)] [D acc: (0.539)(0.531, 0.547)] [G loss: 0.739] [G acc: 0.109]\n",
      "212 [D loss: (0.694)(R 0.687, F 0.702)] [D acc: (0.539)(0.547, 0.531)] [G loss: 0.739] [G acc: 0.156]\n",
      "213 [D loss: (0.690)(R 0.681, F 0.700)] [D acc: (0.570)(0.656, 0.484)] [G loss: 0.741] [G acc: 0.094]\n",
      "214 [D loss: (0.686)(R 0.685, F 0.687)] [D acc: (0.586)(0.484, 0.688)] [G loss: 0.728] [G acc: 0.188]\n",
      "215 [D loss: (0.697)(R 0.684, F 0.709)] [D acc: (0.453)(0.453, 0.453)] [G loss: 0.751] [G acc: 0.062]\n",
      "216 [D loss: (0.692)(R 0.704, F 0.681)] [D acc: (0.523)(0.359, 0.688)] [G loss: 0.741] [G acc: 0.156]\n",
      "217 [D loss: (0.685)(R 0.688, F 0.681)] [D acc: (0.594)(0.500, 0.688)] [G loss: 0.730] [G acc: 0.188]\n",
      "218 [D loss: (0.689)(R 0.677, F 0.701)] [D acc: (0.562)(0.562, 0.562)] [G loss: 0.728] [G acc: 0.219]\n",
      "219 [D loss: (0.696)(R 0.690, F 0.702)] [D acc: (0.484)(0.500, 0.469)] [G loss: 0.727] [G acc: 0.156]\n",
      "220 [D loss: (0.693)(R 0.682, F 0.704)] [D acc: (0.539)(0.562, 0.516)] [G loss: 0.738] [G acc: 0.078]\n",
      "221 [D loss: (0.692)(R 0.688, F 0.696)] [D acc: (0.523)(0.531, 0.516)] [G loss: 0.739] [G acc: 0.094]\n",
      "222 [D loss: (0.681)(R 0.676, F 0.686)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.742] [G acc: 0.109]\n",
      "223 [D loss: (0.698)(R 0.685, F 0.711)] [D acc: (0.531)(0.594, 0.469)] [G loss: 0.743] [G acc: 0.031]\n",
      "224 [D loss: (0.684)(R 0.681, F 0.688)] [D acc: (0.602)(0.547, 0.656)] [G loss: 0.748] [G acc: 0.094]\n",
      "225 [D loss: (0.690)(R 0.688, F 0.692)] [D acc: (0.539)(0.500, 0.578)] [G loss: 0.773] [G acc: 0.047]\n",
      "226 [D loss: (0.689)(R 0.700, F 0.677)] [D acc: (0.531)(0.391, 0.672)] [G loss: 0.772] [G acc: 0.016]\n",
      "227 [D loss: (0.680)(R 0.696, F 0.663)] [D acc: (0.617)(0.422, 0.812)] [G loss: 0.771] [G acc: 0.016]\n",
      "228 [D loss: (0.764)(R 0.607, F 0.922)] [D acc: (0.391)(0.750, 0.031)] [G loss: 0.765] [G acc: 0.047]\n",
      "229 [D loss: (0.686)(R 0.675, F 0.696)] [D acc: (0.562)(0.625, 0.500)] [G loss: 0.737] [G acc: 0.172]\n",
      "230 [D loss: (0.678)(R 0.666, F 0.691)] [D acc: (0.570)(0.531, 0.609)] [G loss: 0.725] [G acc: 0.219]\n",
      "231 [D loss: (0.682)(R 0.654, F 0.711)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.729] [G acc: 0.281]\n",
      "232 [D loss: (0.707)(R 0.643, F 0.772)] [D acc: (0.523)(0.766, 0.281)] [G loss: 0.741] [G acc: 0.109]\n",
      "233 [D loss: (0.684)(R 0.665, F 0.703)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.742] [G acc: 0.109]\n",
      "234 [D loss: (0.687)(R 0.668, F 0.706)] [D acc: (0.555)(0.594, 0.516)] [G loss: 0.740] [G acc: 0.188]\n",
      "235 [D loss: (0.679)(R 0.642, F 0.716)] [D acc: (0.602)(0.734, 0.469)] [G loss: 0.744] [G acc: 0.094]\n",
      "236 [D loss: (0.659)(R 0.655, F 0.662)] [D acc: (0.719)(0.656, 0.781)] [G loss: 0.769] [G acc: 0.156]\n",
      "237 [D loss: (0.709)(R 0.640, F 0.778)] [D acc: (0.523)(0.625, 0.422)] [G loss: 0.792] [G acc: 0.109]\n",
      "238 [D loss: (0.681)(R 0.661, F 0.700)] [D acc: (0.625)(0.703, 0.547)] [G loss: 0.804] [G acc: 0.078]\n",
      "239 [D loss: (0.680)(R 0.642, F 0.718)] [D acc: (0.547)(0.656, 0.438)] [G loss: 0.754] [G acc: 0.234]\n",
      "240 [D loss: (0.686)(R 0.616, F 0.757)] [D acc: (0.547)(0.719, 0.375)] [G loss: 0.767] [G acc: 0.156]\n",
      "241 [D loss: (0.691)(R 0.692, F 0.690)] [D acc: (0.492)(0.406, 0.578)] [G loss: 0.752] [G acc: 0.188]\n",
      "242 [D loss: (0.691)(R 0.679, F 0.703)] [D acc: (0.570)(0.578, 0.562)] [G loss: 0.771] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 [D loss: (0.703)(R 0.667, F 0.739)] [D acc: (0.477)(0.562, 0.391)] [G loss: 0.759] [G acc: 0.109]\n",
      "244 [D loss: (0.692)(R 0.681, F 0.702)] [D acc: (0.523)(0.516, 0.531)] [G loss: 0.731] [G acc: 0.328]\n",
      "245 [D loss: (0.682)(R 0.658, F 0.707)] [D acc: (0.531)(0.672, 0.391)] [G loss: 0.754] [G acc: 0.203]\n",
      "246 [D loss: (0.682)(R 0.667, F 0.696)] [D acc: (0.578)(0.578, 0.578)] [G loss: 0.757] [G acc: 0.125]\n",
      "247 [D loss: (0.689)(R 0.669, F 0.709)] [D acc: (0.570)(0.688, 0.453)] [G loss: 0.752] [G acc: 0.266]\n",
      "248 [D loss: (0.693)(R 0.678, F 0.707)] [D acc: (0.555)(0.578, 0.531)] [G loss: 0.775] [G acc: 0.141]\n",
      "249 [D loss: (0.686)(R 0.677, F 0.695)] [D acc: (0.555)(0.562, 0.547)] [G loss: 0.756] [G acc: 0.109]\n",
      "250 [D loss: (0.680)(R 0.661, F 0.699)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.746] [G acc: 0.219]\n",
      "251 [D loss: (0.681)(R 0.672, F 0.689)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.803] [G acc: 0.062]\n",
      "252 [D loss: (0.679)(R 0.673, F 0.686)] [D acc: (0.633)(0.609, 0.656)] [G loss: 0.771] [G acc: 0.141]\n",
      "253 [D loss: (0.712)(R 0.690, F 0.734)] [D acc: (0.477)(0.484, 0.469)] [G loss: 0.718] [G acc: 0.312]\n",
      "254 [D loss: (0.697)(R 0.662, F 0.732)] [D acc: (0.469)(0.547, 0.391)] [G loss: 0.740] [G acc: 0.266]\n",
      "255 [D loss: (0.690)(R 0.668, F 0.712)] [D acc: (0.516)(0.531, 0.500)] [G loss: 0.750] [G acc: 0.172]\n",
      "256 [D loss: (0.692)(R 0.661, F 0.723)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.749] [G acc: 0.156]\n",
      "257 [D loss: (0.687)(R 0.682, F 0.692)] [D acc: (0.570)(0.531, 0.609)] [G loss: 0.761] [G acc: 0.109]\n",
      "258 [D loss: (0.680)(R 0.674, F 0.685)] [D acc: (0.562)(0.516, 0.609)] [G loss: 0.770] [G acc: 0.156]\n",
      "259 [D loss: (0.689)(R 0.685, F 0.694)] [D acc: (0.539)(0.500, 0.578)] [G loss: 0.771] [G acc: 0.125]\n",
      "260 [D loss: (0.688)(R 0.680, F 0.697)] [D acc: (0.531)(0.531, 0.531)] [G loss: 0.762] [G acc: 0.141]\n",
      "261 [D loss: (0.681)(R 0.676, F 0.686)] [D acc: (0.602)(0.547, 0.656)] [G loss: 0.762] [G acc: 0.109]\n",
      "262 [D loss: (0.678)(R 0.663, F 0.693)] [D acc: (0.578)(0.594, 0.562)] [G loss: 0.777] [G acc: 0.141]\n",
      "263 [D loss: (0.688)(R 0.688, F 0.688)] [D acc: (0.484)(0.406, 0.562)] [G loss: 0.757] [G acc: 0.203]\n",
      "264 [D loss: (0.680)(R 0.662, F 0.698)] [D acc: (0.555)(0.625, 0.484)] [G loss: 0.768] [G acc: 0.156]\n",
      "265 [D loss: (0.682)(R 0.653, F 0.711)] [D acc: (0.562)(0.562, 0.562)] [G loss: 0.790] [G acc: 0.094]\n",
      "266 [D loss: (0.677)(R 0.683, F 0.672)] [D acc: (0.562)(0.438, 0.688)] [G loss: 0.772] [G acc: 0.094]\n",
      "267 [D loss: (0.667)(R 0.625, F 0.709)] [D acc: (0.602)(0.641, 0.562)] [G loss: 0.782] [G acc: 0.094]\n",
      "268 [D loss: (0.655)(R 0.611, F 0.699)] [D acc: (0.617)(0.688, 0.547)] [G loss: 0.787] [G acc: 0.141]\n",
      "269 [D loss: (0.678)(R 0.626, F 0.729)] [D acc: (0.555)(0.688, 0.422)] [G loss: 0.808] [G acc: 0.094]\n",
      "270 [D loss: (0.669)(R 0.637, F 0.701)] [D acc: (0.562)(0.609, 0.516)] [G loss: 0.796] [G acc: 0.078]\n",
      "271 [D loss: (0.688)(R 0.663, F 0.714)] [D acc: (0.539)(0.562, 0.516)] [G loss: 0.778] [G acc: 0.141]\n",
      "272 [D loss: (0.703)(R 0.650, F 0.755)] [D acc: (0.555)(0.641, 0.469)] [G loss: 0.765] [G acc: 0.125]\n",
      "273 [D loss: (0.676)(R 0.671, F 0.681)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.804] [G acc: 0.078]\n",
      "274 [D loss: (0.656)(R 0.600, F 0.713)] [D acc: (0.617)(0.734, 0.500)] [G loss: 0.783] [G acc: 0.172]\n",
      "275 [D loss: (0.628)(R 0.621, F 0.635)] [D acc: (0.719)(0.688, 0.750)] [G loss: 0.785] [G acc: 0.250]\n",
      "276 [D loss: (0.679)(R 0.587, F 0.771)] [D acc: (0.609)(0.766, 0.453)] [G loss: 0.894] [G acc: 0.031]\n",
      "277 [D loss: (0.678)(R 0.616, F 0.740)] [D acc: (0.555)(0.688, 0.422)] [G loss: 0.797] [G acc: 0.109]\n",
      "278 [D loss: (0.684)(R 0.600, F 0.768)] [D acc: (0.578)(0.719, 0.438)] [G loss: 0.783] [G acc: 0.203]\n",
      "279 [D loss: (0.669)(R 0.615, F 0.724)] [D acc: (0.578)(0.641, 0.516)] [G loss: 0.769] [G acc: 0.250]\n",
      "280 [D loss: (0.704)(R 0.668, F 0.740)] [D acc: (0.500)(0.547, 0.453)] [G loss: 0.760] [G acc: 0.266]\n",
      "281 [D loss: (0.649)(R 0.634, F 0.664)] [D acc: (0.648)(0.641, 0.656)] [G loss: 0.778] [G acc: 0.234]\n",
      "282 [D loss: (0.691)(R 0.662, F 0.720)] [D acc: (0.516)(0.578, 0.453)] [G loss: 0.808] [G acc: 0.125]\n",
      "283 [D loss: (0.702)(R 0.640, F 0.765)] [D acc: (0.523)(0.672, 0.375)] [G loss: 0.803] [G acc: 0.078]\n",
      "284 [D loss: (0.663)(R 0.670, F 0.656)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.787] [G acc: 0.141]\n",
      "285 [D loss: (0.659)(R 0.637, F 0.681)] [D acc: (0.656)(0.625, 0.688)] [G loss: 0.791] [G acc: 0.203]\n",
      "286 [D loss: (0.659)(R 0.587, F 0.731)] [D acc: (0.586)(0.719, 0.453)] [G loss: 0.810] [G acc: 0.203]\n",
      "287 [D loss: (0.679)(R 0.680, F 0.678)] [D acc: (0.531)(0.500, 0.562)] [G loss: 0.845] [G acc: 0.078]\n",
      "288 [D loss: (0.655)(R 0.615, F 0.695)] [D acc: (0.633)(0.672, 0.594)] [G loss: 0.800] [G acc: 0.188]\n",
      "289 [D loss: (0.674)(R 0.570, F 0.779)] [D acc: (0.602)(0.719, 0.484)] [G loss: 0.795] [G acc: 0.234]\n",
      "290 [D loss: (0.673)(R 0.596, F 0.750)] [D acc: (0.570)(0.672, 0.469)] [G loss: 0.783] [G acc: 0.188]\n",
      "291 [D loss: (0.716)(R 0.619, F 0.813)] [D acc: (0.562)(0.703, 0.422)] [G loss: 0.803] [G acc: 0.109]\n",
      "292 [D loss: (0.652)(R 0.636, F 0.669)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.798] [G acc: 0.094]\n",
      "293 [D loss: (0.689)(R 0.635, F 0.744)] [D acc: (0.492)(0.562, 0.422)] [G loss: 0.777] [G acc: 0.203]\n",
      "294 [D loss: (0.671)(R 0.650, F 0.693)] [D acc: (0.602)(0.609, 0.594)] [G loss: 0.793] [G acc: 0.125]\n",
      "295 [D loss: (0.671)(R 0.631, F 0.711)] [D acc: (0.562)(0.625, 0.500)] [G loss: 0.817] [G acc: 0.062]\n",
      "296 [D loss: (0.646)(R 0.632, F 0.660)] [D acc: (0.672)(0.641, 0.703)] [G loss: 0.833] [G acc: 0.078]\n",
      "297 [D loss: (0.694)(R 0.612, F 0.777)] [D acc: (0.547)(0.672, 0.422)] [G loss: 0.814] [G acc: 0.172]\n",
      "298 [D loss: (0.682)(R 0.618, F 0.746)] [D acc: (0.586)(0.703, 0.469)] [G loss: 0.781] [G acc: 0.219]\n",
      "299 [D loss: (0.691)(R 0.664, F 0.719)] [D acc: (0.539)(0.578, 0.500)] [G loss: 0.811] [G acc: 0.141]\n",
      "300 [D loss: (0.673)(R 0.650, F 0.695)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.809] [G acc: 0.109]\n",
      "301 [D loss: (0.668)(R 0.681, F 0.654)] [D acc: (0.609)(0.500, 0.719)] [G loss: 0.877] [G acc: 0.062]\n",
      "302 [D loss: (0.684)(R 0.632, F 0.735)] [D acc: (0.570)(0.609, 0.531)] [G loss: 0.813] [G acc: 0.172]\n",
      "303 [D loss: (0.684)(R 0.665, F 0.702)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.811] [G acc: 0.109]\n",
      "304 [D loss: (0.677)(R 0.629, F 0.724)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.827] [G acc: 0.156]\n",
      "305 [D loss: (0.674)(R 0.640, F 0.708)] [D acc: (0.578)(0.609, 0.547)] [G loss: 0.808] [G acc: 0.156]\n",
      "306 [D loss: (0.640)(R 0.621, F 0.659)] [D acc: (0.695)(0.672, 0.719)] [G loss: 0.788] [G acc: 0.312]\n",
      "307 [D loss: (0.647)(R 0.614, F 0.680)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.843] [G acc: 0.219]\n",
      "308 [D loss: (0.674)(R 0.683, F 0.664)] [D acc: (0.586)(0.500, 0.672)] [G loss: 0.868] [G acc: 0.109]\n",
      "309 [D loss: (0.664)(R 0.619, F 0.709)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.787] [G acc: 0.266]\n",
      "310 [D loss: (0.654)(R 0.618, F 0.691)] [D acc: (0.617)(0.641, 0.594)] [G loss: 0.792] [G acc: 0.172]\n",
      "311 [D loss: (0.668)(R 0.582, F 0.754)] [D acc: (0.609)(0.766, 0.453)] [G loss: 0.788] [G acc: 0.250]\n",
      "312 [D loss: (0.652)(R 0.585, F 0.719)] [D acc: (0.617)(0.734, 0.500)] [G loss: 0.860] [G acc: 0.188]\n",
      "313 [D loss: (0.654)(R 0.663, F 0.645)] [D acc: (0.625)(0.562, 0.688)] [G loss: 0.866] [G acc: 0.141]\n",
      "314 [D loss: (0.720)(R 0.638, F 0.802)] [D acc: (0.484)(0.594, 0.375)] [G loss: 0.841] [G acc: 0.125]\n",
      "315 [D loss: (0.682)(R 0.642, F 0.722)] [D acc: (0.570)(0.625, 0.516)] [G loss: 0.822] [G acc: 0.172]\n",
      "316 [D loss: (0.685)(R 0.657, F 0.713)] [D acc: (0.555)(0.562, 0.547)] [G loss: 0.822] [G acc: 0.109]\n",
      "317 [D loss: (0.658)(R 0.630, F 0.685)] [D acc: (0.586)(0.672, 0.500)] [G loss: 0.794] [G acc: 0.219]\n",
      "318 [D loss: (0.680)(R 0.666, F 0.694)] [D acc: (0.523)(0.609, 0.438)] [G loss: 0.845] [G acc: 0.078]\n",
      "319 [D loss: (0.664)(R 0.638, F 0.690)] [D acc: (0.641)(0.688, 0.594)] [G loss: 0.830] [G acc: 0.141]\n",
      "320 [D loss: (0.624)(R 0.634, F 0.614)] [D acc: (0.672)(0.578, 0.766)] [G loss: 0.835] [G acc: 0.172]\n",
      "321 [D loss: (0.691)(R 0.616, F 0.767)] [D acc: (0.523)(0.641, 0.406)] [G loss: 0.868] [G acc: 0.125]\n",
      "322 [D loss: (0.670)(R 0.674, F 0.665)] [D acc: (0.617)(0.500, 0.734)] [G loss: 0.836] [G acc: 0.094]\n",
      "323 [D loss: (0.666)(R 0.672, F 0.660)] [D acc: (0.680)(0.641, 0.719)] [G loss: 0.815] [G acc: 0.156]\n",
      "324 [D loss: (0.710)(R 0.699, F 0.720)] [D acc: (0.500)(0.500, 0.500)] [G loss: 0.825] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325 [D loss: (0.691)(R 0.684, F 0.699)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.769] [G acc: 0.281]\n",
      "326 [D loss: (0.703)(R 0.622, F 0.783)] [D acc: (0.500)(0.625, 0.375)] [G loss: 0.897] [G acc: 0.062]\n",
      "327 [D loss: (0.684)(R 0.676, F 0.692)] [D acc: (0.570)(0.578, 0.562)] [G loss: 0.795] [G acc: 0.109]\n",
      "328 [D loss: (0.648)(R 0.606, F 0.689)] [D acc: (0.648)(0.672, 0.625)] [G loss: 0.775] [G acc: 0.266]\n",
      "329 [D loss: (0.660)(R 0.583, F 0.738)] [D acc: (0.617)(0.703, 0.531)] [G loss: 0.809] [G acc: 0.141]\n",
      "330 [D loss: (0.659)(R 0.631, F 0.688)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.822] [G acc: 0.188]\n",
      "331 [D loss: (0.694)(R 0.631, F 0.757)] [D acc: (0.555)(0.656, 0.453)] [G loss: 0.871] [G acc: 0.062]\n",
      "332 [D loss: (0.663)(R 0.653, F 0.673)] [D acc: (0.578)(0.531, 0.625)] [G loss: 0.831] [G acc: 0.156]\n",
      "333 [D loss: (0.653)(R 0.629, F 0.678)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.823] [G acc: 0.266]\n",
      "334 [D loss: (0.646)(R 0.582, F 0.711)] [D acc: (0.641)(0.750, 0.531)] [G loss: 0.826] [G acc: 0.141]\n",
      "335 [D loss: (0.707)(R 0.624, F 0.789)] [D acc: (0.562)(0.625, 0.500)] [G loss: 0.792] [G acc: 0.281]\n",
      "336 [D loss: (0.647)(R 0.635, F 0.659)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.841] [G acc: 0.219]\n",
      "337 [D loss: (0.680)(R 0.667, F 0.694)] [D acc: (0.539)(0.531, 0.547)] [G loss: 0.854] [G acc: 0.141]\n",
      "338 [D loss: (0.717)(R 0.677, F 0.756)] [D acc: (0.516)(0.547, 0.484)] [G loss: 0.792] [G acc: 0.250]\n",
      "339 [D loss: (0.681)(R 0.651, F 0.711)] [D acc: (0.539)(0.562, 0.516)] [G loss: 0.752] [G acc: 0.266]\n",
      "340 [D loss: (0.675)(R 0.659, F 0.692)] [D acc: (0.562)(0.594, 0.531)] [G loss: 0.790] [G acc: 0.219]\n",
      "341 [D loss: (0.670)(R 0.649, F 0.692)] [D acc: (0.578)(0.594, 0.562)] [G loss: 0.793] [G acc: 0.219]\n",
      "342 [D loss: (0.679)(R 0.652, F 0.705)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.829] [G acc: 0.172]\n",
      "343 [D loss: (0.690)(R 0.686, F 0.693)] [D acc: (0.547)(0.484, 0.609)] [G loss: 0.794] [G acc: 0.172]\n",
      "344 [D loss: (0.668)(R 0.669, F 0.667)] [D acc: (0.617)(0.594, 0.641)] [G loss: 0.823] [G acc: 0.156]\n",
      "345 [D loss: (0.652)(R 0.657, F 0.646)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.849] [G acc: 0.203]\n",
      "346 [D loss: (0.650)(R 0.612, F 0.687)] [D acc: (0.602)(0.625, 0.578)] [G loss: 0.844] [G acc: 0.172]\n",
      "347 [D loss: (0.664)(R 0.651, F 0.677)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.897] [G acc: 0.094]\n",
      "348 [D loss: (0.656)(R 0.646, F 0.665)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.891] [G acc: 0.094]\n",
      "349 [D loss: (0.648)(R 0.607, F 0.689)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.841] [G acc: 0.125]\n",
      "350 [D loss: (0.673)(R 0.634, F 0.713)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.818] [G acc: 0.203]\n",
      "351 [D loss: (0.678)(R 0.637, F 0.720)] [D acc: (0.547)(0.625, 0.469)] [G loss: 0.834] [G acc: 0.234]\n",
      "352 [D loss: (0.647)(R 0.628, F 0.666)] [D acc: (0.688)(0.703, 0.672)] [G loss: 0.830] [G acc: 0.219]\n",
      "353 [D loss: (0.680)(R 0.652, F 0.708)] [D acc: (0.570)(0.656, 0.484)] [G loss: 0.887] [G acc: 0.141]\n",
      "354 [D loss: (0.653)(R 0.680, F 0.625)] [D acc: (0.602)(0.484, 0.719)] [G loss: 0.867] [G acc: 0.125]\n",
      "355 [D loss: (0.660)(R 0.620, F 0.700)] [D acc: (0.633)(0.609, 0.656)] [G loss: 0.833] [G acc: 0.250]\n",
      "356 [D loss: (0.628)(R 0.578, F 0.678)] [D acc: (0.641)(0.688, 0.594)] [G loss: 0.908] [G acc: 0.141]\n",
      "357 [D loss: (0.677)(R 0.659, F 0.694)] [D acc: (0.586)(0.578, 0.594)] [G loss: 0.886] [G acc: 0.062]\n",
      "358 [D loss: (0.610)(R 0.584, F 0.637)] [D acc: (0.688)(0.656, 0.719)] [G loss: 0.888] [G acc: 0.125]\n",
      "359 [D loss: (0.628)(R 0.600, F 0.657)] [D acc: (0.688)(0.656, 0.719)] [G loss: 0.902] [G acc: 0.172]\n",
      "360 [D loss: (0.705)(R 0.587, F 0.823)] [D acc: (0.586)(0.750, 0.422)] [G loss: 0.950] [G acc: 0.047]\n",
      "361 [D loss: (0.696)(R 0.751, F 0.640)] [D acc: (0.523)(0.375, 0.672)] [G loss: 0.886] [G acc: 0.078]\n",
      "362 [D loss: (0.663)(R 0.647, F 0.680)] [D acc: (0.625)(0.531, 0.719)] [G loss: 0.865] [G acc: 0.156]\n",
      "363 [D loss: (0.646)(R 0.620, F 0.672)] [D acc: (0.648)(0.672, 0.625)] [G loss: 0.837] [G acc: 0.234]\n",
      "364 [D loss: (0.627)(R 0.597, F 0.656)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.881] [G acc: 0.141]\n",
      "365 [D loss: (0.670)(R 0.644, F 0.695)] [D acc: (0.586)(0.594, 0.578)] [G loss: 0.856] [G acc: 0.141]\n",
      "366 [D loss: (0.689)(R 0.629, F 0.749)] [D acc: (0.547)(0.625, 0.469)] [G loss: 0.915] [G acc: 0.062]\n",
      "367 [D loss: (0.648)(R 0.646, F 0.649)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.850] [G acc: 0.125]\n",
      "368 [D loss: (0.644)(R 0.562, F 0.727)] [D acc: (0.602)(0.688, 0.516)] [G loss: 0.866] [G acc: 0.188]\n",
      "369 [D loss: (0.661)(R 0.603, F 0.720)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.817] [G acc: 0.219]\n",
      "370 [D loss: (0.593)(R 0.537, F 0.649)] [D acc: (0.766)(0.844, 0.688)] [G loss: 0.936] [G acc: 0.094]\n",
      "371 [D loss: (0.663)(R 0.574, F 0.751)] [D acc: (0.617)(0.781, 0.453)] [G loss: 0.930] [G acc: 0.125]\n",
      "372 [D loss: (0.658)(R 0.636, F 0.679)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.945] [G acc: 0.078]\n",
      "373 [D loss: (0.676)(R 0.631, F 0.721)] [D acc: (0.539)(0.531, 0.547)] [G loss: 0.924] [G acc: 0.141]\n",
      "374 [D loss: (0.638)(R 0.636, F 0.640)] [D acc: (0.695)(0.672, 0.719)] [G loss: 0.926] [G acc: 0.125]\n",
      "375 [D loss: (0.674)(R 0.625, F 0.724)] [D acc: (0.594)(0.641, 0.547)] [G loss: 0.919] [G acc: 0.109]\n",
      "376 [D loss: (0.633)(R 0.579, F 0.687)] [D acc: (0.680)(0.719, 0.641)] [G loss: 0.912] [G acc: 0.125]\n",
      "377 [D loss: (0.603)(R 0.539, F 0.668)] [D acc: (0.672)(0.797, 0.547)] [G loss: 1.030] [G acc: 0.078]\n",
      "378 [D loss: (0.660)(R 0.664, F 0.656)] [D acc: (0.602)(0.547, 0.656)] [G loss: 0.995] [G acc: 0.125]\n",
      "379 [D loss: (0.660)(R 0.575, F 0.745)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.906] [G acc: 0.188]\n",
      "380 [D loss: (0.653)(R 0.590, F 0.716)] [D acc: (0.625)(0.672, 0.578)] [G loss: 0.925] [G acc: 0.172]\n",
      "381 [D loss: (0.651)(R 0.564, F 0.738)] [D acc: (0.602)(0.734, 0.469)] [G loss: 0.962] [G acc: 0.125]\n",
      "382 [D loss: (0.664)(R 0.589, F 0.739)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.941] [G acc: 0.047]\n",
      "383 [D loss: (0.598)(R 0.614, F 0.582)] [D acc: (0.727)(0.641, 0.812)] [G loss: 0.886] [G acc: 0.219]\n",
      "384 [D loss: (0.623)(R 0.580, F 0.667)] [D acc: (0.664)(0.688, 0.641)] [G loss: 0.976] [G acc: 0.156]\n",
      "385 [D loss: (0.655)(R 0.586, F 0.725)] [D acc: (0.609)(0.719, 0.500)] [G loss: 0.990] [G acc: 0.109]\n",
      "386 [D loss: (0.657)(R 0.647, F 0.667)] [D acc: (0.609)(0.609, 0.609)] [G loss: 0.904] [G acc: 0.109]\n",
      "387 [D loss: (0.648)(R 0.616, F 0.680)] [D acc: (0.656)(0.703, 0.609)] [G loss: 0.915] [G acc: 0.172]\n",
      "388 [D loss: (0.656)(R 0.590, F 0.722)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.889] [G acc: 0.188]\n",
      "389 [D loss: (0.653)(R 0.642, F 0.664)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.911] [G acc: 0.141]\n",
      "390 [D loss: (0.629)(R 0.622, F 0.636)] [D acc: (0.680)(0.656, 0.703)] [G loss: 0.906] [G acc: 0.156]\n",
      "391 [D loss: (0.642)(R 0.635, F 0.649)] [D acc: (0.617)(0.594, 0.641)] [G loss: 0.877] [G acc: 0.203]\n",
      "392 [D loss: (0.654)(R 0.535, F 0.774)] [D acc: (0.695)(0.766, 0.625)] [G loss: 0.931] [G acc: 0.109]\n",
      "393 [D loss: (0.637)(R 0.592, F 0.681)] [D acc: (0.656)(0.656, 0.656)] [G loss: 0.973] [G acc: 0.094]\n",
      "394 [D loss: (0.639)(R 0.564, F 0.713)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.936] [G acc: 0.172]\n",
      "395 [D loss: (0.680)(R 0.643, F 0.718)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.929] [G acc: 0.141]\n",
      "396 [D loss: (0.653)(R 0.602, F 0.703)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.890] [G acc: 0.203]\n",
      "397 [D loss: (0.629)(R 0.572, F 0.686)] [D acc: (0.648)(0.719, 0.578)] [G loss: 0.972] [G acc: 0.094]\n",
      "398 [D loss: (0.662)(R 0.648, F 0.675)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.942] [G acc: 0.078]\n",
      "399 [D loss: (0.642)(R 0.594, F 0.689)] [D acc: (0.641)(0.656, 0.625)] [G loss: 0.871] [G acc: 0.172]\n",
      "400 [D loss: (0.622)(R 0.622, F 0.621)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.882] [G acc: 0.188]\n",
      "401 [D loss: (0.665)(R 0.632, F 0.698)] [D acc: (0.602)(0.641, 0.562)] [G loss: 0.971] [G acc: 0.156]\n",
      "402 [D loss: (0.642)(R 0.620, F 0.664)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.932] [G acc: 0.094]\n",
      "403 [D loss: (0.610)(R 0.523, F 0.697)] [D acc: (0.688)(0.750, 0.625)] [G loss: 0.958] [G acc: 0.125]\n",
      "404 [D loss: (0.617)(R 0.557, F 0.677)] [D acc: (0.664)(0.688, 0.641)] [G loss: 0.933] [G acc: 0.125]\n",
      "405 [D loss: (0.665)(R 0.632, F 0.698)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.917] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406 [D loss: (0.645)(R 0.626, F 0.664)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.970] [G acc: 0.062]\n",
      "407 [D loss: (0.631)(R 0.664, F 0.597)] [D acc: (0.688)(0.609, 0.766)] [G loss: 0.962] [G acc: 0.109]\n",
      "408 [D loss: (0.628)(R 0.540, F 0.715)] [D acc: (0.680)(0.766, 0.594)] [G loss: 0.971] [G acc: 0.078]\n",
      "409 [D loss: (0.653)(R 0.672, F 0.634)] [D acc: (0.633)(0.594, 0.672)] [G loss: 0.973] [G acc: 0.156]\n",
      "410 [D loss: (0.642)(R 0.611, F 0.673)] [D acc: (0.664)(0.688, 0.641)] [G loss: 0.955] [G acc: 0.141]\n",
      "411 [D loss: (0.669)(R 0.673, F 0.664)] [D acc: (0.555)(0.547, 0.562)] [G loss: 0.959] [G acc: 0.062]\n",
      "412 [D loss: (0.647)(R 0.654, F 0.640)] [D acc: (0.562)(0.453, 0.672)] [G loss: 0.957] [G acc: 0.062]\n",
      "413 [D loss: (0.629)(R 0.608, F 0.650)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.919] [G acc: 0.078]\n",
      "414 [D loss: (0.660)(R 0.627, F 0.693)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.885] [G acc: 0.109]\n",
      "415 [D loss: (0.655)(R 0.575, F 0.734)] [D acc: (0.578)(0.609, 0.547)] [G loss: 0.891] [G acc: 0.203]\n",
      "416 [D loss: (0.608)(R 0.601, F 0.615)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.000] [G acc: 0.109]\n",
      "417 [D loss: (0.707)(R 0.552, F 0.862)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.904] [G acc: 0.141]\n",
      "418 [D loss: (0.647)(R 0.599, F 0.695)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.942] [G acc: 0.172]\n",
      "419 [D loss: (0.628)(R 0.636, F 0.621)] [D acc: (0.672)(0.594, 0.750)] [G loss: 0.914] [G acc: 0.172]\n",
      "420 [D loss: (0.627)(R 0.575, F 0.680)] [D acc: (0.672)(0.734, 0.609)] [G loss: 1.021] [G acc: 0.172]\n",
      "421 [D loss: (0.599)(R 0.622, F 0.576)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.017] [G acc: 0.078]\n",
      "422 [D loss: (0.677)(R 0.520, F 0.834)] [D acc: (0.609)(0.703, 0.516)] [G loss: 0.960] [G acc: 0.125]\n",
      "423 [D loss: (0.693)(R 0.614, F 0.771)] [D acc: (0.555)(0.641, 0.469)] [G loss: 0.937] [G acc: 0.156]\n",
      "424 [D loss: (0.719)(R 0.588, F 0.851)] [D acc: (0.547)(0.656, 0.438)] [G loss: 0.975] [G acc: 0.078]\n",
      "425 [D loss: (0.659)(R 0.673, F 0.644)] [D acc: (0.594)(0.547, 0.641)] [G loss: 0.926] [G acc: 0.156]\n",
      "426 [D loss: (0.694)(R 0.726, F 0.662)] [D acc: (0.523)(0.453, 0.594)] [G loss: 0.918] [G acc: 0.109]\n",
      "427 [D loss: (0.646)(R 0.654, F 0.639)] [D acc: (0.664)(0.547, 0.781)] [G loss: 0.958] [G acc: 0.109]\n",
      "428 [D loss: (0.633)(R 0.667, F 0.600)] [D acc: (0.633)(0.484, 0.781)] [G loss: 0.910] [G acc: 0.141]\n",
      "429 [D loss: (0.673)(R 0.590, F 0.757)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.873] [G acc: 0.188]\n",
      "430 [D loss: (0.623)(R 0.601, F 0.644)] [D acc: (0.711)(0.703, 0.719)] [G loss: 0.878] [G acc: 0.234]\n",
      "431 [D loss: (0.657)(R 0.555, F 0.760)] [D acc: (0.586)(0.688, 0.484)] [G loss: 0.870] [G acc: 0.328]\n",
      "432 [D loss: (0.652)(R 0.663, F 0.642)] [D acc: (0.633)(0.609, 0.656)] [G loss: 0.889] [G acc: 0.188]\n",
      "433 [D loss: (0.671)(R 0.628, F 0.713)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.911] [G acc: 0.172]\n",
      "434 [D loss: (0.671)(R 0.653, F 0.690)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.889] [G acc: 0.141]\n",
      "435 [D loss: (0.654)(R 0.646, F 0.662)] [D acc: (0.586)(0.516, 0.656)] [G loss: 0.878] [G acc: 0.172]\n",
      "436 [D loss: (0.639)(R 0.626, F 0.651)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.912] [G acc: 0.156]\n",
      "437 [D loss: (0.656)(R 0.613, F 0.700)] [D acc: (0.602)(0.609, 0.594)] [G loss: 0.900] [G acc: 0.141]\n",
      "438 [D loss: (0.645)(R 0.569, F 0.721)] [D acc: (0.656)(0.781, 0.531)] [G loss: 0.909] [G acc: 0.141]\n",
      "439 [D loss: (0.628)(R 0.607, F 0.649)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.941] [G acc: 0.156]\n",
      "440 [D loss: (0.636)(R 0.660, F 0.613)] [D acc: (0.648)(0.562, 0.734)] [G loss: 0.985] [G acc: 0.125]\n",
      "441 [D loss: (0.656)(R 0.619, F 0.692)] [D acc: (0.617)(0.594, 0.641)] [G loss: 0.850] [G acc: 0.312]\n",
      "442 [D loss: (0.699)(R 0.637, F 0.760)] [D acc: (0.617)(0.656, 0.578)] [G loss: 0.895] [G acc: 0.156]\n",
      "443 [D loss: (0.651)(R 0.660, F 0.642)] [D acc: (0.602)(0.531, 0.672)] [G loss: 0.913] [G acc: 0.156]\n",
      "444 [D loss: (0.660)(R 0.651, F 0.669)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.932] [G acc: 0.094]\n",
      "445 [D loss: (0.641)(R 0.628, F 0.655)] [D acc: (0.664)(0.656, 0.672)] [G loss: 0.911] [G acc: 0.219]\n",
      "446 [D loss: (0.631)(R 0.637, F 0.626)] [D acc: (0.656)(0.594, 0.719)] [G loss: 0.929] [G acc: 0.141]\n",
      "447 [D loss: (0.712)(R 0.687, F 0.738)] [D acc: (0.516)(0.531, 0.500)] [G loss: 0.874] [G acc: 0.188]\n",
      "448 [D loss: (0.617)(R 0.605, F 0.630)] [D acc: (0.680)(0.688, 0.672)] [G loss: 0.975] [G acc: 0.094]\n",
      "449 [D loss: (0.592)(R 0.545, F 0.639)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.016] [G acc: 0.141]\n",
      "450 [D loss: (0.612)(R 0.621, F 0.603)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.039] [G acc: 0.062]\n",
      "451 [D loss: (0.717)(R 0.662, F 0.772)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.993] [G acc: 0.156]\n",
      "452 [D loss: (0.612)(R 0.647, F 0.577)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.029] [G acc: 0.078]\n",
      "453 [D loss: (0.649)(R 0.568, F 0.730)] [D acc: (0.586)(0.625, 0.547)] [G loss: 0.894] [G acc: 0.250]\n",
      "454 [D loss: (0.672)(R 0.593, F 0.751)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.954] [G acc: 0.078]\n",
      "455 [D loss: (0.659)(R 0.669, F 0.649)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.956] [G acc: 0.156]\n",
      "456 [D loss: (0.662)(R 0.700, F 0.624)] [D acc: (0.594)(0.469, 0.719)] [G loss: 0.932] [G acc: 0.156]\n",
      "457 [D loss: (0.637)(R 0.626, F 0.649)] [D acc: (0.586)(0.578, 0.594)] [G loss: 0.890] [G acc: 0.234]\n",
      "458 [D loss: (0.640)(R 0.636, F 0.644)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.957] [G acc: 0.078]\n",
      "459 [D loss: (0.663)(R 0.692, F 0.633)] [D acc: (0.602)(0.484, 0.719)] [G loss: 0.899] [G acc: 0.141]\n",
      "460 [D loss: (0.624)(R 0.570, F 0.677)] [D acc: (0.664)(0.719, 0.609)] [G loss: 0.929] [G acc: 0.250]\n",
      "461 [D loss: (0.622)(R 0.587, F 0.656)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.987] [G acc: 0.141]\n",
      "462 [D loss: (0.644)(R 0.621, F 0.666)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.859] [G acc: 0.203]\n",
      "463 [D loss: (0.598)(R 0.569, F 0.627)] [D acc: (0.703)(0.719, 0.688)] [G loss: 0.894] [G acc: 0.203]\n",
      "464 [D loss: (0.609)(R 0.649, F 0.570)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.172] [G acc: 0.016]\n",
      "465 [D loss: (0.612)(R 0.579, F 0.645)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.085] [G acc: 0.047]\n",
      "466 [D loss: (0.665)(R 0.606, F 0.725)] [D acc: (0.617)(0.656, 0.578)] [G loss: 0.940] [G acc: 0.094]\n",
      "467 [D loss: (0.639)(R 0.557, F 0.721)] [D acc: (0.625)(0.672, 0.578)] [G loss: 0.961] [G acc: 0.203]\n",
      "468 [D loss: (0.617)(R 0.588, F 0.646)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.962] [G acc: 0.156]\n",
      "469 [D loss: (0.621)(R 0.557, F 0.685)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.966] [G acc: 0.094]\n",
      "470 [D loss: (0.625)(R 0.635, F 0.615)] [D acc: (0.602)(0.500, 0.703)] [G loss: 0.926] [G acc: 0.172]\n",
      "471 [D loss: (0.611)(R 0.582, F 0.640)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.971] [G acc: 0.188]\n",
      "472 [D loss: (0.625)(R 0.646, F 0.604)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.069] [G acc: 0.094]\n",
      "473 [D loss: (0.608)(R 0.587, F 0.628)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.007] [G acc: 0.156]\n",
      "474 [D loss: (0.631)(R 0.553, F 0.709)] [D acc: (0.586)(0.625, 0.547)] [G loss: 0.943] [G acc: 0.188]\n",
      "475 [D loss: (0.654)(R 0.654, F 0.653)] [D acc: (0.586)(0.469, 0.703)] [G loss: 0.985] [G acc: 0.125]\n",
      "476 [D loss: (0.686)(R 0.690, F 0.681)] [D acc: (0.586)(0.516, 0.656)] [G loss: 0.985] [G acc: 0.094]\n",
      "477 [D loss: (0.628)(R 0.571, F 0.686)] [D acc: (0.680)(0.719, 0.641)] [G loss: 0.928] [G acc: 0.203]\n",
      "478 [D loss: (0.643)(R 0.616, F 0.670)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.005] [G acc: 0.109]\n",
      "479 [D loss: (0.705)(R 0.730, F 0.679)] [D acc: (0.500)(0.391, 0.609)] [G loss: 0.886] [G acc: 0.172]\n",
      "480 [D loss: (0.623)(R 0.604, F 0.642)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.031] [G acc: 0.156]\n",
      "481 [D loss: (0.612)(R 0.584, F 0.640)] [D acc: (0.602)(0.562, 0.641)] [G loss: 0.998] [G acc: 0.125]\n",
      "482 [D loss: (0.633)(R 0.688, F 0.578)] [D acc: (0.617)(0.516, 0.719)] [G loss: 0.926] [G acc: 0.234]\n",
      "483 [D loss: (0.686)(R 0.710, F 0.662)] [D acc: (0.547)(0.469, 0.625)] [G loss: 0.907] [G acc: 0.172]\n",
      "484 [D loss: (0.642)(R 0.624, F 0.660)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.926] [G acc: 0.156]\n",
      "485 [D loss: (0.629)(R 0.625, F 0.634)] [D acc: (0.617)(0.562, 0.672)] [G loss: 0.906] [G acc: 0.188]\n",
      "486 [D loss: (0.658)(R 0.590, F 0.726)] [D acc: (0.586)(0.625, 0.547)] [G loss: 0.911] [G acc: 0.156]\n",
      "487 [D loss: (0.639)(R 0.630, F 0.647)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.873] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488 [D loss: (0.607)(R 0.575, F 0.638)] [D acc: (0.703)(0.703, 0.703)] [G loss: 0.922] [G acc: 0.172]\n",
      "489 [D loss: (0.665)(R 0.562, F 0.769)] [D acc: (0.602)(0.672, 0.531)] [G loss: 0.921] [G acc: 0.172]\n",
      "490 [D loss: (0.632)(R 0.613, F 0.651)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.997] [G acc: 0.141]\n",
      "491 [D loss: (0.592)(R 0.570, F 0.614)] [D acc: (0.695)(0.672, 0.719)] [G loss: 0.927] [G acc: 0.188]\n",
      "492 [D loss: (0.641)(R 0.602, F 0.680)] [D acc: (0.594)(0.547, 0.641)] [G loss: 0.936] [G acc: 0.109]\n",
      "493 [D loss: (0.624)(R 0.531, F 0.717)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.008] [G acc: 0.125]\n",
      "494 [D loss: (0.607)(R 0.611, F 0.604)] [D acc: (0.695)(0.594, 0.797)] [G loss: 0.953] [G acc: 0.156]\n",
      "495 [D loss: (0.642)(R 0.553, F 0.731)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.979] [G acc: 0.141]\n",
      "496 [D loss: (0.609)(R 0.629, F 0.589)] [D acc: (0.672)(0.562, 0.781)] [G loss: 0.880] [G acc: 0.250]\n",
      "497 [D loss: (0.704)(R 0.520, F 0.887)] [D acc: (0.602)(0.734, 0.469)] [G loss: 0.903] [G acc: 0.234]\n",
      "498 [D loss: (0.611)(R 0.620, F 0.602)] [D acc: (0.711)(0.641, 0.781)] [G loss: 0.972] [G acc: 0.125]\n",
      "499 [D loss: (0.614)(R 0.527, F 0.701)] [D acc: (0.633)(0.719, 0.547)] [G loss: 0.900] [G acc: 0.172]\n",
      "500 [D loss: (0.625)(R 0.565, F 0.685)] [D acc: (0.672)(0.656, 0.688)] [G loss: 0.962] [G acc: 0.234]\n",
      "501 [D loss: (0.605)(R 0.545, F 0.665)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.055] [G acc: 0.078]\n",
      "502 [D loss: (0.647)(R 0.667, F 0.626)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.193] [G acc: 0.031]\n",
      "503 [D loss: (0.598)(R 0.628, F 0.567)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.047] [G acc: 0.125]\n",
      "504 [D loss: (0.661)(R 0.595, F 0.728)] [D acc: (0.672)(0.703, 0.641)] [G loss: 0.954] [G acc: 0.203]\n",
      "505 [D loss: (0.647)(R 0.699, F 0.595)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.009] [G acc: 0.078]\n",
      "506 [D loss: (0.586)(R 0.646, F 0.527)] [D acc: (0.711)(0.516, 0.906)] [G loss: 1.037] [G acc: 0.094]\n",
      "507 [D loss: (0.572)(R 0.496, F 0.649)] [D acc: (0.727)(0.812, 0.641)] [G loss: 1.076] [G acc: 0.141]\n",
      "508 [D loss: (0.673)(R 0.669, F 0.676)] [D acc: (0.586)(0.531, 0.641)] [G loss: 0.950] [G acc: 0.172]\n",
      "509 [D loss: (0.665)(R 0.585, F 0.744)] [D acc: (0.570)(0.625, 0.516)] [G loss: 0.944] [G acc: 0.141]\n",
      "510 [D loss: (0.603)(R 0.597, F 0.610)] [D acc: (0.656)(0.594, 0.719)] [G loss: 0.973] [G acc: 0.172]\n",
      "511 [D loss: (0.641)(R 0.590, F 0.692)] [D acc: (0.641)(0.609, 0.672)] [G loss: 0.932] [G acc: 0.203]\n",
      "512 [D loss: (0.635)(R 0.600, F 0.670)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.034] [G acc: 0.078]\n",
      "513 [D loss: (0.606)(R 0.522, F 0.691)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.022] [G acc: 0.125]\n",
      "514 [D loss: (0.620)(R 0.598, F 0.641)] [D acc: (0.688)(0.641, 0.734)] [G loss: 0.954] [G acc: 0.156]\n",
      "515 [D loss: (0.660)(R 0.626, F 0.693)] [D acc: (0.625)(0.562, 0.688)] [G loss: 0.976] [G acc: 0.156]\n",
      "516 [D loss: (0.668)(R 0.636, F 0.699)] [D acc: (0.531)(0.516, 0.547)] [G loss: 0.897] [G acc: 0.188]\n",
      "517 [D loss: (0.630)(R 0.606, F 0.654)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.011] [G acc: 0.109]\n",
      "518 [D loss: (0.605)(R 0.582, F 0.628)] [D acc: (0.703)(0.641, 0.766)] [G loss: 0.982] [G acc: 0.141]\n",
      "519 [D loss: (0.574)(R 0.552, F 0.596)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.074] [G acc: 0.094]\n",
      "520 [D loss: (0.617)(R 0.629, F 0.606)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.038] [G acc: 0.141]\n",
      "521 [D loss: (0.596)(R 0.617, F 0.575)] [D acc: (0.688)(0.578, 0.797)] [G loss: 0.998] [G acc: 0.172]\n",
      "522 [D loss: (0.635)(R 0.631, F 0.639)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.906] [G acc: 0.188]\n",
      "523 [D loss: (0.636)(R 0.647, F 0.626)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.915] [G acc: 0.250]\n",
      "524 [D loss: (0.617)(R 0.571, F 0.663)] [D acc: (0.688)(0.641, 0.734)] [G loss: 0.994] [G acc: 0.109]\n",
      "525 [D loss: (0.594)(R 0.603, F 0.586)] [D acc: (0.719)(0.625, 0.812)] [G loss: 0.887] [G acc: 0.219]\n",
      "526 [D loss: (0.626)(R 0.509, F 0.743)] [D acc: (0.609)(0.719, 0.500)] [G loss: 0.992] [G acc: 0.125]\n",
      "527 [D loss: (0.548)(R 0.502, F 0.594)] [D acc: (0.789)(0.750, 0.828)] [G loss: 0.995] [G acc: 0.219]\n",
      "528 [D loss: (0.655)(R 0.570, F 0.740)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.991] [G acc: 0.172]\n",
      "529 [D loss: (0.669)(R 0.645, F 0.693)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.872] [G acc: 0.234]\n",
      "530 [D loss: (0.593)(R 0.525, F 0.661)] [D acc: (0.695)(0.781, 0.609)] [G loss: 0.959] [G acc: 0.203]\n",
      "531 [D loss: (0.628)(R 0.628, F 0.628)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.041] [G acc: 0.094]\n",
      "532 [D loss: (0.563)(R 0.496, F 0.630)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.063] [G acc: 0.125]\n",
      "533 [D loss: (0.649)(R 0.552, F 0.747)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.928] [G acc: 0.219]\n",
      "534 [D loss: (0.646)(R 0.600, F 0.692)] [D acc: (0.672)(0.609, 0.734)] [G loss: 0.958] [G acc: 0.203]\n",
      "535 [D loss: (0.607)(R 0.598, F 0.616)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.001] [G acc: 0.094]\n",
      "536 [D loss: (0.629)(R 0.679, F 0.580)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.006] [G acc: 0.156]\n",
      "537 [D loss: (0.595)(R 0.492, F 0.697)] [D acc: (0.648)(0.719, 0.578)] [G loss: 0.966] [G acc: 0.188]\n",
      "538 [D loss: (0.615)(R 0.568, F 0.662)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.113] [G acc: 0.141]\n",
      "539 [D loss: (0.564)(R 0.539, F 0.588)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.112] [G acc: 0.125]\n",
      "540 [D loss: (0.684)(R 0.670, F 0.698)] [D acc: (0.586)(0.531, 0.641)] [G loss: 0.980] [G acc: 0.109]\n",
      "541 [D loss: (0.568)(R 0.572, F 0.565)] [D acc: (0.688)(0.703, 0.672)] [G loss: 0.936] [G acc: 0.250]\n",
      "542 [D loss: (0.672)(R 0.477, F 0.867)] [D acc: (0.562)(0.688, 0.438)] [G loss: 1.128] [G acc: 0.125]\n",
      "543 [D loss: (0.601)(R 0.601, F 0.600)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.082] [G acc: 0.094]\n",
      "544 [D loss: (0.622)(R 0.589, F 0.655)] [D acc: (0.680)(0.625, 0.734)] [G loss: 0.968] [G acc: 0.172]\n",
      "545 [D loss: (0.659)(R 0.647, F 0.670)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.106] [G acc: 0.062]\n",
      "546 [D loss: (0.637)(R 0.609, F 0.666)] [D acc: (0.617)(0.609, 0.625)] [G loss: 0.993] [G acc: 0.156]\n",
      "547 [D loss: (0.658)(R 0.658, F 0.658)] [D acc: (0.602)(0.562, 0.641)] [G loss: 0.915] [G acc: 0.219]\n",
      "548 [D loss: (0.602)(R 0.583, F 0.621)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.069] [G acc: 0.094]\n",
      "549 [D loss: (0.645)(R 0.633, F 0.658)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.053] [G acc: 0.094]\n",
      "550 [D loss: (0.649)(R 0.648, F 0.651)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.052] [G acc: 0.141]\n",
      "551 [D loss: (0.581)(R 0.582, F 0.580)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.049] [G acc: 0.078]\n",
      "552 [D loss: (0.646)(R 0.622, F 0.671)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.149] [G acc: 0.031]\n",
      "553 [D loss: (0.608)(R 0.623, F 0.594)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.085] [G acc: 0.078]\n",
      "554 [D loss: (0.607)(R 0.569, F 0.645)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.102] [G acc: 0.047]\n",
      "555 [D loss: (0.669)(R 0.645, F 0.692)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.068] [G acc: 0.094]\n",
      "556 [D loss: (0.643)(R 0.645, F 0.641)] [D acc: (0.633)(0.578, 0.688)] [G loss: 0.933] [G acc: 0.234]\n",
      "557 [D loss: (0.659)(R 0.712, F 0.605)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.021] [G acc: 0.094]\n",
      "558 [D loss: (0.626)(R 0.674, F 0.577)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.095] [G acc: 0.109]\n",
      "559 [D loss: (0.614)(R 0.555, F 0.672)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.021] [G acc: 0.094]\n",
      "560 [D loss: (0.592)(R 0.614, F 0.571)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.050] [G acc: 0.203]\n",
      "561 [D loss: (0.614)(R 0.661, F 0.567)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.108] [G acc: 0.172]\n",
      "562 [D loss: (0.534)(R 0.486, F 0.583)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.029] [G acc: 0.234]\n",
      "563 [D loss: (0.632)(R 0.548, F 0.717)] [D acc: (0.688)(0.719, 0.656)] [G loss: 0.960] [G acc: 0.250]\n",
      "564 [D loss: (0.638)(R 0.553, F 0.723)] [D acc: (0.633)(0.656, 0.609)] [G loss: 1.007] [G acc: 0.219]\n",
      "565 [D loss: (0.673)(R 0.649, F 0.697)] [D acc: (0.586)(0.578, 0.594)] [G loss: 0.986] [G acc: 0.234]\n",
      "566 [D loss: (0.751)(R 0.675, F 0.827)] [D acc: (0.555)(0.562, 0.547)] [G loss: 0.989] [G acc: 0.141]\n",
      "567 [D loss: (0.635)(R 0.642, F 0.629)] [D acc: (0.602)(0.562, 0.641)] [G loss: 1.000] [G acc: 0.125]\n",
      "568 [D loss: (0.624)(R 0.627, F 0.621)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.959] [G acc: 0.125]\n",
      "569 [D loss: (0.628)(R 0.620, F 0.635)] [D acc: (0.680)(0.609, 0.750)] [G loss: 0.971] [G acc: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 [D loss: (0.623)(R 0.601, F 0.645)] [D acc: (0.680)(0.656, 0.703)] [G loss: 0.974] [G acc: 0.156]\n",
      "571 [D loss: (0.661)(R 0.679, F 0.642)] [D acc: (0.578)(0.469, 0.688)] [G loss: 0.937] [G acc: 0.203]\n",
      "572 [D loss: (0.603)(R 0.588, F 0.618)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.002] [G acc: 0.141]\n",
      "573 [D loss: (0.615)(R 0.672, F 0.558)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.005] [G acc: 0.125]\n",
      "574 [D loss: (0.576)(R 0.515, F 0.637)] [D acc: (0.680)(0.703, 0.656)] [G loss: 0.990] [G acc: 0.172]\n",
      "575 [D loss: (0.692)(R 0.696, F 0.688)] [D acc: (0.531)(0.500, 0.562)] [G loss: 0.914] [G acc: 0.203]\n",
      "576 [D loss: (0.587)(R 0.590, F 0.583)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.061] [G acc: 0.109]\n",
      "577 [D loss: (0.619)(R 0.578, F 0.660)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.940] [G acc: 0.234]\n",
      "578 [D loss: (0.631)(R 0.588, F 0.673)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.985] [G acc: 0.109]\n",
      "579 [D loss: (0.613)(R 0.607, F 0.620)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.158] [G acc: 0.078]\n",
      "580 [D loss: (0.580)(R 0.632, F 0.528)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.144] [G acc: 0.094]\n",
      "581 [D loss: (0.642)(R 0.555, F 0.729)] [D acc: (0.641)(0.672, 0.609)] [G loss: 0.969] [G acc: 0.203]\n",
      "582 [D loss: (0.662)(R 0.583, F 0.741)] [D acc: (0.609)(0.625, 0.594)] [G loss: 1.037] [G acc: 0.047]\n",
      "583 [D loss: (0.683)(R 0.716, F 0.649)] [D acc: (0.586)(0.500, 0.672)] [G loss: 0.978] [G acc: 0.172]\n",
      "584 [D loss: (0.661)(R 0.660, F 0.662)] [D acc: (0.602)(0.547, 0.656)] [G loss: 0.945] [G acc: 0.266]\n",
      "585 [D loss: (0.655)(R 0.647, F 0.663)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.969] [G acc: 0.156]\n",
      "586 [D loss: (0.648)(R 0.629, F 0.666)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.915] [G acc: 0.094]\n",
      "587 [D loss: (0.623)(R 0.629, F 0.617)] [D acc: (0.664)(0.594, 0.734)] [G loss: 0.938] [G acc: 0.203]\n",
      "588 [D loss: (0.671)(R 0.650, F 0.692)] [D acc: (0.570)(0.547, 0.594)] [G loss: 0.910] [G acc: 0.219]\n",
      "589 [D loss: (0.605)(R 0.560, F 0.649)] [D acc: (0.719)(0.750, 0.688)] [G loss: 0.942] [G acc: 0.250]\n",
      "590 [D loss: (0.615)(R 0.611, F 0.619)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.026] [G acc: 0.125]\n",
      "591 [D loss: (0.635)(R 0.671, F 0.599)] [D acc: (0.672)(0.578, 0.766)] [G loss: 0.944] [G acc: 0.188]\n",
      "592 [D loss: (0.661)(R 0.597, F 0.725)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.924] [G acc: 0.219]\n",
      "593 [D loss: (0.683)(R 0.644, F 0.722)] [D acc: (0.609)(0.547, 0.672)] [G loss: 0.992] [G acc: 0.125]\n",
      "594 [D loss: (0.599)(R 0.613, F 0.584)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.002] [G acc: 0.125]\n",
      "595 [D loss: (0.577)(R 0.539, F 0.615)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.038] [G acc: 0.188]\n",
      "596 [D loss: (0.613)(R 0.630, F 0.596)] [D acc: (0.703)(0.625, 0.781)] [G loss: 0.938] [G acc: 0.109]\n",
      "597 [D loss: (0.674)(R 0.544, F 0.804)] [D acc: (0.617)(0.688, 0.547)] [G loss: 1.035] [G acc: 0.094]\n",
      "598 [D loss: (0.608)(R 0.658, F 0.557)] [D acc: (0.734)(0.594, 0.875)] [G loss: 0.998] [G acc: 0.125]\n",
      "599 [D loss: (0.664)(R 0.613, F 0.715)] [D acc: (0.602)(0.578, 0.625)] [G loss: 1.018] [G acc: 0.188]\n",
      "600 [D loss: (0.606)(R 0.602, F 0.610)] [D acc: (0.633)(0.531, 0.734)] [G loss: 0.985] [G acc: 0.219]\n",
      "601 [D loss: (0.644)(R 0.585, F 0.704)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.051] [G acc: 0.141]\n",
      "602 [D loss: (0.646)(R 0.696, F 0.595)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.029] [G acc: 0.188]\n",
      "603 [D loss: (0.632)(R 0.681, F 0.583)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.035] [G acc: 0.094]\n",
      "604 [D loss: (0.648)(R 0.596, F 0.700)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.966] [G acc: 0.141]\n",
      "605 [D loss: (0.631)(R 0.605, F 0.657)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.972] [G acc: 0.156]\n",
      "606 [D loss: (0.632)(R 0.556, F 0.708)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.983] [G acc: 0.172]\n",
      "607 [D loss: (0.594)(R 0.619, F 0.568)] [D acc: (0.695)(0.625, 0.766)] [G loss: 0.887] [G acc: 0.297]\n",
      "608 [D loss: (0.642)(R 0.529, F 0.754)] [D acc: (0.633)(0.688, 0.578)] [G loss: 1.142] [G acc: 0.125]\n",
      "609 [D loss: (0.638)(R 0.724, F 0.552)] [D acc: (0.617)(0.453, 0.781)] [G loss: 0.953] [G acc: 0.188]\n",
      "610 [D loss: (0.666)(R 0.554, F 0.779)] [D acc: (0.633)(0.688, 0.578)] [G loss: 1.006] [G acc: 0.172]\n",
      "611 [D loss: (0.642)(R 0.599, F 0.684)] [D acc: (0.648)(0.656, 0.641)] [G loss: 0.956] [G acc: 0.172]\n",
      "612 [D loss: (0.650)(R 0.530, F 0.771)] [D acc: (0.609)(0.719, 0.500)] [G loss: 0.970] [G acc: 0.109]\n",
      "613 [D loss: (0.646)(R 0.602, F 0.690)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.971] [G acc: 0.125]\n",
      "614 [D loss: (0.630)(R 0.678, F 0.581)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.066] [G acc: 0.125]\n",
      "615 [D loss: (0.649)(R 0.637, F 0.660)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.952] [G acc: 0.219]\n",
      "616 [D loss: (0.580)(R 0.558, F 0.601)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.030] [G acc: 0.109]\n",
      "617 [D loss: (0.672)(R 0.683, F 0.661)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.034] [G acc: 0.172]\n",
      "618 [D loss: (0.656)(R 0.624, F 0.688)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.963] [G acc: 0.172]\n",
      "619 [D loss: (0.582)(R 0.599, F 0.565)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.100] [G acc: 0.172]\n",
      "620 [D loss: (0.622)(R 0.558, F 0.686)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.071] [G acc: 0.156]\n",
      "621 [D loss: (0.634)(R 0.740, F 0.528)] [D acc: (0.609)(0.391, 0.828)] [G loss: 0.989] [G acc: 0.219]\n",
      "622 [D loss: (0.613)(R 0.614, F 0.612)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.010] [G acc: 0.203]\n",
      "623 [D loss: (0.673)(R 0.575, F 0.770)] [D acc: (0.625)(0.672, 0.578)] [G loss: 1.024] [G acc: 0.188]\n",
      "624 [D loss: (0.631)(R 0.633, F 0.629)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.038] [G acc: 0.094]\n",
      "625 [D loss: (0.640)(R 0.599, F 0.682)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.018] [G acc: 0.172]\n",
      "626 [D loss: (0.634)(R 0.669, F 0.598)] [D acc: (0.656)(0.547, 0.766)] [G loss: 0.978] [G acc: 0.156]\n",
      "627 [D loss: (0.658)(R 0.563, F 0.754)] [D acc: (0.555)(0.641, 0.469)] [G loss: 0.992] [G acc: 0.078]\n",
      "628 [D loss: (0.656)(R 0.657, F 0.656)] [D acc: (0.625)(0.562, 0.688)] [G loss: 0.975] [G acc: 0.062]\n",
      "629 [D loss: (0.625)(R 0.592, F 0.658)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.040] [G acc: 0.109]\n",
      "630 [D loss: (0.579)(R 0.615, F 0.542)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.011] [G acc: 0.078]\n",
      "631 [D loss: (0.592)(R 0.609, F 0.575)] [D acc: (0.703)(0.594, 0.812)] [G loss: 0.994] [G acc: 0.109]\n",
      "632 [D loss: (0.630)(R 0.565, F 0.694)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.139] [G acc: 0.094]\n",
      "633 [D loss: (0.640)(R 0.696, F 0.584)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.048] [G acc: 0.125]\n",
      "634 [D loss: (0.598)(R 0.597, F 0.599)] [D acc: (0.664)(0.578, 0.750)] [G loss: 0.995] [G acc: 0.172]\n",
      "635 [D loss: (0.605)(R 0.600, F 0.609)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.135] [G acc: 0.234]\n",
      "636 [D loss: (0.636)(R 0.617, F 0.655)] [D acc: (0.680)(0.672, 0.688)] [G loss: 0.880] [G acc: 0.266]\n",
      "637 [D loss: (0.612)(R 0.605, F 0.619)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.941] [G acc: 0.203]\n",
      "638 [D loss: (0.628)(R 0.461, F 0.795)] [D acc: (0.727)(0.844, 0.609)] [G loss: 1.045] [G acc: 0.094]\n",
      "639 [D loss: (0.682)(R 0.637, F 0.728)] [D acc: (0.578)(0.516, 0.641)] [G loss: 0.998] [G acc: 0.172]\n",
      "640 [D loss: (0.606)(R 0.606, F 0.605)] [D acc: (0.633)(0.594, 0.672)] [G loss: 0.972] [G acc: 0.203]\n",
      "641 [D loss: (0.598)(R 0.618, F 0.579)] [D acc: (0.664)(0.547, 0.781)] [G loss: 0.991] [G acc: 0.156]\n",
      "642 [D loss: (0.616)(R 0.612, F 0.619)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.054] [G acc: 0.109]\n",
      "643 [D loss: (0.656)(R 0.538, F 0.775)] [D acc: (0.617)(0.688, 0.547)] [G loss: 0.962] [G acc: 0.250]\n",
      "644 [D loss: (0.723)(R 0.686, F 0.761)] [D acc: (0.531)(0.531, 0.531)] [G loss: 0.989] [G acc: 0.156]\n",
      "645 [D loss: (0.678)(R 0.759, F 0.597)] [D acc: (0.586)(0.438, 0.734)] [G loss: 0.987] [G acc: 0.125]\n",
      "646 [D loss: (0.624)(R 0.619, F 0.628)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.948] [G acc: 0.125]\n",
      "647 [D loss: (0.623)(R 0.606, F 0.639)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.022] [G acc: 0.094]\n",
      "648 [D loss: (0.639)(R 0.602, F 0.677)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.995] [G acc: 0.094]\n",
      "649 [D loss: (0.633)(R 0.630, F 0.637)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.017] [G acc: 0.125]\n",
      "650 [D loss: (0.598)(R 0.551, F 0.645)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.000] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651 [D loss: (0.666)(R 0.562, F 0.770)] [D acc: (0.602)(0.641, 0.562)] [G loss: 1.061] [G acc: 0.094]\n",
      "652 [D loss: (0.664)(R 0.749, F 0.580)] [D acc: (0.586)(0.406, 0.766)] [G loss: 0.954] [G acc: 0.188]\n",
      "653 [D loss: (0.656)(R 0.637, F 0.674)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.928] [G acc: 0.172]\n",
      "654 [D loss: (0.598)(R 0.614, F 0.581)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.006] [G acc: 0.125]\n",
      "655 [D loss: (0.608)(R 0.611, F 0.606)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.025] [G acc: 0.125]\n",
      "656 [D loss: (0.647)(R 0.528, F 0.766)] [D acc: (0.641)(0.719, 0.562)] [G loss: 1.021] [G acc: 0.094]\n",
      "657 [D loss: (0.631)(R 0.663, F 0.599)] [D acc: (0.664)(0.578, 0.750)] [G loss: 0.986] [G acc: 0.094]\n",
      "658 [D loss: (0.638)(R 0.610, F 0.666)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.990] [G acc: 0.125]\n",
      "659 [D loss: (0.591)(R 0.599, F 0.583)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.007] [G acc: 0.125]\n",
      "660 [D loss: (0.582)(R 0.577, F 0.587)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.044] [G acc: 0.156]\n",
      "661 [D loss: (0.605)(R 0.547, F 0.662)] [D acc: (0.602)(0.672, 0.531)] [G loss: 1.021] [G acc: 0.172]\n",
      "662 [D loss: (0.691)(R 0.607, F 0.774)] [D acc: (0.578)(0.609, 0.547)] [G loss: 1.181] [G acc: 0.062]\n",
      "663 [D loss: (0.690)(R 0.775, F 0.605)] [D acc: (0.586)(0.469, 0.703)] [G loss: 1.038] [G acc: 0.062]\n",
      "664 [D loss: (0.652)(R 0.612, F 0.692)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.949] [G acc: 0.234]\n",
      "665 [D loss: (0.603)(R 0.553, F 0.654)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.009] [G acc: 0.141]\n",
      "666 [D loss: (0.600)(R 0.607, F 0.594)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.022] [G acc: 0.141]\n",
      "667 [D loss: (0.655)(R 0.673, F 0.638)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.028] [G acc: 0.125]\n",
      "668 [D loss: (0.631)(R 0.611, F 0.651)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.101] [G acc: 0.062]\n",
      "669 [D loss: (0.618)(R 0.606, F 0.629)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.989] [G acc: 0.094]\n",
      "670 [D loss: (0.620)(R 0.693, F 0.548)] [D acc: (0.695)(0.531, 0.859)] [G loss: 0.971] [G acc: 0.156]\n",
      "671 [D loss: (0.628)(R 0.593, F 0.664)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.027] [G acc: 0.156]\n",
      "672 [D loss: (0.641)(R 0.591, F 0.690)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.976] [G acc: 0.125]\n",
      "673 [D loss: (0.608)(R 0.513, F 0.703)] [D acc: (0.656)(0.734, 0.578)] [G loss: 1.000] [G acc: 0.266]\n",
      "674 [D loss: (0.607)(R 0.637, F 0.577)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.041] [G acc: 0.141]\n",
      "675 [D loss: (0.683)(R 0.618, F 0.748)] [D acc: (0.594)(0.609, 0.578)] [G loss: 0.929] [G acc: 0.203]\n",
      "676 [D loss: (0.607)(R 0.566, F 0.647)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.952] [G acc: 0.203]\n",
      "677 [D loss: (0.659)(R 0.697, F 0.621)] [D acc: (0.625)(0.500, 0.750)] [G loss: 0.986] [G acc: 0.188]\n",
      "678 [D loss: (0.619)(R 0.643, F 0.596)] [D acc: (0.688)(0.578, 0.797)] [G loss: 0.966] [G acc: 0.141]\n",
      "679 [D loss: (0.645)(R 0.663, F 0.626)] [D acc: (0.672)(0.578, 0.766)] [G loss: 0.898] [G acc: 0.250]\n",
      "680 [D loss: (0.618)(R 0.518, F 0.717)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.933] [G acc: 0.141]\n",
      "681 [D loss: (0.662)(R 0.619, F 0.706)] [D acc: (0.562)(0.531, 0.594)] [G loss: 0.838] [G acc: 0.266]\n",
      "682 [D loss: (0.683)(R 0.630, F 0.737)] [D acc: (0.586)(0.547, 0.625)] [G loss: 0.941] [G acc: 0.172]\n",
      "683 [D loss: (0.602)(R 0.618, F 0.586)] [D acc: (0.695)(0.578, 0.812)] [G loss: 0.935] [G acc: 0.219]\n",
      "684 [D loss: (0.602)(R 0.622, F 0.582)] [D acc: (0.688)(0.578, 0.797)] [G loss: 0.966] [G acc: 0.188]\n",
      "685 [D loss: (0.653)(R 0.657, F 0.648)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.117] [G acc: 0.094]\n",
      "686 [D loss: (0.618)(R 0.613, F 0.622)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.070] [G acc: 0.109]\n",
      "687 [D loss: (0.594)(R 0.646, F 0.543)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.003] [G acc: 0.141]\n",
      "688 [D loss: (0.676)(R 0.527, F 0.825)] [D acc: (0.633)(0.703, 0.562)] [G loss: 0.931] [G acc: 0.234]\n",
      "689 [D loss: (0.648)(R 0.619, F 0.676)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.913] [G acc: 0.188]\n",
      "690 [D loss: (0.622)(R 0.668, F 0.577)] [D acc: (0.602)(0.484, 0.719)] [G loss: 0.958] [G acc: 0.203]\n",
      "691 [D loss: (0.648)(R 0.692, F 0.605)] [D acc: (0.594)(0.484, 0.703)] [G loss: 0.914] [G acc: 0.156]\n",
      "692 [D loss: (0.629)(R 0.615, F 0.643)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.954] [G acc: 0.219]\n",
      "693 [D loss: (0.595)(R 0.571, F 0.619)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.914] [G acc: 0.172]\n",
      "694 [D loss: (0.625)(R 0.604, F 0.645)] [D acc: (0.703)(0.672, 0.734)] [G loss: 0.905] [G acc: 0.250]\n",
      "695 [D loss: (0.733)(R 0.630, F 0.836)] [D acc: (0.570)(0.562, 0.578)] [G loss: 1.008] [G acc: 0.141]\n",
      "696 [D loss: (0.648)(R 0.635, F 0.661)] [D acc: (0.594)(0.562, 0.625)] [G loss: 0.902] [G acc: 0.188]\n",
      "697 [D loss: (0.624)(R 0.571, F 0.678)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.965] [G acc: 0.188]\n",
      "698 [D loss: (0.650)(R 0.641, F 0.658)] [D acc: (0.578)(0.516, 0.641)] [G loss: 1.049] [G acc: 0.094]\n",
      "699 [D loss: (0.640)(R 0.616, F 0.663)] [D acc: (0.617)(0.641, 0.594)] [G loss: 0.919] [G acc: 0.250]\n",
      "700 [D loss: (0.618)(R 0.533, F 0.703)] [D acc: (0.719)(0.750, 0.688)] [G loss: 0.985] [G acc: 0.125]\n",
      "701 [D loss: (0.644)(R 0.596, F 0.693)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.927] [G acc: 0.250]\n",
      "702 [D loss: (0.601)(R 0.578, F 0.624)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.076] [G acc: 0.172]\n",
      "703 [D loss: (0.606)(R 0.646, F 0.566)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.114] [G acc: 0.047]\n",
      "704 [D loss: (0.604)(R 0.579, F 0.629)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.003] [G acc: 0.188]\n",
      "705 [D loss: (0.607)(R 0.562, F 0.652)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.928] [G acc: 0.266]\n",
      "706 [D loss: (0.581)(R 0.606, F 0.557)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.067] [G acc: 0.141]\n",
      "707 [D loss: (0.631)(R 0.558, F 0.704)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.967] [G acc: 0.188]\n",
      "708 [D loss: (0.567)(R 0.532, F 0.602)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.115] [G acc: 0.141]\n",
      "709 [D loss: (0.627)(R 0.569, F 0.685)] [D acc: (0.703)(0.703, 0.703)] [G loss: 0.981] [G acc: 0.188]\n",
      "710 [D loss: (0.579)(R 0.550, F 0.609)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.075] [G acc: 0.156]\n",
      "711 [D loss: (0.618)(R 0.620, F 0.616)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.169] [G acc: 0.047]\n",
      "712 [D loss: (0.629)(R 0.578, F 0.679)] [D acc: (0.641)(0.688, 0.594)] [G loss: 1.069] [G acc: 0.094]\n",
      "713 [D loss: (0.653)(R 0.637, F 0.669)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.979] [G acc: 0.266]\n",
      "714 [D loss: (0.647)(R 0.683, F 0.612)] [D acc: (0.609)(0.531, 0.688)] [G loss: 0.967] [G acc: 0.250]\n",
      "715 [D loss: (0.634)(R 0.587, F 0.681)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.037] [G acc: 0.156]\n",
      "716 [D loss: (0.580)(R 0.637, F 0.524)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.007] [G acc: 0.109]\n",
      "717 [D loss: (0.618)(R 0.567, F 0.670)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.010] [G acc: 0.250]\n",
      "718 [D loss: (0.680)(R 0.659, F 0.700)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.020] [G acc: 0.109]\n",
      "719 [D loss: (0.671)(R 0.748, F 0.594)] [D acc: (0.578)(0.438, 0.719)] [G loss: 1.003] [G acc: 0.188]\n",
      "720 [D loss: (0.602)(R 0.610, F 0.595)] [D acc: (0.656)(0.578, 0.734)] [G loss: 0.981] [G acc: 0.172]\n",
      "721 [D loss: (0.619)(R 0.560, F 0.677)] [D acc: (0.695)(0.656, 0.734)] [G loss: 0.955] [G acc: 0.172]\n",
      "722 [D loss: (0.630)(R 0.614, F 0.647)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.957] [G acc: 0.219]\n",
      "723 [D loss: (0.576)(R 0.595, F 0.557)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.013] [G acc: 0.125]\n",
      "724 [D loss: (0.618)(R 0.591, F 0.644)] [D acc: (0.625)(0.609, 0.641)] [G loss: 1.030] [G acc: 0.156]\n",
      "725 [D loss: (0.633)(R 0.614, F 0.652)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.000] [G acc: 0.203]\n",
      "726 [D loss: (0.601)(R 0.575, F 0.628)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.130] [G acc: 0.047]\n",
      "727 [D loss: (0.611)(R 0.578, F 0.644)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.062] [G acc: 0.062]\n",
      "728 [D loss: (0.668)(R 0.694, F 0.643)] [D acc: (0.617)(0.516, 0.719)] [G loss: 1.030] [G acc: 0.141]\n",
      "729 [D loss: (0.667)(R 0.697, F 0.636)] [D acc: (0.555)(0.484, 0.625)] [G loss: 1.041] [G acc: 0.109]\n",
      "730 [D loss: (0.653)(R 0.661, F 0.645)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.958] [G acc: 0.156]\n",
      "731 [D loss: (0.682)(R 0.643, F 0.720)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.991] [G acc: 0.172]\n",
      "732 [D loss: (0.676)(R 0.652, F 0.701)] [D acc: (0.555)(0.516, 0.594)] [G loss: 0.897] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733 [D loss: (0.615)(R 0.618, F 0.612)] [D acc: (0.680)(0.594, 0.766)] [G loss: 0.911] [G acc: 0.219]\n",
      "734 [D loss: (0.615)(R 0.608, F 0.622)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.964] [G acc: 0.109]\n",
      "735 [D loss: (0.628)(R 0.578, F 0.678)] [D acc: (0.578)(0.562, 0.594)] [G loss: 0.970] [G acc: 0.203]\n",
      "736 [D loss: (0.639)(R 0.635, F 0.643)] [D acc: (0.617)(0.578, 0.656)] [G loss: 0.995] [G acc: 0.078]\n",
      "737 [D loss: (0.658)(R 0.627, F 0.688)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.027] [G acc: 0.125]\n",
      "738 [D loss: (0.633)(R 0.648, F 0.618)] [D acc: (0.617)(0.531, 0.703)] [G loss: 1.020] [G acc: 0.094]\n",
      "739 [D loss: (0.627)(R 0.631, F 0.622)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.056] [G acc: 0.109]\n",
      "740 [D loss: (0.687)(R 0.652, F 0.722)] [D acc: (0.562)(0.578, 0.547)] [G loss: 1.066] [G acc: 0.031]\n",
      "741 [D loss: (0.589)(R 0.616, F 0.562)] [D acc: (0.688)(0.625, 0.750)] [G loss: 0.997] [G acc: 0.047]\n",
      "742 [D loss: (0.591)(R 0.607, F 0.574)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.052] [G acc: 0.141]\n",
      "743 [D loss: (0.573)(R 0.592, F 0.553)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.034] [G acc: 0.062]\n",
      "744 [D loss: (0.725)(R 0.596, F 0.853)] [D acc: (0.539)(0.562, 0.516)] [G loss: 0.981] [G acc: 0.156]\n",
      "745 [D loss: (0.573)(R 0.561, F 0.585)] [D acc: (0.695)(0.625, 0.766)] [G loss: 0.990] [G acc: 0.219]\n",
      "746 [D loss: (0.566)(R 0.555, F 0.576)] [D acc: (0.680)(0.625, 0.734)] [G loss: 0.979] [G acc: 0.125]\n",
      "747 [D loss: (0.632)(R 0.545, F 0.719)] [D acc: (0.680)(0.734, 0.625)] [G loss: 1.254] [G acc: 0.047]\n",
      "748 [D loss: (0.513)(R 0.607, F 0.420)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.324] [G acc: 0.094]\n",
      "749 [D loss: (0.725)(R 0.603, F 0.848)] [D acc: (0.555)(0.625, 0.484)] [G loss: 0.975] [G acc: 0.172]\n",
      "750 [D loss: (0.597)(R 0.551, F 0.643)] [D acc: (0.664)(0.656, 0.672)] [G loss: 0.993] [G acc: 0.203]\n",
      "751 [D loss: (0.653)(R 0.560, F 0.746)] [D acc: (0.648)(0.719, 0.578)] [G loss: 1.034] [G acc: 0.094]\n",
      "752 [D loss: (0.576)(R 0.605, F 0.547)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.024] [G acc: 0.172]\n",
      "753 [D loss: (0.633)(R 0.617, F 0.649)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.050] [G acc: 0.188]\n",
      "754 [D loss: (0.593)(R 0.599, F 0.587)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.125] [G acc: 0.156]\n",
      "755 [D loss: (0.677)(R 0.710, F 0.643)] [D acc: (0.586)(0.516, 0.656)] [G loss: 1.077] [G acc: 0.172]\n",
      "756 [D loss: (0.673)(R 0.688, F 0.659)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.063] [G acc: 0.062]\n",
      "757 [D loss: (0.646)(R 0.717, F 0.575)] [D acc: (0.641)(0.469, 0.812)] [G loss: 1.029] [G acc: 0.094]\n",
      "758 [D loss: (0.561)(R 0.550, F 0.573)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.019] [G acc: 0.203]\n",
      "759 [D loss: (0.620)(R 0.556, F 0.684)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.019] [G acc: 0.125]\n",
      "760 [D loss: (0.636)(R 0.693, F 0.579)] [D acc: (0.609)(0.469, 0.750)] [G loss: 0.981] [G acc: 0.141]\n",
      "761 [D loss: (0.612)(R 0.624, F 0.599)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.017] [G acc: 0.156]\n",
      "762 [D loss: (0.586)(R 0.559, F 0.613)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.033] [G acc: 0.125]\n",
      "763 [D loss: (0.639)(R 0.603, F 0.675)] [D acc: (0.594)(0.609, 0.578)] [G loss: 1.032] [G acc: 0.156]\n",
      "764 [D loss: (0.642)(R 0.578, F 0.706)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.110] [G acc: 0.047]\n",
      "765 [D loss: (0.601)(R 0.611, F 0.591)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.070] [G acc: 0.141]\n",
      "766 [D loss: (0.590)(R 0.585, F 0.595)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.125] [G acc: 0.141]\n",
      "767 [D loss: (0.662)(R 0.682, F 0.641)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.059] [G acc: 0.188]\n",
      "768 [D loss: (0.620)(R 0.637, F 0.603)] [D acc: (0.680)(0.609, 0.750)] [G loss: 0.990] [G acc: 0.234]\n",
      "769 [D loss: (0.709)(R 0.725, F 0.694)] [D acc: (0.523)(0.469, 0.578)] [G loss: 0.994] [G acc: 0.141]\n",
      "770 [D loss: (0.595)(R 0.566, F 0.625)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.994] [G acc: 0.172]\n",
      "771 [D loss: (0.622)(R 0.612, F 0.633)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.981] [G acc: 0.281]\n",
      "772 [D loss: (0.609)(R 0.565, F 0.653)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.918] [G acc: 0.172]\n",
      "773 [D loss: (0.622)(R 0.616, F 0.628)] [D acc: (0.617)(0.594, 0.641)] [G loss: 0.979] [G acc: 0.203]\n",
      "774 [D loss: (0.555)(R 0.500, F 0.609)] [D acc: (0.766)(0.812, 0.719)] [G loss: 1.002] [G acc: 0.188]\n",
      "775 [D loss: (0.627)(R 0.580, F 0.674)] [D acc: (0.664)(0.672, 0.656)] [G loss: 0.974] [G acc: 0.188]\n",
      "776 [D loss: (0.598)(R 0.586, F 0.610)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.018] [G acc: 0.125]\n",
      "777 [D loss: (0.646)(R 0.652, F 0.640)] [D acc: (0.578)(0.547, 0.609)] [G loss: 1.010] [G acc: 0.188]\n",
      "778 [D loss: (0.649)(R 0.584, F 0.714)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.011] [G acc: 0.141]\n",
      "779 [D loss: (0.567)(R 0.581, F 0.553)] [D acc: (0.758)(0.703, 0.812)] [G loss: 0.981] [G acc: 0.188]\n",
      "780 [D loss: (0.527)(R 0.493, F 0.562)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.054] [G acc: 0.203]\n",
      "781 [D loss: (0.677)(R 0.591, F 0.763)] [D acc: (0.602)(0.578, 0.625)] [G loss: 1.028] [G acc: 0.172]\n",
      "782 [D loss: (0.577)(R 0.567, F 0.586)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.335] [G acc: 0.062]\n",
      "783 [D loss: (0.588)(R 0.631, F 0.545)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.306] [G acc: 0.062]\n",
      "784 [D loss: (0.579)(R 0.540, F 0.619)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.137] [G acc: 0.141]\n",
      "785 [D loss: (0.613)(R 0.608, F 0.618)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.006] [G acc: 0.172]\n",
      "786 [D loss: (0.618)(R 0.550, F 0.687)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.117] [G acc: 0.141]\n",
      "787 [D loss: (0.587)(R 0.549, F 0.625)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.174] [G acc: 0.078]\n",
      "788 [D loss: (0.598)(R 0.626, F 0.571)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.092] [G acc: 0.094]\n",
      "789 [D loss: (0.586)(R 0.572, F 0.600)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.094] [G acc: 0.203]\n",
      "790 [D loss: (0.606)(R 0.619, F 0.594)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.040] [G acc: 0.266]\n",
      "791 [D loss: (0.674)(R 0.639, F 0.709)] [D acc: (0.594)(0.578, 0.609)] [G loss: 1.064] [G acc: 0.172]\n",
      "792 [D loss: (0.657)(R 0.622, F 0.692)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.029] [G acc: 0.125]\n",
      "793 [D loss: (0.637)(R 0.661, F 0.612)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.025] [G acc: 0.094]\n",
      "794 [D loss: (0.570)(R 0.595, F 0.546)] [D acc: (0.711)(0.609, 0.812)] [G loss: 0.952] [G acc: 0.188]\n",
      "795 [D loss: (0.674)(R 0.669, F 0.679)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.035] [G acc: 0.156]\n",
      "796 [D loss: (0.540)(R 0.550, F 0.530)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.158] [G acc: 0.094]\n",
      "797 [D loss: (0.614)(R 0.604, F 0.623)] [D acc: (0.641)(0.578, 0.703)] [G loss: 0.943] [G acc: 0.266]\n",
      "798 [D loss: (0.588)(R 0.469, F 0.706)] [D acc: (0.680)(0.750, 0.609)] [G loss: 1.047] [G acc: 0.141]\n",
      "799 [D loss: (0.558)(R 0.534, F 0.582)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.237] [G acc: 0.078]\n",
      "800 [D loss: (0.564)(R 0.594, F 0.534)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.249] [G acc: 0.141]\n",
      "801 [D loss: (0.696)(R 0.700, F 0.692)] [D acc: (0.586)(0.562, 0.609)] [G loss: 1.124] [G acc: 0.094]\n",
      "802 [D loss: (0.618)(R 0.614, F 0.622)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.149] [G acc: 0.109]\n",
      "803 [D loss: (0.591)(R 0.606, F 0.576)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.139] [G acc: 0.078]\n",
      "804 [D loss: (0.682)(R 0.632, F 0.733)] [D acc: (0.602)(0.609, 0.594)] [G loss: 1.177] [G acc: 0.125]\n",
      "805 [D loss: (0.628)(R 0.694, F 0.563)] [D acc: (0.625)(0.453, 0.797)] [G loss: 1.102] [G acc: 0.109]\n",
      "806 [D loss: (0.587)(R 0.636, F 0.537)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.092] [G acc: 0.156]\n",
      "807 [D loss: (0.599)(R 0.616, F 0.582)] [D acc: (0.664)(0.594, 0.734)] [G loss: 0.954] [G acc: 0.219]\n",
      "808 [D loss: (0.673)(R 0.621, F 0.725)] [D acc: (0.594)(0.547, 0.641)] [G loss: 1.062] [G acc: 0.141]\n",
      "809 [D loss: (0.627)(R 0.605, F 0.648)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.050] [G acc: 0.156]\n",
      "810 [D loss: (0.615)(R 0.639, F 0.590)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.017] [G acc: 0.156]\n",
      "811 [D loss: (0.644)(R 0.636, F 0.652)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.027] [G acc: 0.125]\n",
      "812 [D loss: (0.581)(R 0.556, F 0.606)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.026] [G acc: 0.109]\n",
      "813 [D loss: (0.691)(R 0.640, F 0.742)] [D acc: (0.586)(0.578, 0.594)] [G loss: 1.008] [G acc: 0.125]\n",
      "814 [D loss: (0.555)(R 0.505, F 0.605)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.023] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815 [D loss: (0.618)(R 0.616, F 0.621)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.095] [G acc: 0.094]\n",
      "816 [D loss: (0.605)(R 0.571, F 0.639)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.019] [G acc: 0.203]\n",
      "817 [D loss: (0.622)(R 0.588, F 0.656)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.970] [G acc: 0.219]\n",
      "818 [D loss: (0.627)(R 0.602, F 0.652)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.045] [G acc: 0.094]\n",
      "819 [D loss: (0.580)(R 0.632, F 0.528)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.008] [G acc: 0.125]\n",
      "820 [D loss: (0.580)(R 0.537, F 0.624)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.020] [G acc: 0.156]\n",
      "821 [D loss: (0.574)(R 0.563, F 0.585)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.101] [G acc: 0.219]\n",
      "822 [D loss: (0.602)(R 0.608, F 0.597)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.046] [G acc: 0.078]\n",
      "823 [D loss: (0.624)(R 0.606, F 0.643)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.037] [G acc: 0.078]\n",
      "824 [D loss: (0.615)(R 0.504, F 0.727)] [D acc: (0.711)(0.766, 0.656)] [G loss: 1.149] [G acc: 0.047]\n",
      "825 [D loss: (0.594)(R 0.582, F 0.605)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.129] [G acc: 0.109]\n",
      "826 [D loss: (0.672)(R 0.668, F 0.675)] [D acc: (0.594)(0.531, 0.656)] [G loss: 1.037] [G acc: 0.156]\n",
      "827 [D loss: (0.579)(R 0.561, F 0.597)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.036] [G acc: 0.172]\n",
      "828 [D loss: (0.632)(R 0.483, F 0.781)] [D acc: (0.641)(0.719, 0.562)] [G loss: 1.127] [G acc: 0.078]\n",
      "829 [D loss: (0.576)(R 0.682, F 0.470)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.174] [G acc: 0.078]\n",
      "830 [D loss: (0.646)(R 0.647, F 0.644)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.012] [G acc: 0.156]\n",
      "831 [D loss: (0.636)(R 0.564, F 0.709)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.006] [G acc: 0.203]\n",
      "832 [D loss: (0.599)(R 0.556, F 0.642)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.106] [G acc: 0.141]\n",
      "833 [D loss: (0.601)(R 0.654, F 0.547)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.093] [G acc: 0.109]\n",
      "834 [D loss: (0.594)(R 0.565, F 0.622)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.114] [G acc: 0.156]\n",
      "835 [D loss: (0.621)(R 0.635, F 0.607)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.077] [G acc: 0.094]\n",
      "836 [D loss: (0.642)(R 0.619, F 0.665)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.056] [G acc: 0.094]\n",
      "837 [D loss: (0.611)(R 0.628, F 0.595)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.094] [G acc: 0.062]\n",
      "838 [D loss: (0.602)(R 0.504, F 0.700)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.010] [G acc: 0.172]\n",
      "839 [D loss: (0.542)(R 0.547, F 0.537)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.079] [G acc: 0.141]\n",
      "840 [D loss: (0.600)(R 0.623, F 0.578)] [D acc: (0.664)(0.578, 0.750)] [G loss: 0.991] [G acc: 0.219]\n",
      "841 [D loss: (0.620)(R 0.523, F 0.717)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.102] [G acc: 0.156]\n",
      "842 [D loss: (0.614)(R 0.611, F 0.617)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.083] [G acc: 0.141]\n",
      "843 [D loss: (0.639)(R 0.643, F 0.635)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.088] [G acc: 0.078]\n",
      "844 [D loss: (0.621)(R 0.645, F 0.597)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.142] [G acc: 0.109]\n",
      "845 [D loss: (0.590)(R 0.653, F 0.527)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.067] [G acc: 0.078]\n",
      "846 [D loss: (0.615)(R 0.573, F 0.658)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.028] [G acc: 0.172]\n",
      "847 [D loss: (0.606)(R 0.603, F 0.609)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.087] [G acc: 0.031]\n",
      "848 [D loss: (0.590)(R 0.495, F 0.684)] [D acc: (0.688)(0.766, 0.609)] [G loss: 1.203] [G acc: 0.125]\n",
      "849 [D loss: (0.619)(R 0.651, F 0.587)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.073] [G acc: 0.188]\n",
      "850 [D loss: (0.698)(R 0.609, F 0.787)] [D acc: (0.617)(0.641, 0.594)] [G loss: 1.057] [G acc: 0.141]\n",
      "851 [D loss: (0.583)(R 0.624, F 0.542)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.015] [G acc: 0.188]\n",
      "852 [D loss: (0.587)(R 0.570, F 0.605)] [D acc: (0.711)(0.672, 0.750)] [G loss: 0.993] [G acc: 0.250]\n",
      "853 [D loss: (0.671)(R 0.610, F 0.731)] [D acc: (0.562)(0.547, 0.578)] [G loss: 0.954] [G acc: 0.250]\n",
      "854 [D loss: (0.581)(R 0.501, F 0.660)] [D acc: (0.648)(0.688, 0.609)] [G loss: 1.087] [G acc: 0.141]\n",
      "855 [D loss: (0.635)(R 0.650, F 0.619)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.048] [G acc: 0.219]\n",
      "856 [D loss: (0.591)(R 0.586, F 0.595)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.150] [G acc: 0.141]\n",
      "857 [D loss: (0.639)(R 0.623, F 0.655)] [D acc: (0.602)(0.562, 0.641)] [G loss: 1.062] [G acc: 0.109]\n",
      "858 [D loss: (0.642)(R 0.629, F 0.655)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.003] [G acc: 0.250]\n",
      "859 [D loss: (0.612)(R 0.625, F 0.599)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.071] [G acc: 0.125]\n",
      "860 [D loss: (0.611)(R 0.612, F 0.609)] [D acc: (0.617)(0.531, 0.703)] [G loss: 1.053] [G acc: 0.219]\n",
      "861 [D loss: (0.641)(R 0.671, F 0.611)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.988] [G acc: 0.141]\n",
      "862 [D loss: (0.556)(R 0.527, F 0.585)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.082] [G acc: 0.172]\n",
      "863 [D loss: (0.621)(R 0.655, F 0.586)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.120] [G acc: 0.109]\n",
      "864 [D loss: (0.628)(R 0.649, F 0.607)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.011] [G acc: 0.125]\n",
      "865 [D loss: (0.661)(R 0.559, F 0.763)] [D acc: (0.633)(0.703, 0.562)] [G loss: 1.095] [G acc: 0.094]\n",
      "866 [D loss: (0.652)(R 0.617, F 0.686)] [D acc: (0.680)(0.594, 0.766)] [G loss: 0.974] [G acc: 0.203]\n",
      "867 [D loss: (0.581)(R 0.528, F 0.634)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.083] [G acc: 0.141]\n",
      "868 [D loss: (0.630)(R 0.683, F 0.577)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.078] [G acc: 0.109]\n",
      "869 [D loss: (0.625)(R 0.633, F 0.616)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.004] [G acc: 0.203]\n",
      "870 [D loss: (0.581)(R 0.607, F 0.555)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.023] [G acc: 0.125]\n",
      "871 [D loss: (0.608)(R 0.613, F 0.602)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.047] [G acc: 0.109]\n",
      "872 [D loss: (0.592)(R 0.561, F 0.624)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.093] [G acc: 0.141]\n",
      "873 [D loss: (0.637)(R 0.644, F 0.630)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.022] [G acc: 0.172]\n",
      "874 [D loss: (0.613)(R 0.606, F 0.621)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.042] [G acc: 0.109]\n",
      "875 [D loss: (0.583)(R 0.609, F 0.557)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.013] [G acc: 0.172]\n",
      "876 [D loss: (0.585)(R 0.594, F 0.576)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.026] [G acc: 0.125]\n",
      "877 [D loss: (0.712)(R 0.599, F 0.825)] [D acc: (0.500)(0.500, 0.500)] [G loss: 1.000] [G acc: 0.094]\n",
      "878 [D loss: (0.603)(R 0.650, F 0.556)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.014] [G acc: 0.078]\n",
      "879 [D loss: (0.551)(R 0.532, F 0.570)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.046] [G acc: 0.141]\n",
      "880 [D loss: (0.700)(R 0.665, F 0.735)] [D acc: (0.602)(0.562, 0.641)] [G loss: 1.016] [G acc: 0.125]\n",
      "881 [D loss: (0.590)(R 0.623, F 0.557)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.116] [G acc: 0.125]\n",
      "882 [D loss: (0.628)(R 0.669, F 0.586)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.017] [G acc: 0.172]\n",
      "883 [D loss: (0.608)(R 0.547, F 0.668)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.032] [G acc: 0.203]\n",
      "884 [D loss: (0.609)(R 0.626, F 0.592)] [D acc: (0.695)(0.609, 0.781)] [G loss: 0.996] [G acc: 0.156]\n",
      "885 [D loss: (0.640)(R 0.671, F 0.610)] [D acc: (0.555)(0.453, 0.656)] [G loss: 1.053] [G acc: 0.156]\n",
      "886 [D loss: (0.671)(R 0.667, F 0.674)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.113] [G acc: 0.016]\n",
      "887 [D loss: (0.593)(R 0.598, F 0.589)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.008] [G acc: 0.109]\n",
      "888 [D loss: (0.539)(R 0.513, F 0.565)] [D acc: (0.750)(0.688, 0.812)] [G loss: 0.990] [G acc: 0.188]\n",
      "889 [D loss: (0.676)(R 0.592, F 0.760)] [D acc: (0.625)(0.641, 0.609)] [G loss: 1.017] [G acc: 0.156]\n",
      "890 [D loss: (0.611)(R 0.535, F 0.687)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.146] [G acc: 0.125]\n",
      "891 [D loss: (0.591)(R 0.634, F 0.548)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.039] [G acc: 0.188]\n",
      "892 [D loss: (0.602)(R 0.597, F 0.608)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.194] [G acc: 0.141]\n",
      "893 [D loss: (0.667)(R 0.739, F 0.595)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.021] [G acc: 0.109]\n",
      "894 [D loss: (0.646)(R 0.622, F 0.670)] [D acc: (0.633)(0.609, 0.656)] [G loss: 0.973] [G acc: 0.188]\n",
      "895 [D loss: (0.656)(R 0.739, F 0.572)] [D acc: (0.648)(0.484, 0.812)] [G loss: 0.986] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896 [D loss: (0.607)(R 0.599, F 0.616)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.045] [G acc: 0.094]\n",
      "897 [D loss: (0.579)(R 0.583, F 0.575)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.008] [G acc: 0.172]\n",
      "898 [D loss: (0.591)(R 0.537, F 0.645)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.055] [G acc: 0.156]\n",
      "899 [D loss: (0.569)(R 0.517, F 0.620)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.948] [G acc: 0.266]\n",
      "900 [D loss: (0.610)(R 0.507, F 0.713)] [D acc: (0.625)(0.703, 0.547)] [G loss: 1.132] [G acc: 0.109]\n",
      "901 [D loss: (0.565)(R 0.567, F 0.563)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.064] [G acc: 0.125]\n",
      "902 [D loss: (0.615)(R 0.565, F 0.665)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.913] [G acc: 0.203]\n",
      "903 [D loss: (0.634)(R 0.594, F 0.673)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.152] [G acc: 0.109]\n",
      "904 [D loss: (0.630)(R 0.570, F 0.691)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.073] [G acc: 0.141]\n",
      "905 [D loss: (0.639)(R 0.660, F 0.617)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.037] [G acc: 0.094]\n",
      "906 [D loss: (0.670)(R 0.715, F 0.625)] [D acc: (0.594)(0.516, 0.672)] [G loss: 0.986] [G acc: 0.156]\n",
      "907 [D loss: (0.628)(R 0.587, F 0.670)] [D acc: (0.633)(0.641, 0.625)] [G loss: 1.050] [G acc: 0.109]\n",
      "908 [D loss: (0.595)(R 0.628, F 0.562)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.028] [G acc: 0.203]\n",
      "909 [D loss: (0.601)(R 0.620, F 0.583)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.143] [G acc: 0.125]\n",
      "910 [D loss: (0.608)(R 0.655, F 0.561)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.001] [G acc: 0.188]\n",
      "911 [D loss: (0.597)(R 0.506, F 0.688)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.089] [G acc: 0.141]\n",
      "912 [D loss: (0.582)(R 0.481, F 0.682)] [D acc: (0.656)(0.703, 0.609)] [G loss: 1.055] [G acc: 0.109]\n",
      "913 [D loss: (0.609)(R 0.646, F 0.573)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.063] [G acc: 0.219]\n",
      "914 [D loss: (0.583)(R 0.548, F 0.618)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.052] [G acc: 0.141]\n",
      "915 [D loss: (0.578)(R 0.515, F 0.640)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.124] [G acc: 0.141]\n",
      "916 [D loss: (0.649)(R 0.678, F 0.620)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.036] [G acc: 0.141]\n",
      "917 [D loss: (0.632)(R 0.599, F 0.665)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.040] [G acc: 0.188]\n",
      "918 [D loss: (0.518)(R 0.484, F 0.551)] [D acc: (0.797)(0.734, 0.859)] [G loss: 1.055] [G acc: 0.203]\n",
      "919 [D loss: (0.612)(R 0.632, F 0.592)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.192] [G acc: 0.141]\n",
      "920 [D loss: (0.668)(R 0.557, F 0.778)] [D acc: (0.625)(0.641, 0.609)] [G loss: 1.119] [G acc: 0.141]\n",
      "921 [D loss: (0.615)(R 0.568, F 0.663)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.034] [G acc: 0.203]\n",
      "922 [D loss: (0.554)(R 0.528, F 0.580)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.085] [G acc: 0.094]\n",
      "923 [D loss: (0.701)(R 0.697, F 0.705)] [D acc: (0.539)(0.500, 0.578)] [G loss: 1.124] [G acc: 0.141]\n",
      "924 [D loss: (0.617)(R 0.726, F 0.509)] [D acc: (0.719)(0.531, 0.906)] [G loss: 1.011] [G acc: 0.172]\n",
      "925 [D loss: (0.676)(R 0.676, F 0.676)] [D acc: (0.617)(0.531, 0.703)] [G loss: 1.006] [G acc: 0.109]\n",
      "926 [D loss: (0.583)(R 0.568, F 0.599)] [D acc: (0.688)(0.625, 0.750)] [G loss: 0.981] [G acc: 0.172]\n",
      "927 [D loss: (0.649)(R 0.629, F 0.669)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.069] [G acc: 0.156]\n",
      "928 [D loss: (0.590)(R 0.630, F 0.549)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.057] [G acc: 0.203]\n",
      "929 [D loss: (0.651)(R 0.693, F 0.609)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.027] [G acc: 0.109]\n",
      "930 [D loss: (0.624)(R 0.633, F 0.615)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.017] [G acc: 0.188]\n",
      "931 [D loss: (0.659)(R 0.592, F 0.727)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.961] [G acc: 0.172]\n",
      "932 [D loss: (0.626)(R 0.580, F 0.673)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.016] [G acc: 0.172]\n",
      "933 [D loss: (0.613)(R 0.658, F 0.567)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.044] [G acc: 0.125]\n",
      "934 [D loss: (0.577)(R 0.579, F 0.574)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.028] [G acc: 0.172]\n",
      "935 [D loss: (0.619)(R 0.573, F 0.666)] [D acc: (0.680)(0.688, 0.672)] [G loss: 0.977] [G acc: 0.172]\n",
      "936 [D loss: (0.631)(R 0.576, F 0.686)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.039] [G acc: 0.109]\n",
      "937 [D loss: (0.622)(R 0.674, F 0.570)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.087] [G acc: 0.188]\n",
      "938 [D loss: (0.672)(R 0.628, F 0.717)] [D acc: (0.602)(0.578, 0.625)] [G loss: 1.068] [G acc: 0.109]\n",
      "939 [D loss: (0.603)(R 0.614, F 0.591)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.008] [G acc: 0.125]\n",
      "940 [D loss: (0.565)(R 0.550, F 0.581)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.109] [G acc: 0.125]\n",
      "941 [D loss: (0.583)(R 0.543, F 0.622)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.159] [G acc: 0.094]\n",
      "942 [D loss: (0.527)(R 0.480, F 0.575)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.197] [G acc: 0.125]\n",
      "943 [D loss: (0.613)(R 0.592, F 0.634)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.053] [G acc: 0.062]\n",
      "944 [D loss: (0.669)(R 0.668, F 0.669)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.038] [G acc: 0.109]\n",
      "945 [D loss: (0.576)(R 0.603, F 0.549)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.092] [G acc: 0.141]\n",
      "946 [D loss: (0.578)(R 0.527, F 0.630)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.129] [G acc: 0.109]\n",
      "947 [D loss: (0.589)(R 0.589, F 0.589)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.044] [G acc: 0.125]\n",
      "948 [D loss: (0.621)(R 0.520, F 0.721)] [D acc: (0.664)(0.734, 0.594)] [G loss: 1.247] [G acc: 0.172]\n",
      "949 [D loss: (0.662)(R 0.793, F 0.531)] [D acc: (0.641)(0.453, 0.828)] [G loss: 1.075] [G acc: 0.109]\n",
      "950 [D loss: (0.568)(R 0.517, F 0.618)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.084] [G acc: 0.047]\n",
      "951 [D loss: (0.517)(R 0.478, F 0.556)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.134] [G acc: 0.156]\n",
      "952 [D loss: (0.621)(R 0.482, F 0.760)] [D acc: (0.680)(0.750, 0.609)] [G loss: 1.192] [G acc: 0.125]\n",
      "953 [D loss: (0.535)(R 0.542, F 0.528)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.168] [G acc: 0.141]\n",
      "954 [D loss: (0.662)(R 0.636, F 0.688)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.087] [G acc: 0.203]\n",
      "955 [D loss: (0.623)(R 0.642, F 0.604)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.145] [G acc: 0.109]\n",
      "956 [D loss: (0.589)(R 0.612, F 0.565)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.166] [G acc: 0.047]\n",
      "957 [D loss: (0.583)(R 0.533, F 0.634)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.150] [G acc: 0.125]\n",
      "958 [D loss: (0.566)(R 0.560, F 0.572)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.189] [G acc: 0.047]\n",
      "959 [D loss: (0.607)(R 0.634, F 0.580)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.144] [G acc: 0.094]\n",
      "960 [D loss: (0.650)(R 0.637, F 0.662)] [D acc: (0.602)(0.547, 0.656)] [G loss: 1.145] [G acc: 0.078]\n",
      "961 [D loss: (0.642)(R 0.612, F 0.672)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.169] [G acc: 0.078]\n",
      "962 [D loss: (0.639)(R 0.613, F 0.664)] [D acc: (0.609)(0.578, 0.641)] [G loss: 1.041] [G acc: 0.203]\n",
      "963 [D loss: (0.591)(R 0.558, F 0.623)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.072] [G acc: 0.125]\n",
      "964 [D loss: (0.616)(R 0.653, F 0.579)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.061] [G acc: 0.125]\n",
      "965 [D loss: (0.587)(R 0.611, F 0.563)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.087] [G acc: 0.125]\n",
      "966 [D loss: (0.581)(R 0.574, F 0.588)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.152] [G acc: 0.031]\n",
      "967 [D loss: (0.689)(R 0.652, F 0.726)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.096] [G acc: 0.047]\n",
      "968 [D loss: (0.598)(R 0.663, F 0.533)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.096] [G acc: 0.078]\n",
      "969 [D loss: (0.613)(R 0.562, F 0.665)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.096] [G acc: 0.078]\n",
      "970 [D loss: (0.612)(R 0.606, F 0.617)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.065] [G acc: 0.125]\n",
      "971 [D loss: (0.641)(R 0.617, F 0.665)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.090] [G acc: 0.094]\n",
      "972 [D loss: (0.602)(R 0.641, F 0.563)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.051] [G acc: 0.078]\n",
      "973 [D loss: (0.585)(R 0.552, F 0.618)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.051] [G acc: 0.109]\n",
      "974 [D loss: (0.626)(R 0.575, F 0.677)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.057] [G acc: 0.172]\n",
      "975 [D loss: (0.636)(R 0.626, F 0.645)] [D acc: (0.609)(0.547, 0.672)] [G loss: 1.061] [G acc: 0.141]\n",
      "976 [D loss: (0.603)(R 0.583, F 0.623)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.072] [G acc: 0.188]\n",
      "977 [D loss: (0.551)(R 0.558, F 0.544)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.132] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978 [D loss: (0.599)(R 0.526, F 0.673)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.244] [G acc: 0.125]\n",
      "979 [D loss: (0.579)(R 0.585, F 0.573)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.131] [G acc: 0.141]\n",
      "980 [D loss: (0.629)(R 0.663, F 0.595)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.102] [G acc: 0.156]\n",
      "981 [D loss: (0.563)(R 0.579, F 0.547)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.026] [G acc: 0.125]\n",
      "982 [D loss: (0.504)(R 0.436, F 0.572)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.122] [G acc: 0.188]\n",
      "983 [D loss: (0.567)(R 0.449, F 0.685)] [D acc: (0.734)(0.781, 0.688)] [G loss: 1.267] [G acc: 0.109]\n",
      "984 [D loss: (0.632)(R 0.573, F 0.690)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.148] [G acc: 0.094]\n",
      "985 [D loss: (0.556)(R 0.526, F 0.586)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.281] [G acc: 0.156]\n",
      "986 [D loss: (0.641)(R 0.582, F 0.700)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.070] [G acc: 0.234]\n",
      "987 [D loss: (0.609)(R 0.662, F 0.555)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.103] [G acc: 0.188]\n",
      "988 [D loss: (0.619)(R 0.557, F 0.680)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.115] [G acc: 0.156]\n",
      "989 [D loss: (0.621)(R 0.561, F 0.681)] [D acc: (0.602)(0.578, 0.625)] [G loss: 1.227] [G acc: 0.062]\n",
      "990 [D loss: (0.608)(R 0.629, F 0.588)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.157] [G acc: 0.047]\n",
      "991 [D loss: (0.596)(R 0.515, F 0.677)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.074] [G acc: 0.156]\n",
      "992 [D loss: (0.652)(R 0.612, F 0.693)] [D acc: (0.602)(0.562, 0.641)] [G loss: 1.236] [G acc: 0.141]\n",
      "993 [D loss: (0.553)(R 0.599, F 0.507)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.149] [G acc: 0.062]\n",
      "994 [D loss: (0.600)(R 0.622, F 0.579)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.136] [G acc: 0.031]\n",
      "995 [D loss: (0.587)(R 0.618, F 0.556)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.139] [G acc: 0.141]\n",
      "996 [D loss: (0.537)(R 0.567, F 0.507)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.176] [G acc: 0.172]\n",
      "997 [D loss: (0.529)(R 0.491, F 0.567)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.100] [G acc: 0.125]\n",
      "998 [D loss: (0.710)(R 0.622, F 0.799)] [D acc: (0.547)(0.594, 0.500)] [G loss: 1.156] [G acc: 0.156]\n",
      "999 [D loss: (0.663)(R 0.647, F 0.679)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.092] [G acc: 0.094]\n",
      "1000 [D loss: (0.629)(R 0.714, F 0.544)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.035] [G acc: 0.125]\n",
      "1001 [D loss: (0.516)(R 0.455, F 0.576)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.312] [G acc: 0.047]\n",
      "1002 [D loss: (0.558)(R 0.604, F 0.512)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.202] [G acc: 0.125]\n",
      "1003 [D loss: (0.578)(R 0.539, F 0.618)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.162] [G acc: 0.125]\n",
      "1004 [D loss: (0.573)(R 0.511, F 0.634)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.146] [G acc: 0.141]\n",
      "1005 [D loss: (0.558)(R 0.602, F 0.515)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.237] [G acc: 0.078]\n",
      "1006 [D loss: (0.665)(R 0.693, F 0.636)] [D acc: (0.570)(0.531, 0.609)] [G loss: 1.189] [G acc: 0.141]\n",
      "1007 [D loss: (0.699)(R 0.677, F 0.721)] [D acc: (0.555)(0.500, 0.609)] [G loss: 1.049] [G acc: 0.156]\n",
      "1008 [D loss: (0.624)(R 0.612, F 0.637)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.056] [G acc: 0.156]\n",
      "1009 [D loss: (0.589)(R 0.631, F 0.546)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.025] [G acc: 0.250]\n",
      "1010 [D loss: (0.659)(R 0.627, F 0.692)] [D acc: (0.555)(0.547, 0.562)] [G loss: 1.012] [G acc: 0.141]\n",
      "1011 [D loss: (0.565)(R 0.531, F 0.600)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.008] [G acc: 0.172]\n",
      "1012 [D loss: (0.539)(R 0.524, F 0.554)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.092] [G acc: 0.250]\n",
      "1013 [D loss: (0.634)(R 0.660, F 0.607)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.118] [G acc: 0.094]\n",
      "1014 [D loss: (0.586)(R 0.650, F 0.522)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.029] [G acc: 0.234]\n",
      "1015 [D loss: (0.582)(R 0.546, F 0.618)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.093] [G acc: 0.156]\n",
      "1016 [D loss: (0.569)(R 0.560, F 0.577)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.160] [G acc: 0.062]\n",
      "1017 [D loss: (0.591)(R 0.555, F 0.628)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.158] [G acc: 0.078]\n",
      "1018 [D loss: (0.586)(R 0.557, F 0.615)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.189] [G acc: 0.109]\n",
      "1019 [D loss: (0.527)(R 0.566, F 0.488)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.190] [G acc: 0.188]\n",
      "1020 [D loss: (0.589)(R 0.613, F 0.565)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.019] [G acc: 0.234]\n",
      "1021 [D loss: (0.585)(R 0.467, F 0.702)] [D acc: (0.648)(0.719, 0.578)] [G loss: 1.221] [G acc: 0.156]\n",
      "1022 [D loss: (0.568)(R 0.551, F 0.584)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.248] [G acc: 0.109]\n",
      "1023 [D loss: (0.596)(R 0.627, F 0.565)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.138] [G acc: 0.094]\n",
      "1024 [D loss: (0.557)(R 0.525, F 0.590)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.106] [G acc: 0.188]\n",
      "1025 [D loss: (0.589)(R 0.557, F 0.622)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.021] [G acc: 0.234]\n",
      "1026 [D loss: (0.619)(R 0.598, F 0.641)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.143] [G acc: 0.141]\n",
      "1027 [D loss: (0.616)(R 0.631, F 0.602)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.129] [G acc: 0.188]\n",
      "1028 [D loss: (0.598)(R 0.605, F 0.590)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.183] [G acc: 0.125]\n",
      "1029 [D loss: (0.554)(R 0.509, F 0.599)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.248] [G acc: 0.109]\n",
      "1030 [D loss: (0.641)(R 0.700, F 0.583)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.105] [G acc: 0.125]\n",
      "1031 [D loss: (0.642)(R 0.656, F 0.628)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.186] [G acc: 0.031]\n",
      "1032 [D loss: (0.612)(R 0.635, F 0.589)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.062] [G acc: 0.109]\n",
      "1033 [D loss: (0.604)(R 0.595, F 0.612)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.021] [G acc: 0.234]\n",
      "1034 [D loss: (0.594)(R 0.601, F 0.587)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.080] [G acc: 0.141]\n",
      "1035 [D loss: (0.546)(R 0.461, F 0.630)] [D acc: (0.672)(0.766, 0.578)] [G loss: 1.227] [G acc: 0.078]\n",
      "1036 [D loss: (0.613)(R 0.690, F 0.536)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.120] [G acc: 0.125]\n",
      "1037 [D loss: (0.582)(R 0.548, F 0.616)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.151] [G acc: 0.094]\n",
      "1038 [D loss: (0.590)(R 0.556, F 0.623)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.018] [G acc: 0.172]\n",
      "1039 [D loss: (0.639)(R 0.610, F 0.667)] [D acc: (0.578)(0.547, 0.609)] [G loss: 1.081] [G acc: 0.078]\n",
      "1040 [D loss: (0.549)(R 0.496, F 0.603)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.213] [G acc: 0.109]\n",
      "1041 [D loss: (0.633)(R 0.656, F 0.610)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.066] [G acc: 0.156]\n",
      "1042 [D loss: (0.557)(R 0.546, F 0.568)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.089] [G acc: 0.062]\n",
      "1043 [D loss: (0.620)(R 0.590, F 0.651)] [D acc: (0.609)(0.594, 0.625)] [G loss: 1.116] [G acc: 0.188]\n",
      "1044 [D loss: (0.591)(R 0.590, F 0.593)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.099] [G acc: 0.125]\n",
      "1045 [D loss: (0.647)(R 0.593, F 0.701)] [D acc: (0.609)(0.641, 0.578)] [G loss: 1.089] [G acc: 0.156]\n",
      "1046 [D loss: (0.604)(R 0.636, F 0.571)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.052] [G acc: 0.156]\n",
      "1047 [D loss: (0.595)(R 0.625, F 0.565)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.129] [G acc: 0.094]\n",
      "1048 [D loss: (0.583)(R 0.560, F 0.606)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.181] [G acc: 0.219]\n",
      "1049 [D loss: (0.580)(R 0.574, F 0.586)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.157] [G acc: 0.062]\n",
      "1050 [D loss: (0.623)(R 0.591, F 0.654)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.053] [G acc: 0.219]\n",
      "1051 [D loss: (0.588)(R 0.591, F 0.585)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.050] [G acc: 0.125]\n",
      "1052 [D loss: (0.575)(R 0.551, F 0.599)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.030] [G acc: 0.203]\n",
      "1053 [D loss: (0.673)(R 0.615, F 0.731)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.121] [G acc: 0.156]\n",
      "1054 [D loss: (0.636)(R 0.696, F 0.576)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.110] [G acc: 0.141]\n",
      "1055 [D loss: (0.609)(R 0.675, F 0.543)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.221] [G acc: 0.109]\n",
      "1056 [D loss: (0.611)(R 0.637, F 0.584)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.127] [G acc: 0.125]\n",
      "1057 [D loss: (0.634)(R 0.587, F 0.681)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.109] [G acc: 0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058 [D loss: (0.579)(R 0.569, F 0.590)] [D acc: (0.719)(0.688, 0.750)] [G loss: 0.951] [G acc: 0.281]\n",
      "1059 [D loss: (0.573)(R 0.559, F 0.587)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.138] [G acc: 0.062]\n",
      "1060 [D loss: (0.633)(R 0.598, F 0.668)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.300] [G acc: 0.078]\n",
      "1061 [D loss: (0.525)(R 0.529, F 0.521)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.230] [G acc: 0.062]\n",
      "1062 [D loss: (0.561)(R 0.535, F 0.587)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.035] [G acc: 0.141]\n",
      "1063 [D loss: (0.640)(R 0.521, F 0.760)] [D acc: (0.641)(0.688, 0.594)] [G loss: 1.186] [G acc: 0.094]\n",
      "1064 [D loss: (0.615)(R 0.607, F 0.622)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.133] [G acc: 0.047]\n",
      "1065 [D loss: (0.546)(R 0.584, F 0.509)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.095] [G acc: 0.188]\n",
      "1066 [D loss: (0.555)(R 0.492, F 0.618)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.248] [G acc: 0.125]\n",
      "1067 [D loss: (0.623)(R 0.573, F 0.673)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.307] [G acc: 0.094]\n",
      "1068 [D loss: (0.580)(R 0.625, F 0.535)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.221] [G acc: 0.125]\n",
      "1069 [D loss: (0.545)(R 0.573, F 0.517)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.131] [G acc: 0.156]\n",
      "1070 [D loss: (0.583)(R 0.600, F 0.567)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.168] [G acc: 0.125]\n",
      "1071 [D loss: (0.618)(R 0.528, F 0.708)] [D acc: (0.656)(0.703, 0.609)] [G loss: 1.134] [G acc: 0.172]\n",
      "1072 [D loss: (0.625)(R 0.619, F 0.630)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.096] [G acc: 0.172]\n",
      "1073 [D loss: (0.596)(R 0.578, F 0.614)] [D acc: (0.625)(0.609, 0.641)] [G loss: 1.013] [G acc: 0.156]\n",
      "1074 [D loss: (0.574)(R 0.593, F 0.554)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.112] [G acc: 0.141]\n",
      "1075 [D loss: (0.538)(R 0.558, F 0.518)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.176] [G acc: 0.156]\n",
      "1076 [D loss: (0.567)(R 0.551, F 0.582)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.004] [G acc: 0.266]\n",
      "1077 [D loss: (0.576)(R 0.589, F 0.563)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.124] [G acc: 0.094]\n",
      "1078 [D loss: (0.547)(R 0.470, F 0.625)] [D acc: (0.734)(0.766, 0.703)] [G loss: 1.082] [G acc: 0.172]\n",
      "1079 [D loss: (0.613)(R 0.624, F 0.602)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.168] [G acc: 0.156]\n",
      "1080 [D loss: (0.639)(R 0.652, F 0.626)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.077] [G acc: 0.094]\n",
      "1081 [D loss: (0.658)(R 0.622, F 0.694)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.055] [G acc: 0.172]\n",
      "1082 [D loss: (0.656)(R 0.663, F 0.649)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.157] [G acc: 0.094]\n",
      "1083 [D loss: (0.583)(R 0.621, F 0.545)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.028] [G acc: 0.109]\n",
      "1084 [D loss: (0.545)(R 0.502, F 0.588)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.165] [G acc: 0.109]\n",
      "1085 [D loss: (0.578)(R 0.563, F 0.593)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.060] [G acc: 0.203]\n",
      "1086 [D loss: (0.609)(R 0.604, F 0.614)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.078] [G acc: 0.125]\n",
      "1087 [D loss: (0.614)(R 0.556, F 0.672)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.167] [G acc: 0.109]\n",
      "1088 [D loss: (0.552)(R 0.555, F 0.550)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.274] [G acc: 0.094]\n",
      "1089 [D loss: (0.550)(R 0.482, F 0.618)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.158] [G acc: 0.078]\n",
      "1090 [D loss: (0.609)(R 0.532, F 0.687)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.215] [G acc: 0.094]\n",
      "1091 [D loss: (0.611)(R 0.660, F 0.561)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.226] [G acc: 0.078]\n",
      "1092 [D loss: (0.575)(R 0.622, F 0.527)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.207] [G acc: 0.125]\n",
      "1093 [D loss: (0.603)(R 0.531, F 0.674)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.134] [G acc: 0.156]\n",
      "1094 [D loss: (0.626)(R 0.662, F 0.590)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.127] [G acc: 0.141]\n",
      "1095 [D loss: (0.584)(R 0.516, F 0.653)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.122] [G acc: 0.156]\n",
      "1096 [D loss: (0.559)(R 0.573, F 0.545)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.168] [G acc: 0.156]\n",
      "1097 [D loss: (0.603)(R 0.579, F 0.627)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.280] [G acc: 0.078]\n",
      "1098 [D loss: (0.592)(R 0.671, F 0.512)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.229] [G acc: 0.047]\n",
      "1099 [D loss: (0.644)(R 0.644, F 0.645)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.095] [G acc: 0.156]\n",
      "1100 [D loss: (0.639)(R 0.644, F 0.633)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.080] [G acc: 0.156]\n",
      "1101 [D loss: (0.616)(R 0.597, F 0.635)] [D acc: (0.625)(0.609, 0.641)] [G loss: 1.043] [G acc: 0.172]\n",
      "1102 [D loss: (0.584)(R 0.604, F 0.564)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.082] [G acc: 0.109]\n",
      "1103 [D loss: (0.612)(R 0.639, F 0.586)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.091] [G acc: 0.141]\n",
      "1104 [D loss: (0.529)(R 0.506, F 0.552)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.070] [G acc: 0.188]\n",
      "1105 [D loss: (0.571)(R 0.533, F 0.609)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.087] [G acc: 0.141]\n",
      "1106 [D loss: (0.609)(R 0.534, F 0.684)] [D acc: (0.672)(0.656, 0.688)] [G loss: 0.982] [G acc: 0.234]\n",
      "1107 [D loss: (0.627)(R 0.478, F 0.777)] [D acc: (0.664)(0.766, 0.562)] [G loss: 1.098] [G acc: 0.156]\n",
      "1108 [D loss: (0.557)(R 0.576, F 0.538)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.087] [G acc: 0.172]\n",
      "1109 [D loss: (0.639)(R 0.687, F 0.590)] [D acc: (0.625)(0.484, 0.766)] [G loss: 1.137] [G acc: 0.078]\n",
      "1110 [D loss: (0.541)(R 0.567, F 0.516)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.158] [G acc: 0.047]\n",
      "1111 [D loss: (0.566)(R 0.516, F 0.617)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.197] [G acc: 0.172]\n",
      "1112 [D loss: (0.582)(R 0.553, F 0.610)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.201] [G acc: 0.125]\n",
      "1113 [D loss: (0.539)(R 0.568, F 0.509)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.214] [G acc: 0.062]\n",
      "1114 [D loss: (0.546)(R 0.504, F 0.587)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.119] [G acc: 0.234]\n",
      "1115 [D loss: (0.686)(R 0.679, F 0.692)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.160] [G acc: 0.062]\n",
      "1116 [D loss: (0.582)(R 0.595, F 0.569)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.193] [G acc: 0.156]\n",
      "1117 [D loss: (0.615)(R 0.619, F 0.610)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.218] [G acc: 0.109]\n",
      "1118 [D loss: (0.579)(R 0.606, F 0.551)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.156] [G acc: 0.125]\n",
      "1119 [D loss: (0.671)(R 0.703, F 0.639)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.019] [G acc: 0.141]\n",
      "1120 [D loss: (0.634)(R 0.639, F 0.630)] [D acc: (0.617)(0.609, 0.625)] [G loss: 1.185] [G acc: 0.125]\n",
      "1121 [D loss: (0.559)(R 0.582, F 0.535)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.193] [G acc: 0.094]\n",
      "1122 [D loss: (0.590)(R 0.630, F 0.550)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.099] [G acc: 0.094]\n",
      "1123 [D loss: (0.591)(R 0.599, F 0.583)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.052] [G acc: 0.125]\n",
      "1124 [D loss: (0.486)(R 0.397, F 0.575)] [D acc: (0.781)(0.828, 0.734)] [G loss: 1.387] [G acc: 0.141]\n",
      "1125 [D loss: (0.567)(R 0.633, F 0.502)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.148] [G acc: 0.109]\n",
      "1126 [D loss: (0.616)(R 0.515, F 0.717)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.121] [G acc: 0.141]\n",
      "1127 [D loss: (0.599)(R 0.522, F 0.676)] [D acc: (0.727)(0.781, 0.672)] [G loss: 1.088] [G acc: 0.125]\n",
      "1128 [D loss: (0.577)(R 0.593, F 0.562)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.256] [G acc: 0.125]\n",
      "1129 [D loss: (0.620)(R 0.713, F 0.527)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.138] [G acc: 0.109]\n",
      "1130 [D loss: (0.602)(R 0.568, F 0.636)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.211] [G acc: 0.094]\n",
      "1131 [D loss: (0.548)(R 0.520, F 0.575)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.164] [G acc: 0.047]\n",
      "1132 [D loss: (0.554)(R 0.539, F 0.569)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.196] [G acc: 0.141]\n",
      "1133 [D loss: (0.557)(R 0.531, F 0.584)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.110] [G acc: 0.172]\n",
      "1134 [D loss: (0.584)(R 0.597, F 0.570)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.153] [G acc: 0.047]\n",
      "1135 [D loss: (0.498)(R 0.441, F 0.556)] [D acc: (0.805)(0.734, 0.875)] [G loss: 1.270] [G acc: 0.078]\n",
      "1136 [D loss: (0.594)(R 0.579, F 0.609)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.226] [G acc: 0.047]\n",
      "1137 [D loss: (0.561)(R 0.577, F 0.545)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.295] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138 [D loss: (0.537)(R 0.446, F 0.627)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.245] [G acc: 0.109]\n",
      "1139 [D loss: (0.604)(R 0.570, F 0.638)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.291] [G acc: 0.109]\n",
      "1140 [D loss: (0.549)(R 0.552, F 0.546)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.152] [G acc: 0.203]\n",
      "1141 [D loss: (0.591)(R 0.582, F 0.601)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.087] [G acc: 0.219]\n",
      "1142 [D loss: (0.676)(R 0.600, F 0.753)] [D acc: (0.570)(0.609, 0.531)] [G loss: 1.088] [G acc: 0.188]\n",
      "1143 [D loss: (0.542)(R 0.552, F 0.532)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.138] [G acc: 0.125]\n",
      "1144 [D loss: (0.592)(R 0.608, F 0.575)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.160] [G acc: 0.062]\n",
      "1145 [D loss: (0.517)(R 0.512, F 0.522)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.121] [G acc: 0.172]\n",
      "1146 [D loss: (0.598)(R 0.586, F 0.610)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.129] [G acc: 0.203]\n",
      "1147 [D loss: (0.507)(R 0.523, F 0.491)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.207] [G acc: 0.156]\n",
      "1148 [D loss: (0.680)(R 0.484, F 0.877)] [D acc: (0.672)(0.719, 0.625)] [G loss: 1.510] [G acc: 0.094]\n",
      "1149 [D loss: (0.589)(R 0.734, F 0.445)] [D acc: (0.680)(0.453, 0.906)] [G loss: 1.233] [G acc: 0.094]\n",
      "1150 [D loss: (0.566)(R 0.587, F 0.546)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.163] [G acc: 0.125]\n",
      "1151 [D loss: (0.638)(R 0.647, F 0.629)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.156] [G acc: 0.094]\n",
      "1152 [D loss: (0.476)(R 0.430, F 0.523)] [D acc: (0.828)(0.812, 0.844)] [G loss: 1.206] [G acc: 0.125]\n",
      "1153 [D loss: (0.639)(R 0.491, F 0.787)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.126] [G acc: 0.203]\n",
      "1154 [D loss: (0.517)(R 0.479, F 0.555)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.489] [G acc: 0.031]\n",
      "1155 [D loss: (0.537)(R 0.481, F 0.593)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.311] [G acc: 0.156]\n",
      "1156 [D loss: (0.667)(R 0.728, F 0.605)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.200] [G acc: 0.078]\n",
      "1157 [D loss: (0.600)(R 0.636, F 0.563)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.225] [G acc: 0.062]\n",
      "1158 [D loss: (0.585)(R 0.601, F 0.569)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.227] [G acc: 0.078]\n",
      "1159 [D loss: (0.533)(R 0.593, F 0.473)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.170] [G acc: 0.188]\n",
      "1160 [D loss: (0.549)(R 0.478, F 0.620)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.301] [G acc: 0.125]\n",
      "1161 [D loss: (0.560)(R 0.603, F 0.518)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.274] [G acc: 0.172]\n",
      "1162 [D loss: (0.551)(R 0.480, F 0.622)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.420] [G acc: 0.078]\n",
      "1163 [D loss: (0.560)(R 0.651, F 0.468)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.253] [G acc: 0.047]\n",
      "1164 [D loss: (0.559)(R 0.499, F 0.619)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.272] [G acc: 0.109]\n",
      "1165 [D loss: (0.557)(R 0.539, F 0.575)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.211] [G acc: 0.062]\n",
      "1166 [D loss: (0.640)(R 0.550, F 0.730)] [D acc: (0.664)(0.719, 0.609)] [G loss: 1.256] [G acc: 0.062]\n",
      "1167 [D loss: (0.533)(R 0.579, F 0.487)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.243] [G acc: 0.141]\n",
      "1168 [D loss: (0.598)(R 0.590, F 0.606)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.214] [G acc: 0.109]\n",
      "1169 [D loss: (0.577)(R 0.542, F 0.613)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.196] [G acc: 0.094]\n",
      "1170 [D loss: (0.660)(R 0.690, F 0.630)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.187] [G acc: 0.094]\n",
      "1171 [D loss: (0.605)(R 0.649, F 0.561)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.133] [G acc: 0.109]\n",
      "1172 [D loss: (0.558)(R 0.531, F 0.585)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.177] [G acc: 0.094]\n",
      "1173 [D loss: (0.527)(R 0.522, F 0.531)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.202] [G acc: 0.109]\n",
      "1174 [D loss: (0.551)(R 0.498, F 0.603)] [D acc: (0.695)(0.750, 0.641)] [G loss: 1.323] [G acc: 0.047]\n",
      "1175 [D loss: (0.480)(R 0.505, F 0.455)] [D acc: (0.812)(0.719, 0.906)] [G loss: 1.277] [G acc: 0.141]\n",
      "1176 [D loss: (0.562)(R 0.622, F 0.502)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.446] [G acc: 0.094]\n",
      "1177 [D loss: (0.556)(R 0.660, F 0.453)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.260] [G acc: 0.125]\n",
      "1178 [D loss: (0.495)(R 0.477, F 0.513)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.296] [G acc: 0.109]\n",
      "1179 [D loss: (0.548)(R 0.507, F 0.588)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.276] [G acc: 0.141]\n",
      "1180 [D loss: (0.589)(R 0.587, F 0.592)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.399] [G acc: 0.062]\n",
      "1181 [D loss: (0.660)(R 0.655, F 0.666)] [D acc: (0.617)(0.516, 0.719)] [G loss: 1.295] [G acc: 0.062]\n",
      "1182 [D loss: (0.593)(R 0.624, F 0.563)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.124] [G acc: 0.203]\n",
      "1183 [D loss: (0.604)(R 0.592, F 0.617)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.212] [G acc: 0.141]\n",
      "1184 [D loss: (0.544)(R 0.510, F 0.577)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.385] [G acc: 0.062]\n",
      "1185 [D loss: (0.507)(R 0.559, F 0.455)] [D acc: (0.781)(0.656, 0.906)] [G loss: 1.358] [G acc: 0.125]\n",
      "1186 [D loss: (0.646)(R 0.520, F 0.771)] [D acc: (0.656)(0.703, 0.609)] [G loss: 1.244] [G acc: 0.188]\n",
      "1187 [D loss: (0.623)(R 0.635, F 0.610)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.289] [G acc: 0.156]\n",
      "1188 [D loss: (0.516)(R 0.514, F 0.518)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.075] [G acc: 0.234]\n",
      "1189 [D loss: (0.691)(R 0.618, F 0.764)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.205] [G acc: 0.156]\n",
      "1190 [D loss: (0.653)(R 0.609, F 0.697)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.188] [G acc: 0.141]\n",
      "1191 [D loss: (0.500)(R 0.533, F 0.468)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.251] [G acc: 0.109]\n",
      "1192 [D loss: (0.602)(R 0.567, F 0.637)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.114] [G acc: 0.203]\n",
      "1193 [D loss: (0.538)(R 0.548, F 0.528)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.149] [G acc: 0.141]\n",
      "1194 [D loss: (0.569)(R 0.512, F 0.627)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.265] [G acc: 0.156]\n",
      "1195 [D loss: (0.701)(R 0.787, F 0.616)] [D acc: (0.570)(0.484, 0.656)] [G loss: 1.293] [G acc: 0.047]\n",
      "1196 [D loss: (0.620)(R 0.591, F 0.650)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.142] [G acc: 0.109]\n",
      "1197 [D loss: (0.613)(R 0.662, F 0.563)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.178] [G acc: 0.141]\n",
      "1198 [D loss: (0.590)(R 0.605, F 0.575)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.170] [G acc: 0.203]\n",
      "1199 [D loss: (0.626)(R 0.655, F 0.596)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.100] [G acc: 0.125]\n",
      "1200 [D loss: (0.548)(R 0.494, F 0.603)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.130] [G acc: 0.125]\n",
      "1201 [D loss: (0.664)(R 0.677, F 0.651)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.025] [G acc: 0.188]\n",
      "1202 [D loss: (0.527)(R 0.555, F 0.500)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.103] [G acc: 0.188]\n",
      "1203 [D loss: (0.608)(R 0.603, F 0.613)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.128] [G acc: 0.141]\n",
      "1204 [D loss: (0.585)(R 0.503, F 0.666)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.167] [G acc: 0.109]\n",
      "1205 [D loss: (0.581)(R 0.577, F 0.585)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.264] [G acc: 0.062]\n",
      "1206 [D loss: (0.636)(R 0.638, F 0.633)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.131] [G acc: 0.078]\n",
      "1207 [D loss: (0.534)(R 0.594, F 0.474)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.114] [G acc: 0.219]\n",
      "1208 [D loss: (0.597)(R 0.576, F 0.618)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.142] [G acc: 0.141]\n",
      "1209 [D loss: (0.518)(R 0.532, F 0.503)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.252] [G acc: 0.109]\n",
      "1210 [D loss: (0.604)(R 0.585, F 0.623)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.234] [G acc: 0.141]\n",
      "1211 [D loss: (0.533)(R 0.518, F 0.548)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.246] [G acc: 0.047]\n",
      "1212 [D loss: (0.534)(R 0.534, F 0.533)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.156] [G acc: 0.094]\n",
      "1213 [D loss: (0.579)(R 0.571, F 0.587)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.132] [G acc: 0.141]\n",
      "1214 [D loss: (0.647)(R 0.646, F 0.648)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.234] [G acc: 0.016]\n",
      "1215 [D loss: (0.519)(R 0.548, F 0.490)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.232] [G acc: 0.047]\n",
      "1216 [D loss: (0.615)(R 0.698, F 0.533)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.154] [G acc: 0.141]\n",
      "1217 [D loss: (0.672)(R 0.579, F 0.766)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.135] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1218 [D loss: (0.560)(R 0.540, F 0.581)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.233] [G acc: 0.141]\n",
      "1219 [D loss: (0.574)(R 0.593, F 0.554)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.224] [G acc: 0.156]\n",
      "1220 [D loss: (0.481)(R 0.465, F 0.497)] [D acc: (0.812)(0.766, 0.859)] [G loss: 1.249] [G acc: 0.109]\n",
      "1221 [D loss: (0.603)(R 0.519, F 0.687)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.330] [G acc: 0.125]\n",
      "1222 [D loss: (0.568)(R 0.639, F 0.497)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.230] [G acc: 0.156]\n",
      "1223 [D loss: (0.634)(R 0.593, F 0.675)] [D acc: (0.633)(0.641, 0.625)] [G loss: 1.075] [G acc: 0.172]\n",
      "1224 [D loss: (0.586)(R 0.554, F 0.617)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.111] [G acc: 0.141]\n",
      "1225 [D loss: (0.549)(R 0.535, F 0.563)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.330] [G acc: 0.125]\n",
      "1226 [D loss: (0.595)(R 0.696, F 0.494)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.204] [G acc: 0.078]\n",
      "1227 [D loss: (0.551)(R 0.493, F 0.609)] [D acc: (0.758)(0.797, 0.719)] [G loss: 1.208] [G acc: 0.109]\n",
      "1228 [D loss: (0.624)(R 0.612, F 0.636)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.105] [G acc: 0.172]\n",
      "1229 [D loss: (0.597)(R 0.563, F 0.631)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.248] [G acc: 0.078]\n",
      "1230 [D loss: (0.675)(R 0.717, F 0.634)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.252] [G acc: 0.062]\n",
      "1231 [D loss: (0.597)(R 0.640, F 0.554)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.202] [G acc: 0.125]\n",
      "1232 [D loss: (0.656)(R 0.707, F 0.606)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.118] [G acc: 0.109]\n",
      "1233 [D loss: (0.562)(R 0.588, F 0.536)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.208] [G acc: 0.062]\n",
      "1234 [D loss: (0.574)(R 0.565, F 0.582)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.143] [G acc: 0.125]\n",
      "1235 [D loss: (0.595)(R 0.545, F 0.645)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.105] [G acc: 0.125]\n",
      "1236 [D loss: (0.495)(R 0.492, F 0.497)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.200] [G acc: 0.109]\n",
      "1237 [D loss: (0.578)(R 0.562, F 0.594)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.103] [G acc: 0.219]\n",
      "1238 [D loss: (0.593)(R 0.597, F 0.588)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.256] [G acc: 0.078]\n",
      "1239 [D loss: (0.555)(R 0.550, F 0.560)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.174] [G acc: 0.109]\n",
      "1240 [D loss: (0.578)(R 0.503, F 0.653)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.160] [G acc: 0.141]\n",
      "1241 [D loss: (0.592)(R 0.599, F 0.585)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.207] [G acc: 0.078]\n",
      "1242 [D loss: (0.487)(R 0.479, F 0.494)] [D acc: (0.797)(0.719, 0.875)] [G loss: 1.289] [G acc: 0.109]\n",
      "1243 [D loss: (0.462)(R 0.431, F 0.493)] [D acc: (0.789)(0.766, 0.812)] [G loss: 1.198] [G acc: 0.109]\n",
      "1244 [D loss: (0.688)(R 0.752, F 0.625)] [D acc: (0.594)(0.531, 0.656)] [G loss: 1.172] [G acc: 0.062]\n",
      "1245 [D loss: (0.600)(R 0.686, F 0.515)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.180] [G acc: 0.078]\n",
      "1246 [D loss: (0.614)(R 0.522, F 0.706)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.172] [G acc: 0.094]\n",
      "1247 [D loss: (0.631)(R 0.545, F 0.717)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.183] [G acc: 0.156]\n",
      "1248 [D loss: (0.601)(R 0.698, F 0.504)] [D acc: (0.680)(0.484, 0.875)] [G loss: 1.241] [G acc: 0.078]\n",
      "1249 [D loss: (0.515)(R 0.562, F 0.468)] [D acc: (0.773)(0.625, 0.922)] [G loss: 1.290] [G acc: 0.078]\n",
      "1250 [D loss: (0.548)(R 0.553, F 0.544)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.235] [G acc: 0.109]\n",
      "1251 [D loss: (0.710)(R 0.584, F 0.835)] [D acc: (0.594)(0.641, 0.547)] [G loss: 1.299] [G acc: 0.094]\n",
      "1252 [D loss: (0.614)(R 0.597, F 0.631)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.182] [G acc: 0.141]\n",
      "1253 [D loss: (0.525)(R 0.531, F 0.520)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.152] [G acc: 0.203]\n",
      "1254 [D loss: (0.637)(R 0.538, F 0.736)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.162] [G acc: 0.156]\n",
      "1255 [D loss: (0.618)(R 0.627, F 0.610)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.212] [G acc: 0.141]\n",
      "1256 [D loss: (0.559)(R 0.579, F 0.538)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.263] [G acc: 0.109]\n",
      "1257 [D loss: (0.571)(R 0.620, F 0.522)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.371] [G acc: 0.125]\n",
      "1258 [D loss: (0.551)(R 0.467, F 0.634)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.200] [G acc: 0.188]\n",
      "1259 [D loss: (0.523)(R 0.518, F 0.527)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.285] [G acc: 0.156]\n",
      "1260 [D loss: (0.614)(R 0.551, F 0.676)] [D acc: (0.625)(0.656, 0.594)] [G loss: 1.217] [G acc: 0.078]\n",
      "1261 [D loss: (0.593)(R 0.584, F 0.603)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.243] [G acc: 0.172]\n",
      "1262 [D loss: (0.628)(R 0.634, F 0.622)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.112] [G acc: 0.203]\n",
      "1263 [D loss: (0.593)(R 0.584, F 0.602)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.278] [G acc: 0.062]\n",
      "1264 [D loss: (0.605)(R 0.587, F 0.622)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.281] [G acc: 0.094]\n",
      "1265 [D loss: (0.641)(R 0.606, F 0.675)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.267] [G acc: 0.109]\n",
      "1266 [D loss: (0.518)(R 0.518, F 0.519)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.131] [G acc: 0.156]\n",
      "1267 [D loss: (0.568)(R 0.519, F 0.618)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.246] [G acc: 0.094]\n",
      "1268 [D loss: (0.589)(R 0.649, F 0.529)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.199] [G acc: 0.109]\n",
      "1269 [D loss: (0.514)(R 0.540, F 0.489)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.116] [G acc: 0.188]\n",
      "1270 [D loss: (0.543)(R 0.573, F 0.514)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.230] [G acc: 0.172]\n",
      "1271 [D loss: (0.630)(R 0.597, F 0.663)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.173] [G acc: 0.109]\n",
      "1272 [D loss: (0.580)(R 0.569, F 0.590)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.237] [G acc: 0.031]\n",
      "1273 [D loss: (0.550)(R 0.587, F 0.513)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.280] [G acc: 0.125]\n",
      "1274 [D loss: (0.619)(R 0.652, F 0.587)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.144] [G acc: 0.125]\n",
      "1275 [D loss: (0.520)(R 0.481, F 0.558)] [D acc: (0.734)(0.781, 0.688)] [G loss: 1.248] [G acc: 0.062]\n",
      "1276 [D loss: (0.609)(R 0.647, F 0.571)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.123] [G acc: 0.156]\n",
      "1277 [D loss: (0.468)(R 0.378, F 0.559)] [D acc: (0.758)(0.797, 0.719)] [G loss: 1.201] [G acc: 0.047]\n",
      "1278 [D loss: (0.599)(R 0.668, F 0.530)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.193] [G acc: 0.234]\n",
      "1279 [D loss: (0.540)(R 0.590, F 0.489)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.231] [G acc: 0.125]\n",
      "1280 [D loss: (0.603)(R 0.579, F 0.627)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.219] [G acc: 0.203]\n",
      "1281 [D loss: (0.636)(R 0.667, F 0.606)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.182] [G acc: 0.094]\n",
      "1282 [D loss: (0.548)(R 0.539, F 0.556)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.141] [G acc: 0.094]\n",
      "1283 [D loss: (0.580)(R 0.572, F 0.589)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.107] [G acc: 0.156]\n",
      "1284 [D loss: (0.588)(R 0.660, F 0.516)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.232] [G acc: 0.109]\n",
      "1285 [D loss: (0.546)(R 0.498, F 0.594)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.091] [G acc: 0.281]\n",
      "1286 [D loss: (0.532)(R 0.511, F 0.552)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.304] [G acc: 0.109]\n",
      "1287 [D loss: (0.579)(R 0.458, F 0.699)] [D acc: (0.727)(0.766, 0.688)] [G loss: 1.202] [G acc: 0.188]\n",
      "1288 [D loss: (0.647)(R 0.726, F 0.569)] [D acc: (0.586)(0.438, 0.734)] [G loss: 1.249] [G acc: 0.125]\n",
      "1289 [D loss: (0.594)(R 0.506, F 0.682)] [D acc: (0.711)(0.750, 0.672)] [G loss: 1.214] [G acc: 0.125]\n",
      "1290 [D loss: (0.566)(R 0.577, F 0.555)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.231] [G acc: 0.094]\n",
      "1291 [D loss: (0.505)(R 0.486, F 0.523)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.270] [G acc: 0.125]\n",
      "1292 [D loss: (0.620)(R 0.623, F 0.616)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.183] [G acc: 0.156]\n",
      "1293 [D loss: (0.516)(R 0.546, F 0.486)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.202] [G acc: 0.172]\n",
      "1294 [D loss: (0.463)(R 0.384, F 0.542)] [D acc: (0.805)(0.812, 0.797)] [G loss: 1.088] [G acc: 0.203]\n",
      "1295 [D loss: (0.564)(R 0.486, F 0.641)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.190] [G acc: 0.141]\n",
      "1296 [D loss: (0.536)(R 0.569, F 0.503)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.266] [G acc: 0.188]\n",
      "1297 [D loss: (0.562)(R 0.617, F 0.507)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.276] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1298 [D loss: (0.522)(R 0.444, F 0.601)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.265] [G acc: 0.062]\n",
      "1299 [D loss: (0.499)(R 0.426, F 0.571)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.529] [G acc: 0.062]\n",
      "1300 [D loss: (0.566)(R 0.625, F 0.507)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.143] [G acc: 0.281]\n",
      "1301 [D loss: (0.643)(R 0.618, F 0.668)] [D acc: (0.633)(0.641, 0.625)] [G loss: 1.396] [G acc: 0.141]\n",
      "1302 [D loss: (0.535)(R 0.648, F 0.422)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.307] [G acc: 0.094]\n",
      "1303 [D loss: (0.574)(R 0.565, F 0.584)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.367] [G acc: 0.172]\n",
      "1304 [D loss: (0.595)(R 0.648, F 0.543)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.146] [G acc: 0.156]\n",
      "1305 [D loss: (0.553)(R 0.525, F 0.581)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.246] [G acc: 0.125]\n",
      "1306 [D loss: (0.651)(R 0.655, F 0.647)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.196] [G acc: 0.141]\n",
      "1307 [D loss: (0.524)(R 0.559, F 0.489)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.321] [G acc: 0.141]\n",
      "1308 [D loss: (0.528)(R 0.527, F 0.528)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.399] [G acc: 0.047]\n",
      "1309 [D loss: (0.564)(R 0.537, F 0.591)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.232] [G acc: 0.203]\n",
      "1310 [D loss: (0.590)(R 0.563, F 0.617)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.308] [G acc: 0.062]\n",
      "1311 [D loss: (0.595)(R 0.631, F 0.559)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.207] [G acc: 0.078]\n",
      "1312 [D loss: (0.641)(R 0.680, F 0.602)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.222] [G acc: 0.047]\n",
      "1313 [D loss: (0.556)(R 0.586, F 0.526)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.228] [G acc: 0.109]\n",
      "1314 [D loss: (0.532)(R 0.532, F 0.531)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.134] [G acc: 0.188]\n",
      "1315 [D loss: (0.544)(R 0.501, F 0.586)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.350] [G acc: 0.078]\n",
      "1316 [D loss: (0.628)(R 0.666, F 0.590)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.215] [G acc: 0.125]\n",
      "1317 [D loss: (0.582)(R 0.585, F 0.579)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.236] [G acc: 0.109]\n",
      "1318 [D loss: (0.531)(R 0.534, F 0.528)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.187] [G acc: 0.047]\n",
      "1319 [D loss: (0.614)(R 0.553, F 0.676)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.286] [G acc: 0.062]\n",
      "1320 [D loss: (0.554)(R 0.612, F 0.496)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.166] [G acc: 0.125]\n",
      "1321 [D loss: (0.580)(R 0.627, F 0.533)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.202] [G acc: 0.109]\n",
      "1322 [D loss: (0.559)(R 0.529, F 0.589)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.282] [G acc: 0.141]\n",
      "1323 [D loss: (0.596)(R 0.594, F 0.597)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.144] [G acc: 0.109]\n",
      "1324 [D loss: (0.611)(R 0.476, F 0.746)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.296] [G acc: 0.078]\n",
      "1325 [D loss: (0.572)(R 0.538, F 0.607)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.265] [G acc: 0.094]\n",
      "1326 [D loss: (0.634)(R 0.757, F 0.511)] [D acc: (0.633)(0.422, 0.844)] [G loss: 1.183] [G acc: 0.047]\n",
      "1327 [D loss: (0.632)(R 0.660, F 0.604)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.079] [G acc: 0.172]\n",
      "1328 [D loss: (0.610)(R 0.616, F 0.604)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.060] [G acc: 0.109]\n",
      "1329 [D loss: (0.545)(R 0.519, F 0.572)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.154] [G acc: 0.125]\n",
      "1330 [D loss: (0.580)(R 0.607, F 0.553)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.136] [G acc: 0.094]\n",
      "1331 [D loss: (0.579)(R 0.621, F 0.537)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.203] [G acc: 0.141]\n",
      "1332 [D loss: (0.595)(R 0.616, F 0.573)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.173] [G acc: 0.094]\n",
      "1333 [D loss: (0.551)(R 0.598, F 0.503)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.270] [G acc: 0.094]\n",
      "1334 [D loss: (0.587)(R 0.606, F 0.568)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.242] [G acc: 0.141]\n",
      "1335 [D loss: (0.588)(R 0.619, F 0.557)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.240] [G acc: 0.156]\n",
      "1336 [D loss: (0.583)(R 0.529, F 0.637)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.207] [G acc: 0.141]\n",
      "1337 [D loss: (0.554)(R 0.545, F 0.564)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.181] [G acc: 0.156]\n",
      "1338 [D loss: (0.465)(R 0.501, F 0.430)] [D acc: (0.789)(0.672, 0.906)] [G loss: 1.401] [G acc: 0.156]\n",
      "1339 [D loss: (0.560)(R 0.619, F 0.501)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.264] [G acc: 0.109]\n",
      "1340 [D loss: (0.593)(R 0.533, F 0.653)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.129] [G acc: 0.141]\n",
      "1341 [D loss: (0.583)(R 0.614, F 0.553)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.138] [G acc: 0.219]\n",
      "1342 [D loss: (0.610)(R 0.475, F 0.745)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.286] [G acc: 0.141]\n",
      "1343 [D loss: (0.566)(R 0.607, F 0.526)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.277] [G acc: 0.094]\n",
      "1344 [D loss: (0.584)(R 0.542, F 0.626)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.363] [G acc: 0.219]\n",
      "1345 [D loss: (0.536)(R 0.465, F 0.607)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.317] [G acc: 0.078]\n",
      "1346 [D loss: (0.617)(R 0.661, F 0.573)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.081] [G acc: 0.219]\n",
      "1347 [D loss: (0.587)(R 0.583, F 0.591)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.375] [G acc: 0.062]\n",
      "1348 [D loss: (0.566)(R 0.617, F 0.515)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.251] [G acc: 0.109]\n",
      "1349 [D loss: (0.562)(R 0.607, F 0.516)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.154] [G acc: 0.125]\n",
      "1350 [D loss: (0.625)(R 0.678, F 0.571)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.113] [G acc: 0.188]\n",
      "1351 [D loss: (0.553)(R 0.515, F 0.591)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.348] [G acc: 0.016]\n",
      "1352 [D loss: (0.589)(R 0.631, F 0.548)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.094] [G acc: 0.203]\n",
      "1353 [D loss: (0.633)(R 0.545, F 0.721)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.260] [G acc: 0.188]\n",
      "1354 [D loss: (0.562)(R 0.627, F 0.497)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.128] [G acc: 0.219]\n",
      "1355 [D loss: (0.587)(R 0.615, F 0.558)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.245] [G acc: 0.109]\n",
      "1356 [D loss: (0.585)(R 0.554, F 0.617)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.247] [G acc: 0.078]\n",
      "1357 [D loss: (0.591)(R 0.594, F 0.589)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.186] [G acc: 0.109]\n",
      "1358 [D loss: (0.554)(R 0.570, F 0.538)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.177] [G acc: 0.109]\n",
      "1359 [D loss: (0.555)(R 0.528, F 0.583)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.175] [G acc: 0.172]\n",
      "1360 [D loss: (0.581)(R 0.509, F 0.653)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.215] [G acc: 0.141]\n",
      "1361 [D loss: (0.595)(R 0.540, F 0.651)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.231] [G acc: 0.188]\n",
      "1362 [D loss: (0.564)(R 0.655, F 0.472)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.248] [G acc: 0.172]\n",
      "1363 [D loss: (0.556)(R 0.543, F 0.568)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.342] [G acc: 0.156]\n",
      "1364 [D loss: (0.613)(R 0.715, F 0.511)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.263] [G acc: 0.094]\n",
      "1365 [D loss: (0.517)(R 0.518, F 0.517)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.286] [G acc: 0.078]\n",
      "1366 [D loss: (0.726)(R 0.651, F 0.801)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.247] [G acc: 0.094]\n",
      "1367 [D loss: (0.516)(R 0.482, F 0.551)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.278] [G acc: 0.078]\n",
      "1368 [D loss: (0.585)(R 0.537, F 0.633)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.164] [G acc: 0.219]\n",
      "1369 [D loss: (0.592)(R 0.584, F 0.599)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.377] [G acc: 0.094]\n",
      "1370 [D loss: (0.508)(R 0.535, F 0.482)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.347] [G acc: 0.078]\n",
      "1371 [D loss: (0.600)(R 0.525, F 0.674)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.322] [G acc: 0.031]\n",
      "1372 [D loss: (0.534)(R 0.577, F 0.492)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.360] [G acc: 0.094]\n",
      "1373 [D loss: (0.557)(R 0.537, F 0.576)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.386] [G acc: 0.047]\n",
      "1374 [D loss: (0.577)(R 0.584, F 0.571)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.233] [G acc: 0.062]\n",
      "1375 [D loss: (0.613)(R 0.558, F 0.668)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.208] [G acc: 0.234]\n",
      "1376 [D loss: (0.593)(R 0.605, F 0.582)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.158] [G acc: 0.156]\n",
      "1377 [D loss: (0.609)(R 0.623, F 0.596)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.146] [G acc: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378 [D loss: (0.543)(R 0.522, F 0.565)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.259] [G acc: 0.141]\n",
      "1379 [D loss: (0.606)(R 0.643, F 0.568)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.166] [G acc: 0.141]\n",
      "1380 [D loss: (0.605)(R 0.599, F 0.612)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.168] [G acc: 0.109]\n",
      "1381 [D loss: (0.598)(R 0.587, F 0.609)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.177] [G acc: 0.125]\n",
      "1382 [D loss: (0.567)(R 0.554, F 0.580)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.229] [G acc: 0.172]\n",
      "1383 [D loss: (0.590)(R 0.606, F 0.573)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.147] [G acc: 0.141]\n",
      "1384 [D loss: (0.632)(R 0.590, F 0.674)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.208] [G acc: 0.172]\n",
      "1385 [D loss: (0.553)(R 0.598, F 0.507)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.227] [G acc: 0.109]\n",
      "1386 [D loss: (0.611)(R 0.643, F 0.580)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.148] [G acc: 0.125]\n",
      "1387 [D loss: (0.546)(R 0.508, F 0.584)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.165] [G acc: 0.062]\n",
      "1388 [D loss: (0.602)(R 0.650, F 0.554)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.176] [G acc: 0.094]\n",
      "1389 [D loss: (0.571)(R 0.621, F 0.521)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.134] [G acc: 0.094]\n",
      "1390 [D loss: (0.579)(R 0.561, F 0.597)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.111] [G acc: 0.078]\n",
      "1391 [D loss: (0.651)(R 0.650, F 0.651)] [D acc: (0.609)(0.547, 0.672)] [G loss: 1.086] [G acc: 0.109]\n",
      "1392 [D loss: (0.550)(R 0.556, F 0.543)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.243] [G acc: 0.047]\n",
      "1393 [D loss: (0.501)(R 0.528, F 0.474)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.229] [G acc: 0.125]\n",
      "1394 [D loss: (0.614)(R 0.570, F 0.658)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.246] [G acc: 0.094]\n",
      "1395 [D loss: (0.673)(R 0.602, F 0.744)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.205] [G acc: 0.094]\n",
      "1396 [D loss: (0.612)(R 0.671, F 0.553)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.046] [G acc: 0.219]\n",
      "1397 [D loss: (0.540)(R 0.510, F 0.570)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.065] [G acc: 0.266]\n",
      "1398 [D loss: (0.723)(R 0.643, F 0.803)] [D acc: (0.555)(0.578, 0.531)] [G loss: 1.101] [G acc: 0.141]\n",
      "1399 [D loss: (0.571)(R 0.595, F 0.548)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.103] [G acc: 0.125]\n",
      "1400 [D loss: (0.568)(R 0.554, F 0.581)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.255] [G acc: 0.094]\n",
      "1401 [D loss: (0.581)(R 0.638, F 0.523)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.114] [G acc: 0.125]\n",
      "1402 [D loss: (0.612)(R 0.563, F 0.661)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.119] [G acc: 0.125]\n",
      "1403 [D loss: (0.532)(R 0.459, F 0.605)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.156] [G acc: 0.125]\n",
      "1404 [D loss: (0.544)(R 0.566, F 0.521)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.254] [G acc: 0.125]\n",
      "1405 [D loss: (0.619)(R 0.592, F 0.646)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.206] [G acc: 0.062]\n",
      "1406 [D loss: (0.597)(R 0.615, F 0.579)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.065] [G acc: 0.125]\n",
      "1407 [D loss: (0.507)(R 0.488, F 0.525)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.181] [G acc: 0.141]\n",
      "1408 [D loss: (0.600)(R 0.570, F 0.629)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.196] [G acc: 0.062]\n",
      "1409 [D loss: (0.657)(R 0.679, F 0.634)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.246] [G acc: 0.062]\n",
      "1410 [D loss: (0.555)(R 0.591, F 0.520)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.283] [G acc: 0.078]\n",
      "1411 [D loss: (0.551)(R 0.521, F 0.581)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.144] [G acc: 0.172]\n",
      "1412 [D loss: (0.589)(R 0.640, F 0.537)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.338] [G acc: 0.062]\n",
      "1413 [D loss: (0.516)(R 0.491, F 0.540)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.354] [G acc: 0.031]\n",
      "1414 [D loss: (0.557)(R 0.497, F 0.616)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.204] [G acc: 0.125]\n",
      "1415 [D loss: (0.606)(R 0.642, F 0.569)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.174] [G acc: 0.141]\n",
      "1416 [D loss: (0.553)(R 0.620, F 0.486)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.322] [G acc: 0.062]\n",
      "1417 [D loss: (0.526)(R 0.536, F 0.515)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.327] [G acc: 0.078]\n",
      "1418 [D loss: (0.554)(R 0.536, F 0.572)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.304] [G acc: 0.094]\n",
      "1419 [D loss: (0.558)(R 0.538, F 0.579)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.215] [G acc: 0.047]\n",
      "1420 [D loss: (0.556)(R 0.502, F 0.611)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.334] [G acc: 0.141]\n",
      "1421 [D loss: (0.516)(R 0.492, F 0.540)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.315] [G acc: 0.156]\n",
      "1422 [D loss: (0.529)(R 0.484, F 0.575)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.245] [G acc: 0.109]\n",
      "1423 [D loss: (0.595)(R 0.554, F 0.636)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.233] [G acc: 0.141]\n",
      "1424 [D loss: (0.488)(R 0.481, F 0.494)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.328] [G acc: 0.156]\n",
      "1425 [D loss: (0.628)(R 0.679, F 0.577)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.375] [G acc: 0.078]\n",
      "1426 [D loss: (0.594)(R 0.609, F 0.579)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.295] [G acc: 0.109]\n",
      "1427 [D loss: (0.571)(R 0.528, F 0.615)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.181] [G acc: 0.156]\n",
      "1428 [D loss: (0.637)(R 0.631, F 0.644)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.116] [G acc: 0.156]\n",
      "1429 [D loss: (0.572)(R 0.553, F 0.592)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.309] [G acc: 0.062]\n",
      "1430 [D loss: (0.603)(R 0.649, F 0.556)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.235] [G acc: 0.094]\n",
      "1431 [D loss: (0.618)(R 0.656, F 0.581)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.226] [G acc: 0.156]\n",
      "1432 [D loss: (0.533)(R 0.553, F 0.512)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.115] [G acc: 0.094]\n",
      "1433 [D loss: (0.562)(R 0.588, F 0.537)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.179] [G acc: 0.125]\n",
      "1434 [D loss: (0.610)(R 0.579, F 0.641)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.158] [G acc: 0.250]\n",
      "1435 [D loss: (0.573)(R 0.550, F 0.596)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.212] [G acc: 0.141]\n",
      "1436 [D loss: (0.492)(R 0.472, F 0.513)] [D acc: (0.797)(0.750, 0.844)] [G loss: 1.302] [G acc: 0.125]\n",
      "1437 [D loss: (0.569)(R 0.607, F 0.532)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.259] [G acc: 0.078]\n",
      "1438 [D loss: (0.572)(R 0.620, F 0.524)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.238] [G acc: 0.109]\n",
      "1439 [D loss: (0.645)(R 0.626, F 0.664)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.302] [G acc: 0.078]\n",
      "1440 [D loss: (0.554)(R 0.562, F 0.546)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.177] [G acc: 0.109]\n",
      "1441 [D loss: (0.556)(R 0.582, F 0.530)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.031] [G acc: 0.172]\n",
      "1442 [D loss: (0.638)(R 0.708, F 0.567)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.233] [G acc: 0.109]\n",
      "1443 [D loss: (0.555)(R 0.535, F 0.575)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.183] [G acc: 0.078]\n",
      "1444 [D loss: (0.529)(R 0.489, F 0.569)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.250] [G acc: 0.141]\n",
      "1445 [D loss: (0.570)(R 0.549, F 0.590)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.315] [G acc: 0.109]\n",
      "1446 [D loss: (0.580)(R 0.635, F 0.526)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.163] [G acc: 0.141]\n",
      "1447 [D loss: (0.479)(R 0.446, F 0.511)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.332] [G acc: 0.078]\n",
      "1448 [D loss: (0.545)(R 0.567, F 0.523)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.307] [G acc: 0.109]\n",
      "1449 [D loss: (0.469)(R 0.399, F 0.540)] [D acc: (0.781)(0.797, 0.766)] [G loss: 1.409] [G acc: 0.078]\n",
      "1450 [D loss: (0.567)(R 0.538, F 0.597)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.208] [G acc: 0.062]\n",
      "1451 [D loss: (0.615)(R 0.616, F 0.614)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.209] [G acc: 0.109]\n",
      "1452 [D loss: (0.600)(R 0.576, F 0.624)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.155] [G acc: 0.156]\n",
      "1453 [D loss: (0.547)(R 0.596, F 0.498)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.191] [G acc: 0.125]\n",
      "1454 [D loss: (0.615)(R 0.648, F 0.582)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.174] [G acc: 0.109]\n",
      "1455 [D loss: (0.539)(R 0.500, F 0.577)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.191] [G acc: 0.109]\n",
      "1456 [D loss: (0.525)(R 0.496, F 0.553)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.275] [G acc: 0.062]\n",
      "1457 [D loss: (0.526)(R 0.468, F 0.585)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.243] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1458 [D loss: (0.679)(R 0.765, F 0.593)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.126] [G acc: 0.219]\n",
      "1459 [D loss: (0.559)(R 0.547, F 0.570)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.156] [G acc: 0.219]\n",
      "1460 [D loss: (0.519)(R 0.476, F 0.563)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.147] [G acc: 0.156]\n",
      "1461 [D loss: (0.529)(R 0.607, F 0.450)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.257] [G acc: 0.188]\n",
      "1462 [D loss: (0.482)(R 0.443, F 0.521)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.173] [G acc: 0.172]\n",
      "1463 [D loss: (0.600)(R 0.520, F 0.679)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.212] [G acc: 0.094]\n",
      "1464 [D loss: (0.601)(R 0.608, F 0.594)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.319] [G acc: 0.109]\n",
      "1465 [D loss: (0.611)(R 0.508, F 0.715)] [D acc: (0.688)(0.734, 0.641)] [G loss: 1.241] [G acc: 0.125]\n",
      "1466 [D loss: (0.639)(R 0.718, F 0.560)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.081] [G acc: 0.078]\n",
      "1467 [D loss: (0.515)(R 0.533, F 0.497)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.160] [G acc: 0.219]\n",
      "1468 [D loss: (0.638)(R 0.652, F 0.623)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.172] [G acc: 0.109]\n",
      "1469 [D loss: (0.678)(R 0.594, F 0.761)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.347] [G acc: 0.094]\n",
      "1470 [D loss: (0.571)(R 0.681, F 0.460)] [D acc: (0.742)(0.547, 0.938)] [G loss: 1.252] [G acc: 0.141]\n",
      "1471 [D loss: (0.525)(R 0.453, F 0.597)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.128] [G acc: 0.219]\n",
      "1472 [D loss: (0.533)(R 0.535, F 0.530)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.276] [G acc: 0.109]\n",
      "1473 [D loss: (0.591)(R 0.649, F 0.534)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.191] [G acc: 0.094]\n",
      "1474 [D loss: (0.594)(R 0.515, F 0.674)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.113] [G acc: 0.203]\n",
      "1475 [D loss: (0.615)(R 0.592, F 0.637)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.198] [G acc: 0.094]\n",
      "1476 [D loss: (0.547)(R 0.624, F 0.471)] [D acc: (0.742)(0.594, 0.891)] [G loss: 1.371] [G acc: 0.109]\n",
      "1477 [D loss: (0.585)(R 0.612, F 0.557)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.201] [G acc: 0.094]\n",
      "1478 [D loss: (0.546)(R 0.493, F 0.598)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.295] [G acc: 0.094]\n",
      "1479 [D loss: (0.561)(R 0.474, F 0.649)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.257] [G acc: 0.094]\n",
      "1480 [D loss: (0.529)(R 0.611, F 0.447)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.312] [G acc: 0.125]\n",
      "1481 [D loss: (0.563)(R 0.533, F 0.594)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.184] [G acc: 0.156]\n",
      "1482 [D loss: (0.643)(R 0.741, F 0.546)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.144] [G acc: 0.141]\n",
      "1483 [D loss: (0.530)(R 0.537, F 0.522)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.286] [G acc: 0.172]\n",
      "1484 [D loss: (0.519)(R 0.444, F 0.593)] [D acc: (0.750)(0.766, 0.734)] [G loss: 1.370] [G acc: 0.109]\n",
      "1485 [D loss: (0.574)(R 0.502, F 0.645)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.134] [G acc: 0.188]\n",
      "1486 [D loss: (0.613)(R 0.701, F 0.526)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.173] [G acc: 0.172]\n",
      "1487 [D loss: (0.597)(R 0.566, F 0.628)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.194] [G acc: 0.125]\n",
      "1488 [D loss: (0.588)(R 0.568, F 0.609)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.116] [G acc: 0.203]\n",
      "1489 [D loss: (0.594)(R 0.586, F 0.603)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.153] [G acc: 0.172]\n",
      "1490 [D loss: (0.594)(R 0.567, F 0.620)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.236] [G acc: 0.125]\n",
      "1491 [D loss: (0.517)(R 0.493, F 0.540)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.269] [G acc: 0.141]\n",
      "1492 [D loss: (0.540)(R 0.557, F 0.523)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.250] [G acc: 0.141]\n",
      "1493 [D loss: (0.617)(R 0.686, F 0.548)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.188] [G acc: 0.141]\n",
      "1494 [D loss: (0.579)(R 0.541, F 0.618)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.187] [G acc: 0.141]\n",
      "1495 [D loss: (0.607)(R 0.621, F 0.592)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.195] [G acc: 0.109]\n",
      "1496 [D loss: (0.614)(R 0.610, F 0.617)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.287] [G acc: 0.031]\n",
      "1497 [D loss: (0.537)(R 0.482, F 0.591)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.134] [G acc: 0.141]\n",
      "1498 [D loss: (0.617)(R 0.631, F 0.604)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.261] [G acc: 0.078]\n",
      "1499 [D loss: (0.563)(R 0.608, F 0.518)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.225] [G acc: 0.062]\n",
      "1500 [D loss: (0.585)(R 0.595, F 0.576)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.154] [G acc: 0.172]\n",
      "1501 [D loss: (0.602)(R 0.575, F 0.629)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.197] [G acc: 0.141]\n",
      "1502 [D loss: (0.567)(R 0.532, F 0.602)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.212] [G acc: 0.141]\n",
      "1503 [D loss: (0.481)(R 0.491, F 0.471)] [D acc: (0.797)(0.750, 0.844)] [G loss: 1.163] [G acc: 0.172]\n",
      "1504 [D loss: (0.571)(R 0.540, F 0.602)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.520] [G acc: 0.109]\n",
      "1505 [D loss: (0.668)(R 0.741, F 0.595)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.090] [G acc: 0.156]\n",
      "1506 [D loss: (0.606)(R 0.683, F 0.529)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.151] [G acc: 0.125]\n",
      "1507 [D loss: (0.569)(R 0.461, F 0.677)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.232] [G acc: 0.094]\n",
      "1508 [D loss: (0.520)(R 0.557, F 0.483)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.236] [G acc: 0.062]\n",
      "1509 [D loss: (0.566)(R 0.519, F 0.613)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.168] [G acc: 0.156]\n",
      "1510 [D loss: (0.622)(R 0.653, F 0.591)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.340] [G acc: 0.031]\n",
      "1511 [D loss: (0.477)(R 0.531, F 0.424)] [D acc: (0.750)(0.609, 0.891)] [G loss: 1.292] [G acc: 0.156]\n",
      "1512 [D loss: (0.575)(R 0.508, F 0.642)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.221] [G acc: 0.078]\n",
      "1513 [D loss: (0.632)(R 0.584, F 0.680)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.383] [G acc: 0.094]\n",
      "1514 [D loss: (0.571)(R 0.575, F 0.566)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.254] [G acc: 0.125]\n",
      "1515 [D loss: (0.581)(R 0.610, F 0.551)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.262] [G acc: 0.125]\n",
      "1516 [D loss: (0.533)(R 0.567, F 0.498)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.074] [G acc: 0.203]\n",
      "1517 [D loss: (0.521)(R 0.544, F 0.499)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.105] [G acc: 0.203]\n",
      "1518 [D loss: (0.528)(R 0.504, F 0.551)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.242] [G acc: 0.078]\n",
      "1519 [D loss: (0.583)(R 0.508, F 0.659)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.187] [G acc: 0.156]\n",
      "1520 [D loss: (0.665)(R 0.709, F 0.622)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.165] [G acc: 0.109]\n",
      "1521 [D loss: (0.489)(R 0.462, F 0.517)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.242] [G acc: 0.188]\n",
      "1522 [D loss: (0.546)(R 0.505, F 0.588)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.318] [G acc: 0.109]\n",
      "1523 [D loss: (0.593)(R 0.581, F 0.604)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.360] [G acc: 0.047]\n",
      "1524 [D loss: (0.550)(R 0.589, F 0.511)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.346] [G acc: 0.078]\n",
      "1525 [D loss: (0.611)(R 0.591, F 0.631)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.244] [G acc: 0.188]\n",
      "1526 [D loss: (0.608)(R 0.605, F 0.611)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.212] [G acc: 0.094]\n",
      "1527 [D loss: (0.572)(R 0.617, F 0.527)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.172] [G acc: 0.141]\n",
      "1528 [D loss: (0.648)(R 0.711, F 0.585)] [D acc: (0.594)(0.469, 0.719)] [G loss: 1.208] [G acc: 0.141]\n",
      "1529 [D loss: (0.528)(R 0.528, F 0.528)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.299] [G acc: 0.156]\n",
      "1530 [D loss: (0.597)(R 0.517, F 0.678)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.245] [G acc: 0.156]\n",
      "1531 [D loss: (0.596)(R 0.651, F 0.541)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.225] [G acc: 0.078]\n",
      "1532 [D loss: (0.518)(R 0.480, F 0.556)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.236] [G acc: 0.109]\n",
      "1533 [D loss: (0.531)(R 0.514, F 0.548)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.324] [G acc: 0.172]\n",
      "1534 [D loss: (0.572)(R 0.528, F 0.616)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.355] [G acc: 0.062]\n",
      "1535 [D loss: (0.541)(R 0.624, F 0.459)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.284] [G acc: 0.094]\n",
      "1536 [D loss: (0.632)(R 0.651, F 0.614)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.156] [G acc: 0.094]\n",
      "1537 [D loss: (0.644)(R 0.660, F 0.628)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.228] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538 [D loss: (0.639)(R 0.530, F 0.747)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.160] [G acc: 0.094]\n",
      "1539 [D loss: (0.644)(R 0.655, F 0.634)] [D acc: (0.586)(0.516, 0.656)] [G loss: 1.121] [G acc: 0.188]\n",
      "1540 [D loss: (0.511)(R 0.512, F 0.510)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.113] [G acc: 0.188]\n",
      "1541 [D loss: (0.554)(R 0.520, F 0.588)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.229] [G acc: 0.078]\n",
      "1542 [D loss: (0.500)(R 0.525, F 0.475)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.295] [G acc: 0.156]\n",
      "1543 [D loss: (0.611)(R 0.633, F 0.588)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.104] [G acc: 0.172]\n",
      "1544 [D loss: (0.537)(R 0.526, F 0.547)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.172] [G acc: 0.125]\n",
      "1545 [D loss: (0.564)(R 0.554, F 0.574)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.196] [G acc: 0.203]\n",
      "1546 [D loss: (0.619)(R 0.655, F 0.584)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.261] [G acc: 0.188]\n",
      "1547 [D loss: (0.659)(R 0.799, F 0.520)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.175] [G acc: 0.062]\n",
      "1548 [D loss: (0.512)(R 0.547, F 0.478)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.095] [G acc: 0.156]\n",
      "1549 [D loss: (0.592)(R 0.502, F 0.682)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.193] [G acc: 0.172]\n",
      "1550 [D loss: (0.532)(R 0.521, F 0.544)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.200] [G acc: 0.094]\n",
      "1551 [D loss: (0.587)(R 0.559, F 0.615)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.304] [G acc: 0.141]\n",
      "1552 [D loss: (0.547)(R 0.495, F 0.599)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.328] [G acc: 0.141]\n",
      "1553 [D loss: (0.507)(R 0.571, F 0.444)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.345] [G acc: 0.156]\n",
      "1554 [D loss: (0.591)(R 0.502, F 0.680)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.397] [G acc: 0.094]\n",
      "1555 [D loss: (0.596)(R 0.687, F 0.505)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.261] [G acc: 0.109]\n",
      "1556 [D loss: (0.517)(R 0.488, F 0.545)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.425] [G acc: 0.078]\n",
      "1557 [D loss: (0.591)(R 0.490, F 0.691)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.151] [G acc: 0.156]\n",
      "1558 [D loss: (0.533)(R 0.541, F 0.524)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.372] [G acc: 0.078]\n",
      "1559 [D loss: (0.538)(R 0.531, F 0.546)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.301] [G acc: 0.109]\n",
      "1560 [D loss: (0.507)(R 0.582, F 0.431)] [D acc: (0.750)(0.609, 0.891)] [G loss: 1.280] [G acc: 0.109]\n",
      "1561 [D loss: (0.580)(R 0.561, F 0.600)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.436] [G acc: 0.062]\n",
      "1562 [D loss: (0.492)(R 0.524, F 0.461)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.252] [G acc: 0.125]\n",
      "1563 [D loss: (0.579)(R 0.471, F 0.687)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.266] [G acc: 0.047]\n",
      "1564 [D loss: (0.550)(R 0.600, F 0.499)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.306] [G acc: 0.109]\n",
      "1565 [D loss: (0.535)(R 0.539, F 0.531)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.473] [G acc: 0.062]\n",
      "1566 [D loss: (0.601)(R 0.587, F 0.615)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.219] [G acc: 0.141]\n",
      "1567 [D loss: (0.567)(R 0.600, F 0.534)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.231] [G acc: 0.141]\n",
      "1568 [D loss: (0.638)(R 0.542, F 0.734)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.380] [G acc: 0.125]\n",
      "1569 [D loss: (0.539)(R 0.548, F 0.530)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.216] [G acc: 0.078]\n",
      "1570 [D loss: (0.506)(R 0.467, F 0.545)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.492] [G acc: 0.062]\n",
      "1571 [D loss: (0.597)(R 0.647, F 0.548)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.268] [G acc: 0.203]\n",
      "1572 [D loss: (0.537)(R 0.548, F 0.526)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.414] [G acc: 0.125]\n",
      "1573 [D loss: (0.595)(R 0.543, F 0.646)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.330] [G acc: 0.141]\n",
      "1574 [D loss: (0.572)(R 0.600, F 0.545)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.384] [G acc: 0.141]\n",
      "1575 [D loss: (0.518)(R 0.446, F 0.591)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.399] [G acc: 0.094]\n",
      "1576 [D loss: (0.613)(R 0.614, F 0.612)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.242] [G acc: 0.156]\n",
      "1577 [D loss: (0.632)(R 0.568, F 0.695)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.216] [G acc: 0.172]\n",
      "1578 [D loss: (0.613)(R 0.632, F 0.594)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.246] [G acc: 0.062]\n",
      "1579 [D loss: (0.482)(R 0.463, F 0.502)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.233] [G acc: 0.156]\n",
      "1580 [D loss: (0.550)(R 0.550, F 0.550)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.308] [G acc: 0.156]\n",
      "1581 [D loss: (0.592)(R 0.594, F 0.590)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.315] [G acc: 0.141]\n",
      "1582 [D loss: (0.598)(R 0.619, F 0.576)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.295] [G acc: 0.078]\n",
      "1583 [D loss: (0.675)(R 0.655, F 0.694)] [D acc: (0.617)(0.609, 0.625)] [G loss: 1.325] [G acc: 0.078]\n",
      "1584 [D loss: (0.516)(R 0.560, F 0.472)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.168] [G acc: 0.078]\n",
      "1585 [D loss: (0.477)(R 0.511, F 0.443)] [D acc: (0.789)(0.672, 0.906)] [G loss: 1.320] [G acc: 0.094]\n",
      "1586 [D loss: (0.622)(R 0.596, F 0.648)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.269] [G acc: 0.078]\n",
      "1587 [D loss: (0.586)(R 0.608, F 0.563)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.252] [G acc: 0.141]\n",
      "1588 [D loss: (0.614)(R 0.614, F 0.615)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.296] [G acc: 0.172]\n",
      "1589 [D loss: (0.525)(R 0.436, F 0.613)] [D acc: (0.766)(0.766, 0.766)] [G loss: 1.374] [G acc: 0.031]\n",
      "1590 [D loss: (0.547)(R 0.588, F 0.506)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.344] [G acc: 0.125]\n",
      "1591 [D loss: (0.551)(R 0.505, F 0.598)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.343] [G acc: 0.062]\n",
      "1592 [D loss: (0.615)(R 0.693, F 0.536)] [D acc: (0.695)(0.500, 0.891)] [G loss: 1.350] [G acc: 0.047]\n",
      "1593 [D loss: (0.530)(R 0.478, F 0.582)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.333] [G acc: 0.125]\n",
      "1594 [D loss: (0.502)(R 0.417, F 0.587)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.340] [G acc: 0.094]\n",
      "1595 [D loss: (0.558)(R 0.589, F 0.526)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.321] [G acc: 0.078]\n",
      "1596 [D loss: (0.632)(R 0.655, F 0.608)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.230] [G acc: 0.109]\n",
      "1597 [D loss: (0.591)(R 0.607, F 0.575)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.198] [G acc: 0.125]\n",
      "1598 [D loss: (0.646)(R 0.678, F 0.614)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.152] [G acc: 0.078]\n",
      "1599 [D loss: (0.547)(R 0.515, F 0.580)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.305] [G acc: 0.109]\n",
      "1600 [D loss: (0.587)(R 0.638, F 0.536)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.281] [G acc: 0.172]\n",
      "1601 [D loss: (0.656)(R 0.693, F 0.618)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.124] [G acc: 0.156]\n",
      "1602 [D loss: (0.564)(R 0.576, F 0.552)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.129] [G acc: 0.219]\n",
      "1603 [D loss: (0.635)(R 0.477, F 0.793)] [D acc: (0.625)(0.672, 0.578)] [G loss: 1.169] [G acc: 0.156]\n",
      "1604 [D loss: (0.549)(R 0.566, F 0.532)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.151] [G acc: 0.141]\n",
      "1605 [D loss: (0.636)(R 0.661, F 0.611)] [D acc: (0.602)(0.500, 0.703)] [G loss: 1.129] [G acc: 0.109]\n",
      "1606 [D loss: (0.519)(R 0.551, F 0.487)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.164] [G acc: 0.125]\n",
      "1607 [D loss: (0.587)(R 0.619, F 0.556)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.380] [G acc: 0.078]\n",
      "1608 [D loss: (0.511)(R 0.471, F 0.552)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.261] [G acc: 0.141]\n",
      "1609 [D loss: (0.602)(R 0.601, F 0.602)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.208] [G acc: 0.141]\n",
      "1610 [D loss: (0.561)(R 0.527, F 0.596)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.139] [G acc: 0.172]\n",
      "1611 [D loss: (0.588)(R 0.603, F 0.573)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.310] [G acc: 0.141]\n",
      "1612 [D loss: (0.533)(R 0.514, F 0.552)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.338] [G acc: 0.078]\n",
      "1613 [D loss: (0.530)(R 0.523, F 0.536)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.269] [G acc: 0.156]\n",
      "1614 [D loss: (0.587)(R 0.580, F 0.594)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.162] [G acc: 0.141]\n",
      "1615 [D loss: (0.584)(R 0.575, F 0.593)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.297] [G acc: 0.094]\n",
      "1616 [D loss: (0.642)(R 0.615, F 0.669)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.116] [G acc: 0.125]\n",
      "1617 [D loss: (0.560)(R 0.573, F 0.547)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.118] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1618 [D loss: (0.569)(R 0.611, F 0.526)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.213] [G acc: 0.094]\n",
      "1619 [D loss: (0.644)(R 0.686, F 0.601)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.186] [G acc: 0.203]\n",
      "1620 [D loss: (0.602)(R 0.560, F 0.645)] [D acc: (0.656)(0.719, 0.594)] [G loss: 1.160] [G acc: 0.125]\n",
      "1621 [D loss: (0.545)(R 0.450, F 0.639)] [D acc: (0.766)(0.781, 0.750)] [G loss: 1.170] [G acc: 0.125]\n",
      "1622 [D loss: (0.551)(R 0.560, F 0.543)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.151] [G acc: 0.156]\n",
      "1623 [D loss: (0.565)(R 0.596, F 0.535)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.177] [G acc: 0.188]\n",
      "1624 [D loss: (0.629)(R 0.630, F 0.628)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.334] [G acc: 0.062]\n",
      "1625 [D loss: (0.543)(R 0.606, F 0.479)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.158] [G acc: 0.203]\n",
      "1626 [D loss: (0.632)(R 0.637, F 0.628)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.150] [G acc: 0.109]\n",
      "1627 [D loss: (0.631)(R 0.726, F 0.536)] [D acc: (0.625)(0.469, 0.781)] [G loss: 1.068] [G acc: 0.109]\n",
      "1628 [D loss: (0.517)(R 0.552, F 0.482)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.206] [G acc: 0.172]\n",
      "1629 [D loss: (0.517)(R 0.494, F 0.540)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.313] [G acc: 0.109]\n",
      "1630 [D loss: (0.555)(R 0.530, F 0.581)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.264] [G acc: 0.156]\n",
      "1631 [D loss: (0.581)(R 0.537, F 0.626)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.256] [G acc: 0.141]\n",
      "1632 [D loss: (0.620)(R 0.653, F 0.586)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.202] [G acc: 0.109]\n",
      "1633 [D loss: (0.567)(R 0.598, F 0.536)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.236] [G acc: 0.141]\n",
      "1634 [D loss: (0.579)(R 0.602, F 0.556)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.158] [G acc: 0.125]\n",
      "1635 [D loss: (0.523)(R 0.511, F 0.535)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.296] [G acc: 0.109]\n",
      "1636 [D loss: (0.627)(R 0.524, F 0.731)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.246] [G acc: 0.125]\n",
      "1637 [D loss: (0.636)(R 0.687, F 0.585)] [D acc: (0.633)(0.484, 0.781)] [G loss: 1.128] [G acc: 0.109]\n",
      "1638 [D loss: (0.616)(R 0.641, F 0.592)] [D acc: (0.617)(0.531, 0.703)] [G loss: 1.277] [G acc: 0.141]\n",
      "1639 [D loss: (0.568)(R 0.619, F 0.518)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.008] [G acc: 0.234]\n",
      "1640 [D loss: (0.574)(R 0.542, F 0.605)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.158] [G acc: 0.172]\n",
      "1641 [D loss: (0.545)(R 0.526, F 0.564)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.211] [G acc: 0.094]\n",
      "1642 [D loss: (0.613)(R 0.518, F 0.708)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.253] [G acc: 0.141]\n",
      "1643 [D loss: (0.498)(R 0.474, F 0.523)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.209] [G acc: 0.109]\n",
      "1644 [D loss: (0.601)(R 0.633, F 0.570)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.136] [G acc: 0.203]\n",
      "1645 [D loss: (0.579)(R 0.584, F 0.574)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.355] [G acc: 0.094]\n",
      "1646 [D loss: (0.533)(R 0.534, F 0.533)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.331] [G acc: 0.078]\n",
      "1647 [D loss: (0.596)(R 0.556, F 0.635)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.275] [G acc: 0.109]\n",
      "1648 [D loss: (0.565)(R 0.591, F 0.540)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.202] [G acc: 0.172]\n",
      "1649 [D loss: (0.625)(R 0.633, F 0.616)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.240] [G acc: 0.172]\n",
      "1650 [D loss: (0.595)(R 0.596, F 0.595)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.137] [G acc: 0.188]\n",
      "1651 [D loss: (0.538)(R 0.525, F 0.551)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.259] [G acc: 0.156]\n",
      "1652 [D loss: (0.606)(R 0.487, F 0.726)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.272] [G acc: 0.094]\n",
      "1653 [D loss: (0.596)(R 0.577, F 0.615)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.286] [G acc: 0.047]\n",
      "1654 [D loss: (0.570)(R 0.621, F 0.519)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.307] [G acc: 0.078]\n",
      "1655 [D loss: (0.572)(R 0.628, F 0.517)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.319] [G acc: 0.062]\n",
      "1656 [D loss: (0.495)(R 0.521, F 0.470)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.206] [G acc: 0.188]\n",
      "1657 [D loss: (0.546)(R 0.498, F 0.594)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.216] [G acc: 0.203]\n",
      "1658 [D loss: (0.517)(R 0.541, F 0.492)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.366] [G acc: 0.078]\n",
      "1659 [D loss: (0.751)(R 0.713, F 0.788)] [D acc: (0.594)(0.578, 0.609)] [G loss: 1.289] [G acc: 0.047]\n",
      "1660 [D loss: (0.552)(R 0.622, F 0.482)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.162] [G acc: 0.078]\n",
      "1661 [D loss: (0.617)(R 0.604, F 0.630)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.232] [G acc: 0.109]\n",
      "1662 [D loss: (0.584)(R 0.649, F 0.519)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.199] [G acc: 0.125]\n",
      "1663 [D loss: (0.571)(R 0.510, F 0.632)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.110] [G acc: 0.109]\n",
      "1664 [D loss: (0.551)(R 0.586, F 0.517)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.280] [G acc: 0.125]\n",
      "1665 [D loss: (0.593)(R 0.659, F 0.526)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.153] [G acc: 0.125]\n",
      "1666 [D loss: (0.659)(R 0.661, F 0.656)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.121] [G acc: 0.141]\n",
      "1667 [D loss: (0.618)(R 0.679, F 0.557)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.108] [G acc: 0.156]\n",
      "1668 [D loss: (0.683)(R 0.604, F 0.761)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.172] [G acc: 0.094]\n",
      "1669 [D loss: (0.612)(R 0.664, F 0.560)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.028] [G acc: 0.109]\n",
      "1670 [D loss: (0.596)(R 0.624, F 0.568)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.082] [G acc: 0.219]\n",
      "1671 [D loss: (0.552)(R 0.544, F 0.559)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.041] [G acc: 0.156]\n",
      "1672 [D loss: (0.587)(R 0.632, F 0.542)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.172] [G acc: 0.125]\n",
      "1673 [D loss: (0.564)(R 0.545, F 0.583)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.048] [G acc: 0.203]\n",
      "1674 [D loss: (0.487)(R 0.376, F 0.597)] [D acc: (0.750)(0.781, 0.719)] [G loss: 1.296] [G acc: 0.094]\n",
      "1675 [D loss: (0.579)(R 0.591, F 0.567)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.363] [G acc: 0.094]\n",
      "1676 [D loss: (0.607)(R 0.589, F 0.624)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.085] [G acc: 0.250]\n",
      "1677 [D loss: (0.585)(R 0.574, F 0.596)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.273] [G acc: 0.125]\n",
      "1678 [D loss: (0.608)(R 0.735, F 0.481)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.138] [G acc: 0.141]\n",
      "1679 [D loss: (0.509)(R 0.483, F 0.535)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.219] [G acc: 0.031]\n",
      "1680 [D loss: (0.620)(R 0.556, F 0.684)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.077] [G acc: 0.125]\n",
      "1681 [D loss: (0.534)(R 0.521, F 0.547)] [D acc: (0.750)(0.766, 0.734)] [G loss: 1.270] [G acc: 0.125]\n",
      "1682 [D loss: (0.484)(R 0.474, F 0.494)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.385] [G acc: 0.094]\n",
      "1683 [D loss: (0.605)(R 0.621, F 0.589)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.117] [G acc: 0.219]\n",
      "1684 [D loss: (0.588)(R 0.651, F 0.525)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.251] [G acc: 0.109]\n",
      "1685 [D loss: (0.619)(R 0.575, F 0.663)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.204] [G acc: 0.094]\n",
      "1686 [D loss: (0.541)(R 0.502, F 0.580)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.214] [G acc: 0.141]\n",
      "1687 [D loss: (0.676)(R 0.631, F 0.720)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.130] [G acc: 0.109]\n",
      "1688 [D loss: (0.528)(R 0.593, F 0.462)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.201] [G acc: 0.125]\n",
      "1689 [D loss: (0.594)(R 0.635, F 0.552)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.116] [G acc: 0.188]\n",
      "1690 [D loss: (0.590)(R 0.589, F 0.592)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.323] [G acc: 0.094]\n",
      "1691 [D loss: (0.558)(R 0.570, F 0.546)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.171] [G acc: 0.062]\n",
      "1692 [D loss: (0.645)(R 0.703, F 0.587)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.243] [G acc: 0.094]\n",
      "1693 [D loss: (0.605)(R 0.656, F 0.555)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.089] [G acc: 0.141]\n",
      "1694 [D loss: (0.640)(R 0.467, F 0.814)] [D acc: (0.695)(0.734, 0.656)] [G loss: 1.094] [G acc: 0.109]\n",
      "1695 [D loss: (0.540)(R 0.615, F 0.466)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.244] [G acc: 0.125]\n",
      "1696 [D loss: (0.644)(R 0.728, F 0.560)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.151] [G acc: 0.125]\n",
      "1697 [D loss: (0.594)(R 0.569, F 0.619)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.075] [G acc: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698 [D loss: (0.542)(R 0.437, F 0.647)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.146] [G acc: 0.141]\n",
      "1699 [D loss: (0.575)(R 0.553, F 0.597)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.229] [G acc: 0.078]\n",
      "1700 [D loss: (0.617)(R 0.567, F 0.668)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.139] [G acc: 0.125]\n",
      "1701 [D loss: (0.547)(R 0.593, F 0.500)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.268] [G acc: 0.094]\n",
      "1702 [D loss: (0.548)(R 0.481, F 0.615)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.221] [G acc: 0.141]\n",
      "1703 [D loss: (0.485)(R 0.449, F 0.521)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.381] [G acc: 0.078]\n",
      "1704 [D loss: (0.555)(R 0.575, F 0.536)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.300] [G acc: 0.109]\n",
      "1705 [D loss: (0.558)(R 0.498, F 0.619)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.260] [G acc: 0.203]\n",
      "1706 [D loss: (0.572)(R 0.634, F 0.510)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.280] [G acc: 0.062]\n",
      "1707 [D loss: (0.520)(R 0.545, F 0.496)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.218] [G acc: 0.188]\n",
      "1708 [D loss: (0.558)(R 0.557, F 0.560)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.326] [G acc: 0.156]\n",
      "1709 [D loss: (0.558)(R 0.522, F 0.594)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.272] [G acc: 0.172]\n",
      "1710 [D loss: (0.550)(R 0.556, F 0.545)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.254] [G acc: 0.172]\n",
      "1711 [D loss: (0.562)(R 0.514, F 0.610)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.451] [G acc: 0.094]\n",
      "1712 [D loss: (0.603)(R 0.683, F 0.523)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.305] [G acc: 0.125]\n",
      "1713 [D loss: (0.551)(R 0.544, F 0.558)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.235] [G acc: 0.109]\n",
      "1714 [D loss: (0.648)(R 0.579, F 0.718)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.399] [G acc: 0.047]\n",
      "1715 [D loss: (0.630)(R 0.741, F 0.519)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.183] [G acc: 0.156]\n",
      "1716 [D loss: (0.547)(R 0.509, F 0.586)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.213] [G acc: 0.156]\n",
      "1717 [D loss: (0.569)(R 0.541, F 0.598)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.153] [G acc: 0.141]\n",
      "1718 [D loss: (0.625)(R 0.566, F 0.685)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.181] [G acc: 0.172]\n",
      "1719 [D loss: (0.598)(R 0.652, F 0.545)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.131] [G acc: 0.141]\n",
      "1720 [D loss: (0.573)(R 0.689, F 0.457)] [D acc: (0.688)(0.516, 0.859)] [G loss: 1.158] [G acc: 0.109]\n",
      "1721 [D loss: (0.540)(R 0.501, F 0.578)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.201] [G acc: 0.125]\n",
      "1722 [D loss: (0.505)(R 0.468, F 0.542)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.267] [G acc: 0.125]\n",
      "1723 [D loss: (0.513)(R 0.505, F 0.521)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.170] [G acc: 0.172]\n",
      "1724 [D loss: (0.576)(R 0.574, F 0.578)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.142] [G acc: 0.156]\n",
      "1725 [D loss: (0.539)(R 0.539, F 0.539)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.078] [G acc: 0.234]\n",
      "1726 [D loss: (0.619)(R 0.518, F 0.720)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.214] [G acc: 0.141]\n",
      "1727 [D loss: (0.561)(R 0.611, F 0.510)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.282] [G acc: 0.141]\n",
      "1728 [D loss: (0.493)(R 0.455, F 0.531)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.196] [G acc: 0.141]\n",
      "1729 [D loss: (0.567)(R 0.566, F 0.568)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.412] [G acc: 0.094]\n",
      "1730 [D loss: (0.573)(R 0.524, F 0.623)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.257] [G acc: 0.062]\n",
      "1731 [D loss: (0.544)(R 0.650, F 0.438)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.105] [G acc: 0.109]\n",
      "1732 [D loss: (0.604)(R 0.566, F 0.643)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.224] [G acc: 0.109]\n",
      "1733 [D loss: (0.653)(R 0.618, F 0.689)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.342] [G acc: 0.062]\n",
      "1734 [D loss: (0.645)(R 0.715, F 0.576)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.118] [G acc: 0.125]\n",
      "1735 [D loss: (0.518)(R 0.501, F 0.534)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.040] [G acc: 0.141]\n",
      "1736 [D loss: (0.564)(R 0.534, F 0.594)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.148] [G acc: 0.094]\n",
      "1737 [D loss: (0.577)(R 0.605, F 0.549)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.144] [G acc: 0.125]\n",
      "1738 [D loss: (0.585)(R 0.508, F 0.663)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.272] [G acc: 0.078]\n",
      "1739 [D loss: (0.555)(R 0.626, F 0.484)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.192] [G acc: 0.094]\n",
      "1740 [D loss: (0.652)(R 0.619, F 0.685)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.199] [G acc: 0.094]\n",
      "1741 [D loss: (0.644)(R 0.677, F 0.610)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.200] [G acc: 0.078]\n",
      "1742 [D loss: (0.586)(R 0.653, F 0.520)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.066] [G acc: 0.141]\n",
      "1743 [D loss: (0.614)(R 0.517, F 0.710)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.252] [G acc: 0.047]\n",
      "1744 [D loss: (0.514)(R 0.506, F 0.521)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.233] [G acc: 0.094]\n",
      "1745 [D loss: (0.512)(R 0.547, F 0.477)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.195] [G acc: 0.188]\n",
      "1746 [D loss: (0.617)(R 0.684, F 0.549)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.284] [G acc: 0.109]\n",
      "1747 [D loss: (0.571)(R 0.557, F 0.585)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.243] [G acc: 0.188]\n",
      "1748 [D loss: (0.647)(R 0.611, F 0.683)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.146] [G acc: 0.188]\n",
      "1749 [D loss: (0.623)(R 0.723, F 0.522)] [D acc: (0.633)(0.453, 0.812)] [G loss: 1.116] [G acc: 0.094]\n",
      "1750 [D loss: (0.466)(R 0.454, F 0.477)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.183] [G acc: 0.172]\n",
      "1751 [D loss: (0.574)(R 0.518, F 0.630)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.204] [G acc: 0.156]\n",
      "1752 [D loss: (0.580)(R 0.542, F 0.617)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.322] [G acc: 0.062]\n",
      "1753 [D loss: (0.570)(R 0.622, F 0.517)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.350] [G acc: 0.094]\n",
      "1754 [D loss: (0.517)(R 0.433, F 0.601)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.256] [G acc: 0.141]\n",
      "1755 [D loss: (0.668)(R 0.674, F 0.661)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.303] [G acc: 0.109]\n",
      "1756 [D loss: (0.612)(R 0.667, F 0.556)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.298] [G acc: 0.109]\n",
      "1757 [D loss: (0.597)(R 0.595, F 0.600)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.245] [G acc: 0.141]\n",
      "1758 [D loss: (0.589)(R 0.666, F 0.512)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.190] [G acc: 0.125]\n",
      "1759 [D loss: (0.564)(R 0.604, F 0.524)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.238] [G acc: 0.172]\n",
      "1760 [D loss: (0.543)(R 0.481, F 0.605)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.252] [G acc: 0.203]\n",
      "1761 [D loss: (0.514)(R 0.540, F 0.488)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.120] [G acc: 0.188]\n",
      "1762 [D loss: (0.605)(R 0.514, F 0.695)] [D acc: (0.703)(0.750, 0.656)] [G loss: 1.237] [G acc: 0.125]\n",
      "1763 [D loss: (0.573)(R 0.664, F 0.481)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.335] [G acc: 0.125]\n",
      "1764 [D loss: (0.622)(R 0.613, F 0.631)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.182] [G acc: 0.125]\n",
      "1765 [D loss: (0.627)(R 0.605, F 0.649)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.200] [G acc: 0.109]\n",
      "1766 [D loss: (0.596)(R 0.688, F 0.505)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.199] [G acc: 0.109]\n",
      "1767 [D loss: (0.486)(R 0.436, F 0.536)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.174] [G acc: 0.219]\n",
      "1768 [D loss: (0.556)(R 0.490, F 0.622)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.329] [G acc: 0.141]\n",
      "1769 [D loss: (0.521)(R 0.524, F 0.518)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.149] [G acc: 0.188]\n",
      "1770 [D loss: (0.609)(R 0.616, F 0.602)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.239] [G acc: 0.172]\n",
      "1771 [D loss: (0.615)(R 0.638, F 0.592)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.218] [G acc: 0.156]\n",
      "1772 [D loss: (0.603)(R 0.586, F 0.619)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.338] [G acc: 0.094]\n",
      "1773 [D loss: (0.578)(R 0.628, F 0.528)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.292] [G acc: 0.109]\n",
      "1774 [D loss: (0.578)(R 0.545, F 0.611)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.341] [G acc: 0.109]\n",
      "1775 [D loss: (0.554)(R 0.534, F 0.574)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.186] [G acc: 0.125]\n",
      "1776 [D loss: (0.612)(R 0.529, F 0.695)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.249] [G acc: 0.141]\n",
      "1777 [D loss: (0.526)(R 0.514, F 0.538)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.221] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1778 [D loss: (0.579)(R 0.630, F 0.529)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.222] [G acc: 0.125]\n",
      "1779 [D loss: (0.612)(R 0.676, F 0.549)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.331] [G acc: 0.094]\n",
      "1780 [D loss: (0.641)(R 0.613, F 0.669)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.216] [G acc: 0.062]\n",
      "1781 [D loss: (0.475)(R 0.496, F 0.455)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.287] [G acc: 0.125]\n",
      "1782 [D loss: (0.596)(R 0.624, F 0.568)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.161] [G acc: 0.094]\n",
      "1783 [D loss: (0.581)(R 0.579, F 0.582)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.205] [G acc: 0.141]\n",
      "1784 [D loss: (0.597)(R 0.515, F 0.678)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.285] [G acc: 0.109]\n",
      "1785 [D loss: (0.614)(R 0.603, F 0.624)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.207] [G acc: 0.125]\n",
      "1786 [D loss: (0.585)(R 0.627, F 0.544)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.417] [G acc: 0.141]\n",
      "1787 [D loss: (0.481)(R 0.414, F 0.547)] [D acc: (0.781)(0.797, 0.766)] [G loss: 1.371] [G acc: 0.125]\n",
      "1788 [D loss: (0.659)(R 0.609, F 0.710)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.228] [G acc: 0.156]\n",
      "1789 [D loss: (0.627)(R 0.611, F 0.643)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.218] [G acc: 0.125]\n",
      "1790 [D loss: (0.564)(R 0.511, F 0.616)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.324] [G acc: 0.078]\n",
      "1791 [D loss: (0.527)(R 0.571, F 0.483)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.288] [G acc: 0.125]\n",
      "1792 [D loss: (0.622)(R 0.694, F 0.550)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.146] [G acc: 0.172]\n",
      "1793 [D loss: (0.589)(R 0.651, F 0.526)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.199] [G acc: 0.141]\n",
      "1794 [D loss: (0.539)(R 0.439, F 0.639)] [D acc: (0.711)(0.781, 0.641)] [G loss: 1.302] [G acc: 0.062]\n",
      "1795 [D loss: (0.580)(R 0.576, F 0.584)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.151] [G acc: 0.188]\n",
      "1796 [D loss: (0.493)(R 0.493, F 0.494)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.262] [G acc: 0.078]\n",
      "1797 [D loss: (0.554)(R 0.530, F 0.577)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.270] [G acc: 0.141]\n",
      "1798 [D loss: (0.611)(R 0.666, F 0.556)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.295] [G acc: 0.062]\n",
      "1799 [D loss: (0.619)(R 0.689, F 0.548)] [D acc: (0.664)(0.500, 0.828)] [G loss: 1.298] [G acc: 0.094]\n",
      "1800 [D loss: (0.503)(R 0.481, F 0.525)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.350] [G acc: 0.109]\n",
      "1801 [D loss: (0.509)(R 0.537, F 0.481)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.325] [G acc: 0.172]\n",
      "1802 [D loss: (0.616)(R 0.580, F 0.652)] [D acc: (0.617)(0.609, 0.625)] [G loss: 1.312] [G acc: 0.188]\n",
      "1803 [D loss: (0.504)(R 0.521, F 0.488)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.339] [G acc: 0.125]\n",
      "1804 [D loss: (0.482)(R 0.507, F 0.457)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.358] [G acc: 0.094]\n",
      "1805 [D loss: (0.626)(R 0.468, F 0.783)] [D acc: (0.680)(0.734, 0.625)] [G loss: 1.409] [G acc: 0.094]\n",
      "1806 [D loss: (0.522)(R 0.553, F 0.492)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.275] [G acc: 0.172]\n",
      "1807 [D loss: (0.637)(R 0.707, F 0.566)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.216] [G acc: 0.156]\n",
      "1808 [D loss: (0.541)(R 0.507, F 0.574)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.260] [G acc: 0.109]\n",
      "1809 [D loss: (0.593)(R 0.672, F 0.515)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.218] [G acc: 0.125]\n",
      "1810 [D loss: (0.558)(R 0.670, F 0.446)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.222] [G acc: 0.078]\n",
      "1811 [D loss: (0.554)(R 0.470, F 0.638)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.258] [G acc: 0.125]\n",
      "1812 [D loss: (0.520)(R 0.508, F 0.532)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.382] [G acc: 0.141]\n",
      "1813 [D loss: (0.572)(R 0.639, F 0.505)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.082] [G acc: 0.312]\n",
      "1814 [D loss: (0.585)(R 0.578, F 0.591)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.188] [G acc: 0.109]\n",
      "1815 [D loss: (0.580)(R 0.509, F 0.651)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.136] [G acc: 0.125]\n",
      "1816 [D loss: (0.587)(R 0.608, F 0.566)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.301] [G acc: 0.156]\n",
      "1817 [D loss: (0.582)(R 0.678, F 0.485)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.070] [G acc: 0.203]\n",
      "1818 [D loss: (0.618)(R 0.590, F 0.646)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.210] [G acc: 0.109]\n",
      "1819 [D loss: (0.515)(R 0.509, F 0.521)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.362] [G acc: 0.047]\n",
      "1820 [D loss: (0.575)(R 0.517, F 0.632)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.261] [G acc: 0.125]\n",
      "1821 [D loss: (0.610)(R 0.622, F 0.597)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.186] [G acc: 0.078]\n",
      "1822 [D loss: (0.530)(R 0.609, F 0.451)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.123] [G acc: 0.219]\n",
      "1823 [D loss: (0.532)(R 0.519, F 0.545)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.183] [G acc: 0.156]\n",
      "1824 [D loss: (0.596)(R 0.607, F 0.586)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.346] [G acc: 0.094]\n",
      "1825 [D loss: (0.552)(R 0.570, F 0.535)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.181] [G acc: 0.094]\n",
      "1826 [D loss: (0.595)(R 0.514, F 0.676)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.332] [G acc: 0.078]\n",
      "1827 [D loss: (0.602)(R 0.567, F 0.637)] [D acc: (0.602)(0.531, 0.672)] [G loss: 1.338] [G acc: 0.094]\n",
      "1828 [D loss: (0.617)(R 0.656, F 0.578)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.148] [G acc: 0.078]\n",
      "1829 [D loss: (0.497)(R 0.579, F 0.414)] [D acc: (0.797)(0.609, 0.984)] [G loss: 1.204] [G acc: 0.172]\n",
      "1830 [D loss: (0.573)(R 0.601, F 0.544)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.210] [G acc: 0.188]\n",
      "1831 [D loss: (0.521)(R 0.451, F 0.590)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.350] [G acc: 0.109]\n",
      "1832 [D loss: (0.594)(R 0.593, F 0.596)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.347] [G acc: 0.094]\n",
      "1833 [D loss: (0.527)(R 0.564, F 0.490)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.354] [G acc: 0.125]\n",
      "1834 [D loss: (0.708)(R 0.682, F 0.734)] [D acc: (0.594)(0.578, 0.609)] [G loss: 1.269] [G acc: 0.109]\n",
      "1835 [D loss: (0.602)(R 0.619, F 0.586)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.294] [G acc: 0.109]\n",
      "1836 [D loss: (0.575)(R 0.540, F 0.611)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.246] [G acc: 0.125]\n",
      "1837 [D loss: (0.532)(R 0.466, F 0.598)] [D acc: (0.750)(0.766, 0.734)] [G loss: 1.345] [G acc: 0.047]\n",
      "1838 [D loss: (0.494)(R 0.553, F 0.434)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.317] [G acc: 0.156]\n",
      "1839 [D loss: (0.529)(R 0.510, F 0.547)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.276] [G acc: 0.156]\n",
      "1840 [D loss: (0.605)(R 0.649, F 0.560)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.277] [G acc: 0.094]\n",
      "1841 [D loss: (0.595)(R 0.599, F 0.590)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.296] [G acc: 0.078]\n",
      "1842 [D loss: (0.503)(R 0.550, F 0.455)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.254] [G acc: 0.172]\n",
      "1843 [D loss: (0.615)(R 0.558, F 0.673)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.224] [G acc: 0.125]\n",
      "1844 [D loss: (0.578)(R 0.618, F 0.539)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.365] [G acc: 0.094]\n",
      "1845 [D loss: (0.557)(R 0.532, F 0.581)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.409] [G acc: 0.062]\n",
      "1846 [D loss: (0.605)(R 0.600, F 0.610)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.153] [G acc: 0.188]\n",
      "1847 [D loss: (0.606)(R 0.568, F 0.644)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.270] [G acc: 0.047]\n",
      "1848 [D loss: (0.575)(R 0.615, F 0.534)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.226] [G acc: 0.141]\n",
      "1849 [D loss: (0.614)(R 0.602, F 0.627)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.318] [G acc: 0.141]\n",
      "1850 [D loss: (0.587)(R 0.667, F 0.507)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.342] [G acc: 0.156]\n",
      "1851 [D loss: (0.573)(R 0.555, F 0.591)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.206] [G acc: 0.188]\n",
      "1852 [D loss: (0.581)(R 0.690, F 0.473)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.117] [G acc: 0.172]\n",
      "1853 [D loss: (0.619)(R 0.556, F 0.682)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.191] [G acc: 0.125]\n",
      "1854 [D loss: (0.499)(R 0.499, F 0.499)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.245] [G acc: 0.188]\n",
      "1855 [D loss: (0.613)(R 0.561, F 0.665)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.182] [G acc: 0.109]\n",
      "1856 [D loss: (0.591)(R 0.683, F 0.499)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.249] [G acc: 0.109]\n",
      "1857 [D loss: (0.568)(R 0.533, F 0.604)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.284] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1858 [D loss: (0.584)(R 0.574, F 0.593)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.278] [G acc: 0.156]\n",
      "1859 [D loss: (0.563)(R 0.631, F 0.495)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.247] [G acc: 0.172]\n",
      "1860 [D loss: (0.590)(R 0.564, F 0.616)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.238] [G acc: 0.156]\n",
      "1861 [D loss: (0.616)(R 0.704, F 0.527)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.310] [G acc: 0.141]\n",
      "1862 [D loss: (0.527)(R 0.561, F 0.492)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.253] [G acc: 0.109]\n",
      "1863 [D loss: (0.527)(R 0.516, F 0.537)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.345] [G acc: 0.109]\n",
      "1864 [D loss: (0.603)(R 0.615, F 0.590)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.309] [G acc: 0.094]\n",
      "1865 [D loss: (0.553)(R 0.619, F 0.486)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.265] [G acc: 0.172]\n",
      "1866 [D loss: (0.566)(R 0.583, F 0.550)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.227] [G acc: 0.141]\n",
      "1867 [D loss: (0.573)(R 0.507, F 0.639)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.153] [G acc: 0.141]\n",
      "1868 [D loss: (0.559)(R 0.533, F 0.585)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.253] [G acc: 0.109]\n",
      "1869 [D loss: (0.574)(R 0.579, F 0.568)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.294] [G acc: 0.094]\n",
      "1870 [D loss: (0.614)(R 0.641, F 0.587)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.244] [G acc: 0.094]\n",
      "1871 [D loss: (0.526)(R 0.497, F 0.555)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.373] [G acc: 0.125]\n",
      "1872 [D loss: (0.570)(R 0.542, F 0.598)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.358] [G acc: 0.141]\n",
      "1873 [D loss: (0.621)(R 0.717, F 0.526)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.136] [G acc: 0.172]\n",
      "1874 [D loss: (0.730)(R 0.649, F 0.812)] [D acc: (0.586)(0.594, 0.578)] [G loss: 1.117] [G acc: 0.125]\n",
      "1875 [D loss: (0.644)(R 0.733, F 0.555)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.170] [G acc: 0.141]\n",
      "1876 [D loss: (0.512)(R 0.500, F 0.524)] [D acc: (0.797)(0.781, 0.812)] [G loss: 1.123] [G acc: 0.141]\n",
      "1877 [D loss: (0.539)(R 0.498, F 0.579)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.343] [G acc: 0.078]\n",
      "1878 [D loss: (0.568)(R 0.599, F 0.537)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.153] [G acc: 0.172]\n",
      "1879 [D loss: (0.535)(R 0.540, F 0.530)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.278] [G acc: 0.062]\n",
      "1880 [D loss: (0.519)(R 0.519, F 0.519)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.171] [G acc: 0.172]\n",
      "1881 [D loss: (0.599)(R 0.458, F 0.739)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.399] [G acc: 0.109]\n",
      "1882 [D loss: (0.564)(R 0.608, F 0.520)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.265] [G acc: 0.109]\n",
      "1883 [D loss: (0.582)(R 0.592, F 0.572)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.292] [G acc: 0.109]\n",
      "1884 [D loss: (0.637)(R 0.692, F 0.581)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.175] [G acc: 0.172]\n",
      "1885 [D loss: (0.572)(R 0.570, F 0.574)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.210] [G acc: 0.156]\n",
      "1886 [D loss: (0.631)(R 0.655, F 0.608)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.167] [G acc: 0.109]\n",
      "1887 [D loss: (0.555)(R 0.433, F 0.677)] [D acc: (0.734)(0.781, 0.688)] [G loss: 1.314] [G acc: 0.109]\n",
      "1888 [D loss: (0.565)(R 0.643, F 0.486)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.335] [G acc: 0.125]\n",
      "1889 [D loss: (0.595)(R 0.510, F 0.680)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.215] [G acc: 0.078]\n",
      "1890 [D loss: (0.594)(R 0.645, F 0.542)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.333] [G acc: 0.094]\n",
      "1891 [D loss: (0.525)(R 0.553, F 0.497)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.219] [G acc: 0.172]\n",
      "1892 [D loss: (0.639)(R 0.555, F 0.723)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.275] [G acc: 0.062]\n",
      "1893 [D loss: (0.575)(R 0.649, F 0.501)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.383] [G acc: 0.078]\n",
      "1894 [D loss: (0.516)(R 0.467, F 0.565)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.133] [G acc: 0.141]\n",
      "1895 [D loss: (0.587)(R 0.586, F 0.589)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.242] [G acc: 0.094]\n",
      "1896 [D loss: (0.588)(R 0.545, F 0.631)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.122] [G acc: 0.172]\n",
      "1897 [D loss: (0.522)(R 0.453, F 0.592)] [D acc: (0.789)(0.797, 0.781)] [G loss: 1.269] [G acc: 0.109]\n",
      "1898 [D loss: (0.605)(R 0.688, F 0.522)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.143] [G acc: 0.094]\n",
      "1899 [D loss: (0.588)(R 0.648, F 0.529)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.244] [G acc: 0.188]\n",
      "1900 [D loss: (0.605)(R 0.689, F 0.521)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.239] [G acc: 0.172]\n",
      "1901 [D loss: (0.560)(R 0.498, F 0.623)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.276] [G acc: 0.031]\n",
      "1902 [D loss: (0.552)(R 0.554, F 0.550)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.193] [G acc: 0.094]\n",
      "1903 [D loss: (0.619)(R 0.561, F 0.676)] [D acc: (0.617)(0.609, 0.625)] [G loss: 1.342] [G acc: 0.109]\n",
      "1904 [D loss: (0.579)(R 0.702, F 0.456)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.185] [G acc: 0.141]\n",
      "1905 [D loss: (0.568)(R 0.643, F 0.492)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.231] [G acc: 0.125]\n",
      "1906 [D loss: (0.552)(R 0.565, F 0.538)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.121] [G acc: 0.125]\n",
      "1907 [D loss: (0.687)(R 0.626, F 0.748)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.298] [G acc: 0.125]\n",
      "1908 [D loss: (0.587)(R 0.588, F 0.585)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.239] [G acc: 0.109]\n",
      "1909 [D loss: (0.550)(R 0.507, F 0.592)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.267] [G acc: 0.094]\n",
      "1910 [D loss: (0.621)(R 0.664, F 0.579)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.071] [G acc: 0.109]\n",
      "1911 [D loss: (0.562)(R 0.598, F 0.526)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.202] [G acc: 0.062]\n",
      "1912 [D loss: (0.588)(R 0.523, F 0.653)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.145] [G acc: 0.125]\n",
      "1913 [D loss: (0.496)(R 0.465, F 0.527)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.142] [G acc: 0.188]\n",
      "1914 [D loss: (0.564)(R 0.580, F 0.547)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.169] [G acc: 0.109]\n",
      "1915 [D loss: (0.538)(R 0.511, F 0.565)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.223] [G acc: 0.172]\n",
      "1916 [D loss: (0.529)(R 0.480, F 0.578)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.182] [G acc: 0.219]\n",
      "1917 [D loss: (0.586)(R 0.619, F 0.554)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.121] [G acc: 0.094]\n",
      "1918 [D loss: (0.613)(R 0.502, F 0.724)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.208] [G acc: 0.172]\n",
      "1919 [D loss: (0.593)(R 0.604, F 0.582)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.204] [G acc: 0.062]\n",
      "1920 [D loss: (0.595)(R 0.595, F 0.594)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.160] [G acc: 0.141]\n",
      "1921 [D loss: (0.537)(R 0.553, F 0.522)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.183] [G acc: 0.141]\n",
      "1922 [D loss: (0.564)(R 0.622, F 0.506)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.274] [G acc: 0.109]\n",
      "1923 [D loss: (0.570)(R 0.513, F 0.628)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.270] [G acc: 0.156]\n",
      "1924 [D loss: (0.564)(R 0.505, F 0.624)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.252] [G acc: 0.109]\n",
      "1925 [D loss: (0.577)(R 0.588, F 0.566)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.210] [G acc: 0.094]\n",
      "1926 [D loss: (0.606)(R 0.569, F 0.643)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.197] [G acc: 0.094]\n",
      "1927 [D loss: (0.530)(R 0.553, F 0.506)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.177] [G acc: 0.141]\n",
      "1928 [D loss: (0.591)(R 0.605, F 0.577)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.179] [G acc: 0.156]\n",
      "1929 [D loss: (0.488)(R 0.501, F 0.476)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.347] [G acc: 0.094]\n",
      "1930 [D loss: (0.614)(R 0.580, F 0.647)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.353] [G acc: 0.125]\n",
      "1931 [D loss: (0.598)(R 0.615, F 0.581)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.269] [G acc: 0.062]\n",
      "1932 [D loss: (0.511)(R 0.528, F 0.494)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.225] [G acc: 0.125]\n",
      "1933 [D loss: (0.596)(R 0.670, F 0.523)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.187] [G acc: 0.188]\n",
      "1934 [D loss: (0.559)(R 0.403, F 0.714)] [D acc: (0.711)(0.750, 0.672)] [G loss: 1.168] [G acc: 0.203]\n",
      "1935 [D loss: (0.620)(R 0.634, F 0.606)] [D acc: (0.609)(0.547, 0.672)] [G loss: 1.248] [G acc: 0.094]\n",
      "1936 [D loss: (0.584)(R 0.645, F 0.524)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.324] [G acc: 0.047]\n",
      "1937 [D loss: (0.597)(R 0.667, F 0.527)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.258] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1938 [D loss: (0.574)(R 0.598, F 0.550)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.182] [G acc: 0.109]\n",
      "1939 [D loss: (0.589)(R 0.499, F 0.680)] [D acc: (0.672)(0.719, 0.625)] [G loss: 1.216] [G acc: 0.078]\n",
      "1940 [D loss: (0.544)(R 0.559, F 0.529)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.236] [G acc: 0.094]\n",
      "1941 [D loss: (0.564)(R 0.599, F 0.529)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.428] [G acc: 0.125]\n",
      "1942 [D loss: (0.474)(R 0.478, F 0.471)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.309] [G acc: 0.141]\n",
      "1943 [D loss: (0.518)(R 0.457, F 0.580)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.333] [G acc: 0.125]\n",
      "1944 [D loss: (0.554)(R 0.524, F 0.585)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.435] [G acc: 0.094]\n",
      "1945 [D loss: (0.579)(R 0.679, F 0.479)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.259] [G acc: 0.141]\n",
      "1946 [D loss: (0.500)(R 0.528, F 0.471)] [D acc: (0.812)(0.750, 0.875)] [G loss: 1.322] [G acc: 0.078]\n",
      "1947 [D loss: (0.532)(R 0.508, F 0.557)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.271] [G acc: 0.125]\n",
      "1948 [D loss: (0.514)(R 0.535, F 0.493)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.306] [G acc: 0.219]\n",
      "1949 [D loss: (0.542)(R 0.484, F 0.601)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.391] [G acc: 0.109]\n",
      "1950 [D loss: (0.555)(R 0.537, F 0.573)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.453] [G acc: 0.047]\n",
      "1951 [D loss: (0.572)(R 0.550, F 0.594)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.327] [G acc: 0.109]\n",
      "1952 [D loss: (0.658)(R 0.635, F 0.680)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.308] [G acc: 0.109]\n",
      "1953 [D loss: (0.546)(R 0.578, F 0.514)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.284] [G acc: 0.188]\n",
      "1954 [D loss: (0.477)(R 0.446, F 0.508)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.335] [G acc: 0.125]\n",
      "1955 [D loss: (0.577)(R 0.610, F 0.545)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.357] [G acc: 0.125]\n",
      "1956 [D loss: (0.519)(R 0.528, F 0.510)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.312] [G acc: 0.125]\n",
      "1957 [D loss: (0.546)(R 0.548, F 0.545)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.265] [G acc: 0.172]\n",
      "1958 [D loss: (0.572)(R 0.563, F 0.581)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.271] [G acc: 0.219]\n",
      "1959 [D loss: (0.552)(R 0.556, F 0.549)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.322] [G acc: 0.156]\n",
      "1960 [D loss: (0.561)(R 0.542, F 0.581)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.305] [G acc: 0.125]\n",
      "1961 [D loss: (0.540)(R 0.527, F 0.553)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.308] [G acc: 0.156]\n",
      "1962 [D loss: (0.630)(R 0.556, F 0.704)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.320] [G acc: 0.125]\n",
      "1963 [D loss: (0.682)(R 0.751, F 0.613)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.189] [G acc: 0.094]\n",
      "1964 [D loss: (0.535)(R 0.537, F 0.533)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.360] [G acc: 0.031]\n",
      "1965 [D loss: (0.505)(R 0.503, F 0.508)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.438] [G acc: 0.094]\n",
      "1966 [D loss: (0.497)(R 0.432, F 0.562)] [D acc: (0.797)(0.781, 0.812)] [G loss: 1.251] [G acc: 0.125]\n",
      "1967 [D loss: (0.518)(R 0.597, F 0.438)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.443] [G acc: 0.094]\n",
      "1968 [D loss: (0.536)(R 0.528, F 0.543)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.307] [G acc: 0.188]\n",
      "1969 [D loss: (0.581)(R 0.606, F 0.555)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.421] [G acc: 0.156]\n",
      "1970 [D loss: (0.538)(R 0.607, F 0.469)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.387] [G acc: 0.094]\n",
      "1971 [D loss: (0.540)(R 0.594, F 0.485)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.269] [G acc: 0.094]\n",
      "1972 [D loss: (0.605)(R 0.580, F 0.629)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.221] [G acc: 0.125]\n",
      "1973 [D loss: (0.597)(R 0.603, F 0.592)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.235] [G acc: 0.141]\n",
      "1974 [D loss: (0.523)(R 0.539, F 0.508)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.292] [G acc: 0.094]\n",
      "1975 [D loss: (0.578)(R 0.606, F 0.550)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.285] [G acc: 0.078]\n",
      "1976 [D loss: (0.572)(R 0.635, F 0.509)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.320] [G acc: 0.109]\n",
      "1977 [D loss: (0.586)(R 0.668, F 0.503)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.369] [G acc: 0.047]\n",
      "1978 [D loss: (0.668)(R 0.704, F 0.632)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.104] [G acc: 0.094]\n",
      "1979 [D loss: (0.595)(R 0.555, F 0.634)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.170] [G acc: 0.188]\n",
      "1980 [D loss: (0.609)(R 0.572, F 0.645)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.216] [G acc: 0.062]\n",
      "1981 [D loss: (0.587)(R 0.586, F 0.588)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.211] [G acc: 0.094]\n",
      "1982 [D loss: (0.537)(R 0.483, F 0.592)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.292] [G acc: 0.078]\n",
      "1983 [D loss: (0.526)(R 0.543, F 0.510)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.403] [G acc: 0.156]\n",
      "1984 [D loss: (0.547)(R 0.570, F 0.525)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.533] [G acc: 0.094]\n",
      "1985 [D loss: (0.571)(R 0.503, F 0.639)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.350] [G acc: 0.094]\n",
      "1986 [D loss: (0.600)(R 0.655, F 0.545)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.220] [G acc: 0.141]\n",
      "1987 [D loss: (0.622)(R 0.611, F 0.633)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.209] [G acc: 0.109]\n",
      "1988 [D loss: (0.553)(R 0.559, F 0.547)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.096] [G acc: 0.125]\n",
      "1989 [D loss: (0.527)(R 0.519, F 0.535)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.164] [G acc: 0.172]\n",
      "1990 [D loss: (0.523)(R 0.533, F 0.514)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.120] [G acc: 0.125]\n",
      "1991 [D loss: (0.571)(R 0.557, F 0.585)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.285] [G acc: 0.125]\n",
      "1992 [D loss: (0.548)(R 0.504, F 0.591)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.238] [G acc: 0.062]\n",
      "1993 [D loss: (0.490)(R 0.519, F 0.461)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.167] [G acc: 0.156]\n",
      "1994 [D loss: (0.633)(R 0.568, F 0.698)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.258] [G acc: 0.141]\n",
      "1995 [D loss: (0.555)(R 0.528, F 0.582)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.136] [G acc: 0.172]\n",
      "1996 [D loss: (0.515)(R 0.570, F 0.460)] [D acc: (0.797)(0.688, 0.906)] [G loss: 1.159] [G acc: 0.234]\n",
      "1997 [D loss: (0.634)(R 0.474, F 0.793)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.259] [G acc: 0.141]\n",
      "1998 [D loss: (0.608)(R 0.721, F 0.495)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.217] [G acc: 0.219]\n",
      "1999 [D loss: (0.550)(R 0.566, F 0.533)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.309] [G acc: 0.125]\n",
      "2000 [D loss: (0.600)(R 0.535, F 0.665)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.160] [G acc: 0.172]\n",
      "2001 [D loss: (0.617)(R 0.642, F 0.593)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.050] [G acc: 0.266]\n",
      "2002 [D loss: (0.509)(R 0.512, F 0.506)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.287] [G acc: 0.156]\n",
      "2003 [D loss: (0.701)(R 0.578, F 0.824)] [D acc: (0.570)(0.625, 0.516)] [G loss: 1.322] [G acc: 0.047]\n",
      "2004 [D loss: (0.685)(R 0.760, F 0.609)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.246] [G acc: 0.078]\n",
      "2005 [D loss: (0.573)(R 0.616, F 0.529)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.246] [G acc: 0.156]\n",
      "2006 [D loss: (0.668)(R 0.643, F 0.693)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.265] [G acc: 0.125]\n",
      "2007 [D loss: (0.578)(R 0.517, F 0.639)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.291] [G acc: 0.156]\n",
      "2008 [D loss: (0.553)(R 0.580, F 0.526)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.166] [G acc: 0.062]\n",
      "2009 [D loss: (0.563)(R 0.567, F 0.558)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.162] [G acc: 0.125]\n",
      "2010 [D loss: (0.570)(R 0.556, F 0.583)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.317] [G acc: 0.078]\n",
      "2011 [D loss: (0.635)(R 0.591, F 0.679)] [D acc: (0.625)(0.641, 0.609)] [G loss: 1.193] [G acc: 0.141]\n",
      "2012 [D loss: (0.557)(R 0.619, F 0.495)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.136] [G acc: 0.141]\n",
      "2013 [D loss: (0.422)(R 0.395, F 0.450)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.388] [G acc: 0.141]\n",
      "2014 [D loss: (0.642)(R 0.601, F 0.683)] [D acc: (0.602)(0.578, 0.625)] [G loss: 1.348] [G acc: 0.047]\n",
      "2015 [D loss: (0.525)(R 0.552, F 0.497)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.329] [G acc: 0.078]\n",
      "2016 [D loss: (0.560)(R 0.579, F 0.542)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.310] [G acc: 0.031]\n",
      "2017 [D loss: (0.600)(R 0.636, F 0.565)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.347] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 [D loss: (0.585)(R 0.686, F 0.485)] [D acc: (0.688)(0.516, 0.859)] [G loss: 1.278] [G acc: 0.125]\n",
      "2019 [D loss: (0.574)(R 0.544, F 0.604)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.315] [G acc: 0.094]\n",
      "2020 [D loss: (0.574)(R 0.584, F 0.564)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.360] [G acc: 0.141]\n",
      "2021 [D loss: (0.527)(R 0.559, F 0.496)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.390] [G acc: 0.094]\n",
      "2022 [D loss: (0.556)(R 0.553, F 0.559)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.336] [G acc: 0.047]\n",
      "2023 [D loss: (0.478)(R 0.502, F 0.454)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.469] [G acc: 0.109]\n",
      "2024 [D loss: (0.580)(R 0.556, F 0.604)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.240] [G acc: 0.109]\n",
      "2025 [D loss: (0.485)(R 0.435, F 0.535)] [D acc: (0.773)(0.781, 0.766)] [G loss: 1.345] [G acc: 0.094]\n",
      "2026 [D loss: (0.569)(R 0.584, F 0.555)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.316] [G acc: 0.125]\n",
      "2027 [D loss: (0.524)(R 0.553, F 0.494)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.443] [G acc: 0.109]\n",
      "2028 [D loss: (0.639)(R 0.549, F 0.730)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.130] [G acc: 0.203]\n",
      "2029 [D loss: (0.536)(R 0.503, F 0.569)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.335] [G acc: 0.188]\n",
      "2030 [D loss: (0.580)(R 0.582, F 0.578)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.271] [G acc: 0.125]\n",
      "2031 [D loss: (0.607)(R 0.619, F 0.595)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.292] [G acc: 0.078]\n",
      "2032 [D loss: (0.528)(R 0.550, F 0.506)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.278] [G acc: 0.094]\n",
      "2033 [D loss: (0.491)(R 0.485, F 0.497)] [D acc: (0.805)(0.750, 0.859)] [G loss: 1.167] [G acc: 0.172]\n",
      "2034 [D loss: (0.551)(R 0.542, F 0.561)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.299] [G acc: 0.094]\n",
      "2035 [D loss: (0.532)(R 0.547, F 0.517)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.291] [G acc: 0.078]\n",
      "2036 [D loss: (0.541)(R 0.526, F 0.556)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.341] [G acc: 0.078]\n",
      "2037 [D loss: (0.534)(R 0.545, F 0.523)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.285] [G acc: 0.078]\n",
      "2038 [D loss: (0.591)(R 0.593, F 0.590)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.313] [G acc: 0.047]\n",
      "2039 [D loss: (0.616)(R 0.578, F 0.654)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.205] [G acc: 0.109]\n",
      "2040 [D loss: (0.599)(R 0.628, F 0.571)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.244] [G acc: 0.172]\n",
      "2041 [D loss: (0.522)(R 0.495, F 0.549)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.219] [G acc: 0.156]\n",
      "2042 [D loss: (0.566)(R 0.615, F 0.517)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.305] [G acc: 0.078]\n",
      "2043 [D loss: (0.666)(R 0.729, F 0.603)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.372] [G acc: 0.094]\n",
      "2044 [D loss: (0.747)(R 0.850, F 0.645)] [D acc: (0.539)(0.344, 0.734)] [G loss: 1.250] [G acc: 0.094]\n",
      "2045 [D loss: (0.690)(R 0.851, F 0.530)] [D acc: (0.586)(0.359, 0.812)] [G loss: 1.077] [G acc: 0.125]\n",
      "2046 [D loss: (0.540)(R 0.545, F 0.535)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.163] [G acc: 0.125]\n",
      "2047 [D loss: (0.573)(R 0.519, F 0.627)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.151] [G acc: 0.109]\n",
      "2048 [D loss: (0.523)(R 0.446, F 0.600)] [D acc: (0.742)(0.766, 0.719)] [G loss: 1.326] [G acc: 0.109]\n",
      "2049 [D loss: (0.617)(R 0.632, F 0.602)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.135] [G acc: 0.172]\n",
      "2050 [D loss: (0.589)(R 0.595, F 0.583)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.152] [G acc: 0.109]\n",
      "2051 [D loss: (0.529)(R 0.573, F 0.485)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.341] [G acc: 0.125]\n",
      "2052 [D loss: (0.684)(R 0.644, F 0.723)] [D acc: (0.602)(0.594, 0.609)] [G loss: 1.091] [G acc: 0.156]\n",
      "2053 [D loss: (0.596)(R 0.574, F 0.618)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.052] [G acc: 0.156]\n",
      "2054 [D loss: (0.574)(R 0.588, F 0.560)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.135] [G acc: 0.125]\n",
      "2055 [D loss: (0.601)(R 0.607, F 0.594)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.045] [G acc: 0.203]\n",
      "2056 [D loss: (0.636)(R 0.581, F 0.692)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.305] [G acc: 0.062]\n",
      "2057 [D loss: (0.643)(R 0.692, F 0.595)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.267] [G acc: 0.094]\n",
      "2058 [D loss: (0.542)(R 0.521, F 0.562)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.281] [G acc: 0.016]\n",
      "2059 [D loss: (0.548)(R 0.563, F 0.532)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.267] [G acc: 0.125]\n",
      "2060 [D loss: (0.578)(R 0.589, F 0.568)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.184] [G acc: 0.172]\n",
      "2061 [D loss: (0.628)(R 0.638, F 0.618)] [D acc: (0.594)(0.516, 0.672)] [G loss: 1.141] [G acc: 0.172]\n",
      "2062 [D loss: (0.539)(R 0.496, F 0.581)] [D acc: (0.727)(0.766, 0.688)] [G loss: 1.156] [G acc: 0.094]\n",
      "2063 [D loss: (0.592)(R 0.559, F 0.626)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.218] [G acc: 0.047]\n",
      "2064 [D loss: (0.540)(R 0.536, F 0.544)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.180] [G acc: 0.141]\n",
      "2065 [D loss: (0.652)(R 0.572, F 0.732)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.164] [G acc: 0.125]\n",
      "2066 [D loss: (0.523)(R 0.550, F 0.496)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.246] [G acc: 0.031]\n",
      "2067 [D loss: (0.519)(R 0.440, F 0.597)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.237] [G acc: 0.141]\n",
      "2068 [D loss: (0.555)(R 0.562, F 0.549)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.337] [G acc: 0.141]\n",
      "2069 [D loss: (0.561)(R 0.572, F 0.550)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.164] [G acc: 0.094]\n",
      "2070 [D loss: (0.609)(R 0.505, F 0.712)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.341] [G acc: 0.109]\n",
      "2071 [D loss: (0.626)(R 0.638, F 0.615)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.253] [G acc: 0.078]\n",
      "2072 [D loss: (0.549)(R 0.527, F 0.570)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.238] [G acc: 0.094]\n",
      "2073 [D loss: (0.552)(R 0.553, F 0.552)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.260] [G acc: 0.109]\n",
      "2074 [D loss: (0.542)(R 0.586, F 0.498)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.217] [G acc: 0.141]\n",
      "2075 [D loss: (0.592)(R 0.523, F 0.661)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.314] [G acc: 0.062]\n",
      "2076 [D loss: (0.584)(R 0.652, F 0.515)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.314] [G acc: 0.094]\n",
      "2077 [D loss: (0.582)(R 0.555, F 0.610)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.149] [G acc: 0.188]\n",
      "2078 [D loss: (0.540)(R 0.577, F 0.502)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.158] [G acc: 0.156]\n",
      "2079 [D loss: (0.603)(R 0.633, F 0.572)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.202] [G acc: 0.062]\n",
      "2080 [D loss: (0.604)(R 0.594, F 0.613)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.122] [G acc: 0.141]\n",
      "2081 [D loss: (0.629)(R 0.558, F 0.700)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.281] [G acc: 0.141]\n",
      "2082 [D loss: (0.584)(R 0.634, F 0.533)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.210] [G acc: 0.109]\n",
      "2083 [D loss: (0.582)(R 0.632, F 0.531)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.240] [G acc: 0.125]\n",
      "2084 [D loss: (0.607)(R 0.627, F 0.587)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.147] [G acc: 0.141]\n",
      "2085 [D loss: (0.643)(R 0.653, F 0.633)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.147] [G acc: 0.125]\n",
      "2086 [D loss: (0.582)(R 0.616, F 0.548)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.182] [G acc: 0.062]\n",
      "2087 [D loss: (0.606)(R 0.644, F 0.568)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.111] [G acc: 0.125]\n",
      "2088 [D loss: (0.541)(R 0.496, F 0.587)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.246] [G acc: 0.062]\n",
      "2089 [D loss: (0.594)(R 0.637, F 0.551)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.137] [G acc: 0.125]\n",
      "2090 [D loss: (0.557)(R 0.578, F 0.536)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.182] [G acc: 0.172]\n",
      "2091 [D loss: (0.647)(R 0.683, F 0.611)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.168] [G acc: 0.125]\n",
      "2092 [D loss: (0.546)(R 0.593, F 0.499)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.264] [G acc: 0.094]\n",
      "2093 [D loss: (0.498)(R 0.459, F 0.536)] [D acc: (0.773)(0.781, 0.766)] [G loss: 1.482] [G acc: 0.062]\n",
      "2094 [D loss: (0.636)(R 0.736, F 0.536)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.193] [G acc: 0.172]\n",
      "2095 [D loss: (0.549)(R 0.593, F 0.505)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.231] [G acc: 0.156]\n",
      "2096 [D loss: (0.544)(R 0.484, F 0.604)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.234] [G acc: 0.094]\n",
      "2097 [D loss: (0.599)(R 0.517, F 0.680)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.279] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2098 [D loss: (0.504)(R 0.506, F 0.502)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.244] [G acc: 0.141]\n",
      "2099 [D loss: (0.603)(R 0.643, F 0.563)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.138] [G acc: 0.219]\n",
      "2100 [D loss: (0.540)(R 0.614, F 0.466)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.233] [G acc: 0.141]\n",
      "2101 [D loss: (0.534)(R 0.553, F 0.515)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.189] [G acc: 0.203]\n",
      "2102 [D loss: (0.579)(R 0.523, F 0.636)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.261] [G acc: 0.094]\n",
      "2103 [D loss: (0.623)(R 0.549, F 0.697)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.277] [G acc: 0.109]\n",
      "2104 [D loss: (0.597)(R 0.623, F 0.571)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.213] [G acc: 0.047]\n",
      "2105 [D loss: (0.540)(R 0.592, F 0.488)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.033] [G acc: 0.203]\n",
      "2106 [D loss: (0.510)(R 0.478, F 0.542)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.139] [G acc: 0.125]\n",
      "2107 [D loss: (0.572)(R 0.555, F 0.589)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.285] [G acc: 0.094]\n",
      "2108 [D loss: (0.509)(R 0.542, F 0.476)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.308] [G acc: 0.125]\n",
      "2109 [D loss: (0.513)(R 0.491, F 0.535)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.414] [G acc: 0.109]\n",
      "2110 [D loss: (0.526)(R 0.627, F 0.424)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.224] [G acc: 0.094]\n",
      "2111 [D loss: (0.548)(R 0.501, F 0.596)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.223] [G acc: 0.125]\n",
      "2112 [D loss: (0.560)(R 0.563, F 0.558)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.123] [G acc: 0.188]\n",
      "2113 [D loss: (0.497)(R 0.484, F 0.509)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.291] [G acc: 0.078]\n",
      "2114 [D loss: (0.721)(R 0.703, F 0.738)] [D acc: (0.602)(0.547, 0.656)] [G loss: 1.303] [G acc: 0.094]\n",
      "2115 [D loss: (0.549)(R 0.624, F 0.475)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.144] [G acc: 0.109]\n",
      "2116 [D loss: (0.526)(R 0.476, F 0.575)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.272] [G acc: 0.141]\n",
      "2117 [D loss: (0.566)(R 0.642, F 0.490)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.243] [G acc: 0.109]\n",
      "2118 [D loss: (0.604)(R 0.576, F 0.633)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.180] [G acc: 0.141]\n",
      "2119 [D loss: (0.481)(R 0.514, F 0.447)] [D acc: (0.797)(0.719, 0.875)] [G loss: 1.305] [G acc: 0.188]\n",
      "2120 [D loss: (0.617)(R 0.521, F 0.714)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.373] [G acc: 0.109]\n",
      "2121 [D loss: (0.523)(R 0.576, F 0.471)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.281] [G acc: 0.141]\n",
      "2122 [D loss: (0.550)(R 0.528, F 0.571)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.331] [G acc: 0.094]\n",
      "2123 [D loss: (0.604)(R 0.648, F 0.560)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.270] [G acc: 0.125]\n",
      "2124 [D loss: (0.577)(R 0.562, F 0.591)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.307] [G acc: 0.109]\n",
      "2125 [D loss: (0.586)(R 0.652, F 0.520)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.174] [G acc: 0.141]\n",
      "2126 [D loss: (0.540)(R 0.545, F 0.535)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.364] [G acc: 0.109]\n",
      "2127 [D loss: (0.539)(R 0.528, F 0.550)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.211] [G acc: 0.141]\n",
      "2128 [D loss: (0.511)(R 0.457, F 0.564)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.334] [G acc: 0.109]\n",
      "2129 [D loss: (0.623)(R 0.575, F 0.671)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.292] [G acc: 0.094]\n",
      "2130 [D loss: (0.523)(R 0.583, F 0.463)] [D acc: (0.789)(0.672, 0.906)] [G loss: 1.237] [G acc: 0.141]\n",
      "2131 [D loss: (0.584)(R 0.562, F 0.606)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.399] [G acc: 0.047]\n",
      "2132 [D loss: (0.663)(R 0.650, F 0.676)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.341] [G acc: 0.094]\n",
      "2133 [D loss: (0.674)(R 0.779, F 0.568)] [D acc: (0.648)(0.438, 0.859)] [G loss: 1.189] [G acc: 0.094]\n",
      "2134 [D loss: (0.631)(R 0.735, F 0.528)] [D acc: (0.641)(0.469, 0.812)] [G loss: 1.136] [G acc: 0.125]\n",
      "2135 [D loss: (0.722)(R 0.645, F 0.798)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.134] [G acc: 0.094]\n",
      "2136 [D loss: (0.582)(R 0.645, F 0.518)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.141] [G acc: 0.125]\n",
      "2137 [D loss: (0.599)(R 0.639, F 0.559)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.099] [G acc: 0.094]\n",
      "2138 [D loss: (0.579)(R 0.613, F 0.545)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.093] [G acc: 0.172]\n",
      "2139 [D loss: (0.525)(R 0.579, F 0.470)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.220] [G acc: 0.188]\n",
      "2140 [D loss: (0.575)(R 0.503, F 0.647)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.169] [G acc: 0.156]\n",
      "2141 [D loss: (0.573)(R 0.585, F 0.561)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.285] [G acc: 0.031]\n",
      "2142 [D loss: (0.571)(R 0.667, F 0.474)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.100] [G acc: 0.172]\n",
      "2143 [D loss: (0.582)(R 0.596, F 0.569)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.158] [G acc: 0.234]\n",
      "2144 [D loss: (0.567)(R 0.606, F 0.529)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.125] [G acc: 0.188]\n",
      "2145 [D loss: (0.680)(R 0.566, F 0.795)] [D acc: (0.703)(0.734, 0.672)] [G loss: 1.129] [G acc: 0.156]\n",
      "2146 [D loss: (0.528)(R 0.495, F 0.560)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.249] [G acc: 0.156]\n",
      "2147 [D loss: (0.621)(R 0.609, F 0.634)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.214] [G acc: 0.078]\n",
      "2148 [D loss: (0.552)(R 0.558, F 0.545)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.253] [G acc: 0.125]\n",
      "2149 [D loss: (0.569)(R 0.586, F 0.552)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.185] [G acc: 0.156]\n",
      "2150 [D loss: (0.571)(R 0.555, F 0.586)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.194] [G acc: 0.109]\n",
      "2151 [D loss: (0.517)(R 0.501, F 0.534)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.313] [G acc: 0.109]\n",
      "2152 [D loss: (0.534)(R 0.577, F 0.491)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.322] [G acc: 0.109]\n",
      "2153 [D loss: (0.582)(R 0.652, F 0.513)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.213] [G acc: 0.125]\n",
      "2154 [D loss: (0.584)(R 0.613, F 0.556)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.147] [G acc: 0.141]\n",
      "2155 [D loss: (0.675)(R 0.599, F 0.751)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.158] [G acc: 0.188]\n",
      "2156 [D loss: (0.511)(R 0.525, F 0.498)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.235] [G acc: 0.109]\n",
      "2157 [D loss: (0.528)(R 0.580, F 0.477)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.247] [G acc: 0.141]\n",
      "2158 [D loss: (0.564)(R 0.519, F 0.609)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.278] [G acc: 0.172]\n",
      "2159 [D loss: (0.542)(R 0.541, F 0.543)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.197] [G acc: 0.172]\n",
      "2160 [D loss: (0.524)(R 0.500, F 0.548)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.240] [G acc: 0.125]\n",
      "2161 [D loss: (0.702)(R 0.715, F 0.689)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.264] [G acc: 0.125]\n",
      "2162 [D loss: (0.610)(R 0.610, F 0.610)] [D acc: (0.609)(0.547, 0.672)] [G loss: 1.376] [G acc: 0.094]\n",
      "2163 [D loss: (0.604)(R 0.649, F 0.560)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.289] [G acc: 0.141]\n",
      "2164 [D loss: (0.537)(R 0.544, F 0.530)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.427] [G acc: 0.125]\n",
      "2165 [D loss: (0.571)(R 0.615, F 0.526)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.345] [G acc: 0.094]\n",
      "2166 [D loss: (0.529)(R 0.530, F 0.528)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.384] [G acc: 0.141]\n",
      "2167 [D loss: (0.506)(R 0.502, F 0.510)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.301] [G acc: 0.125]\n",
      "2168 [D loss: (0.558)(R 0.492, F 0.624)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.283] [G acc: 0.062]\n",
      "2169 [D loss: (0.592)(R 0.652, F 0.532)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.384] [G acc: 0.047]\n",
      "2170 [D loss: (0.599)(R 0.514, F 0.684)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.348] [G acc: 0.125]\n",
      "2171 [D loss: (0.715)(R 0.737, F 0.693)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.357] [G acc: 0.109]\n",
      "2172 [D loss: (0.516)(R 0.547, F 0.485)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.401] [G acc: 0.078]\n",
      "2173 [D loss: (0.567)(R 0.458, F 0.677)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.136] [G acc: 0.172]\n",
      "2174 [D loss: (0.451)(R 0.444, F 0.459)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.458] [G acc: 0.109]\n",
      "2175 [D loss: (0.664)(R 0.691, F 0.638)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.240] [G acc: 0.141]\n",
      "2176 [D loss: (0.592)(R 0.603, F 0.581)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.271] [G acc: 0.172]\n",
      "2177 [D loss: (0.567)(R 0.615, F 0.520)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.075] [G acc: 0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178 [D loss: (0.553)(R 0.521, F 0.585)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.238] [G acc: 0.094]\n",
      "2179 [D loss: (0.520)(R 0.527, F 0.512)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.153] [G acc: 0.250]\n",
      "2180 [D loss: (0.611)(R 0.564, F 0.658)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.259] [G acc: 0.125]\n",
      "2181 [D loss: (0.597)(R 0.621, F 0.574)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.244] [G acc: 0.125]\n",
      "2182 [D loss: (0.593)(R 0.606, F 0.581)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.137] [G acc: 0.203]\n",
      "2183 [D loss: (0.619)(R 0.667, F 0.572)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.173] [G acc: 0.094]\n",
      "2184 [D loss: (0.602)(R 0.584, F 0.620)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.209] [G acc: 0.094]\n",
      "2185 [D loss: (0.548)(R 0.546, F 0.551)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.228] [G acc: 0.125]\n",
      "2186 [D loss: (0.512)(R 0.532, F 0.491)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.221] [G acc: 0.141]\n",
      "2187 [D loss: (0.701)(R 0.622, F 0.779)] [D acc: (0.594)(0.609, 0.578)] [G loss: 1.323] [G acc: 0.125]\n",
      "2188 [D loss: (0.560)(R 0.595, F 0.525)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.239] [G acc: 0.062]\n",
      "2189 [D loss: (0.563)(R 0.566, F 0.560)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.206] [G acc: 0.172]\n",
      "2190 [D loss: (0.568)(R 0.553, F 0.583)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.403] [G acc: 0.078]\n",
      "2191 [D loss: (0.499)(R 0.557, F 0.442)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.261] [G acc: 0.125]\n",
      "2192 [D loss: (0.574)(R 0.541, F 0.607)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.372] [G acc: 0.109]\n",
      "2193 [D loss: (0.570)(R 0.627, F 0.514)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.266] [G acc: 0.062]\n",
      "2194 [D loss: (0.626)(R 0.652, F 0.601)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.334] [G acc: 0.047]\n",
      "2195 [D loss: (0.532)(R 0.551, F 0.514)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.221] [G acc: 0.156]\n",
      "2196 [D loss: (0.614)(R 0.662, F 0.567)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.133] [G acc: 0.156]\n",
      "2197 [D loss: (0.569)(R 0.494, F 0.644)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.232] [G acc: 0.109]\n",
      "2198 [D loss: (0.536)(R 0.538, F 0.534)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.206] [G acc: 0.141]\n",
      "2199 [D loss: (0.561)(R 0.557, F 0.565)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.272] [G acc: 0.094]\n",
      "2200 [D loss: (0.625)(R 0.600, F 0.650)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.284] [G acc: 0.094]\n",
      "2201 [D loss: (0.547)(R 0.588, F 0.505)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.137] [G acc: 0.188]\n",
      "2202 [D loss: (0.492)(R 0.432, F 0.553)] [D acc: (0.758)(0.812, 0.703)] [G loss: 1.205] [G acc: 0.188]\n",
      "2203 [D loss: (0.572)(R 0.585, F 0.560)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.207] [G acc: 0.234]\n",
      "2204 [D loss: (0.513)(R 0.428, F 0.597)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.336] [G acc: 0.109]\n",
      "2205 [D loss: (0.551)(R 0.554, F 0.548)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.294] [G acc: 0.109]\n",
      "2206 [D loss: (0.544)(R 0.576, F 0.512)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.251] [G acc: 0.141]\n",
      "2207 [D loss: (0.579)(R 0.596, F 0.563)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.287] [G acc: 0.172]\n",
      "2208 [D loss: (0.601)(R 0.669, F 0.532)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.317] [G acc: 0.078]\n",
      "2209 [D loss: (0.564)(R 0.618, F 0.510)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.355] [G acc: 0.047]\n",
      "2210 [D loss: (0.600)(R 0.541, F 0.659)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.125] [G acc: 0.188]\n",
      "2211 [D loss: (0.591)(R 0.459, F 0.723)] [D acc: (0.711)(0.750, 0.672)] [G loss: 1.302] [G acc: 0.109]\n",
      "2212 [D loss: (0.578)(R 0.673, F 0.483)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.254] [G acc: 0.062]\n",
      "2213 [D loss: (0.546)(R 0.588, F 0.505)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.268] [G acc: 0.172]\n",
      "2214 [D loss: (0.578)(R 0.541, F 0.615)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.201] [G acc: 0.156]\n",
      "2215 [D loss: (0.582)(R 0.674, F 0.491)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.208] [G acc: 0.125]\n",
      "2216 [D loss: (0.555)(R 0.515, F 0.595)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.263] [G acc: 0.141]\n",
      "2217 [D loss: (0.612)(R 0.652, F 0.571)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.294] [G acc: 0.141]\n",
      "2218 [D loss: (0.532)(R 0.554, F 0.510)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.235] [G acc: 0.156]\n",
      "2219 [D loss: (0.560)(R 0.494, F 0.627)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.167] [G acc: 0.156]\n",
      "2220 [D loss: (0.525)(R 0.512, F 0.539)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.198] [G acc: 0.125]\n",
      "2221 [D loss: (0.540)(R 0.583, F 0.498)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.167] [G acc: 0.203]\n",
      "2222 [D loss: (0.591)(R 0.586, F 0.597)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.236] [G acc: 0.156]\n",
      "2223 [D loss: (0.574)(R 0.479, F 0.669)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.378] [G acc: 0.062]\n",
      "2224 [D loss: (0.684)(R 0.541, F 0.826)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.239] [G acc: 0.125]\n",
      "2225 [D loss: (0.576)(R 0.655, F 0.497)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.342] [G acc: 0.109]\n",
      "2226 [D loss: (0.595)(R 0.569, F 0.620)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.302] [G acc: 0.062]\n",
      "2227 [D loss: (0.561)(R 0.556, F 0.566)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.213] [G acc: 0.141]\n",
      "2228 [D loss: (0.573)(R 0.631, F 0.514)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.234] [G acc: 0.109]\n",
      "2229 [D loss: (0.668)(R 0.551, F 0.784)] [D acc: (0.617)(0.609, 0.625)] [G loss: 1.269] [G acc: 0.047]\n",
      "2230 [D loss: (0.518)(R 0.556, F 0.480)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.248] [G acc: 0.078]\n",
      "2231 [D loss: (0.598)(R 0.558, F 0.638)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.263] [G acc: 0.141]\n",
      "2232 [D loss: (0.526)(R 0.592, F 0.460)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.194] [G acc: 0.109]\n",
      "2233 [D loss: (0.639)(R 0.573, F 0.705)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.176] [G acc: 0.094]\n",
      "2234 [D loss: (0.598)(R 0.663, F 0.532)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.282] [G acc: 0.109]\n",
      "2235 [D loss: (0.522)(R 0.518, F 0.526)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.244] [G acc: 0.062]\n",
      "2236 [D loss: (0.616)(R 0.624, F 0.608)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.327] [G acc: 0.156]\n",
      "2237 [D loss: (0.536)(R 0.541, F 0.530)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.334] [G acc: 0.078]\n",
      "2238 [D loss: (0.524)(R 0.576, F 0.471)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.272] [G acc: 0.109]\n",
      "2239 [D loss: (0.563)(R 0.561, F 0.566)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.225] [G acc: 0.172]\n",
      "2240 [D loss: (0.624)(R 0.656, F 0.591)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.190] [G acc: 0.094]\n",
      "2241 [D loss: (0.620)(R 0.562, F 0.678)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.280] [G acc: 0.078]\n",
      "2242 [D loss: (0.536)(R 0.552, F 0.521)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.219] [G acc: 0.141]\n",
      "2243 [D loss: (0.559)(R 0.536, F 0.583)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.162] [G acc: 0.094]\n",
      "2244 [D loss: (0.595)(R 0.559, F 0.631)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.367] [G acc: 0.062]\n",
      "2245 [D loss: (0.555)(R 0.606, F 0.503)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.240] [G acc: 0.109]\n",
      "2246 [D loss: (0.512)(R 0.520, F 0.504)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.212] [G acc: 0.188]\n",
      "2247 [D loss: (0.534)(R 0.534, F 0.534)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.284] [G acc: 0.047]\n",
      "2248 [D loss: (0.540)(R 0.627, F 0.452)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.123] [G acc: 0.141]\n",
      "2249 [D loss: (0.554)(R 0.526, F 0.581)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.268] [G acc: 0.078]\n",
      "2250 [D loss: (0.583)(R 0.554, F 0.611)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.317] [G acc: 0.125]\n",
      "2251 [D loss: (0.609)(R 0.701, F 0.516)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.252] [G acc: 0.078]\n",
      "2252 [D loss: (0.502)(R 0.465, F 0.538)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.173] [G acc: 0.141]\n",
      "2253 [D loss: (0.575)(R 0.621, F 0.529)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.094] [G acc: 0.156]\n",
      "2254 [D loss: (0.497)(R 0.462, F 0.531)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.362] [G acc: 0.078]\n",
      "2255 [D loss: (0.417)(R 0.360, F 0.474)] [D acc: (0.828)(0.828, 0.828)] [G loss: 1.519] [G acc: 0.141]\n",
      "2256 [D loss: (0.721)(R 0.627, F 0.815)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.238] [G acc: 0.016]\n",
      "2257 [D loss: (0.642)(R 0.792, F 0.492)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.230] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2258 [D loss: (0.520)(R 0.542, F 0.497)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.114] [G acc: 0.141]\n",
      "2259 [D loss: (0.648)(R 0.481, F 0.816)] [D acc: (0.703)(0.750, 0.656)] [G loss: 1.308] [G acc: 0.047]\n",
      "2260 [D loss: (0.562)(R 0.618, F 0.507)] [D acc: (0.664)(0.484, 0.844)] [G loss: 1.302] [G acc: 0.062]\n",
      "2261 [D loss: (0.561)(R 0.619, F 0.503)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.215] [G acc: 0.078]\n",
      "2262 [D loss: (0.552)(R 0.533, F 0.571)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.256] [G acc: 0.109]\n",
      "2263 [D loss: (0.497)(R 0.454, F 0.539)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.302] [G acc: 0.078]\n",
      "2264 [D loss: (0.512)(R 0.472, F 0.553)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.396] [G acc: 0.047]\n",
      "2265 [D loss: (0.593)(R 0.553, F 0.632)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.394] [G acc: 0.094]\n",
      "2266 [D loss: (0.629)(R 0.648, F 0.610)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.286] [G acc: 0.094]\n",
      "2267 [D loss: (0.502)(R 0.506, F 0.498)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.312] [G acc: 0.078]\n",
      "2268 [D loss: (0.528)(R 0.566, F 0.490)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.426] [G acc: 0.078]\n",
      "2269 [D loss: (0.693)(R 0.815, F 0.570)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.129] [G acc: 0.172]\n",
      "2270 [D loss: (0.575)(R 0.517, F 0.633)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.135] [G acc: 0.172]\n",
      "2271 [D loss: (0.572)(R 0.601, F 0.542)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.322] [G acc: 0.078]\n",
      "2272 [D loss: (0.537)(R 0.551, F 0.523)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.185] [G acc: 0.109]\n",
      "2273 [D loss: (0.584)(R 0.504, F 0.664)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.319] [G acc: 0.109]\n",
      "2274 [D loss: (0.582)(R 0.697, F 0.467)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.111] [G acc: 0.188]\n",
      "2275 [D loss: (0.641)(R 0.513, F 0.769)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.270] [G acc: 0.141]\n",
      "2276 [D loss: (0.589)(R 0.670, F 0.508)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.290] [G acc: 0.188]\n",
      "2277 [D loss: (0.583)(R 0.671, F 0.495)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.155] [G acc: 0.141]\n",
      "2278 [D loss: (0.544)(R 0.561, F 0.527)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.291] [G acc: 0.047]\n",
      "2279 [D loss: (0.499)(R 0.487, F 0.511)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.343] [G acc: 0.109]\n",
      "2280 [D loss: (0.607)(R 0.540, F 0.673)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.276] [G acc: 0.094]\n",
      "2281 [D loss: (0.593)(R 0.586, F 0.599)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.375] [G acc: 0.062]\n",
      "2282 [D loss: (0.539)(R 0.582, F 0.496)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.306] [G acc: 0.156]\n",
      "2283 [D loss: (0.672)(R 0.675, F 0.668)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.196] [G acc: 0.125]\n",
      "2284 [D loss: (0.499)(R 0.506, F 0.492)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.320] [G acc: 0.172]\n",
      "2285 [D loss: (0.639)(R 0.667, F 0.612)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.261] [G acc: 0.141]\n",
      "2286 [D loss: (0.484)(R 0.460, F 0.508)] [D acc: (0.797)(0.781, 0.812)] [G loss: 1.261] [G acc: 0.141]\n",
      "2287 [D loss: (0.547)(R 0.561, F 0.534)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.327] [G acc: 0.156]\n",
      "2288 [D loss: (0.566)(R 0.476, F 0.656)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.433] [G acc: 0.109]\n",
      "2289 [D loss: (0.527)(R 0.603, F 0.451)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.098] [G acc: 0.141]\n",
      "2290 [D loss: (0.581)(R 0.579, F 0.583)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.233] [G acc: 0.109]\n",
      "2291 [D loss: (0.522)(R 0.434, F 0.609)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.377] [G acc: 0.172]\n",
      "2292 [D loss: (0.541)(R 0.517, F 0.565)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.411] [G acc: 0.094]\n",
      "2293 [D loss: (0.563)(R 0.575, F 0.552)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.398] [G acc: 0.047]\n",
      "2294 [D loss: (0.641)(R 0.604, F 0.678)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.193] [G acc: 0.172]\n",
      "2295 [D loss: (0.542)(R 0.556, F 0.528)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.397] [G acc: 0.062]\n",
      "2296 [D loss: (0.509)(R 0.547, F 0.470)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.517] [G acc: 0.031]\n",
      "2297 [D loss: (0.556)(R 0.590, F 0.523)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.310] [G acc: 0.094]\n",
      "2298 [D loss: (0.625)(R 0.675, F 0.574)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.169] [G acc: 0.125]\n",
      "2299 [D loss: (0.589)(R 0.583, F 0.595)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.299] [G acc: 0.094]\n",
      "2300 [D loss: (0.517)(R 0.545, F 0.489)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.310] [G acc: 0.047]\n",
      "2301 [D loss: (0.521)(R 0.559, F 0.482)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.372] [G acc: 0.141]\n",
      "2302 [D loss: (0.605)(R 0.721, F 0.489)] [D acc: (0.641)(0.484, 0.797)] [G loss: 1.185] [G acc: 0.125]\n",
      "2303 [D loss: (0.474)(R 0.467, F 0.481)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.269] [G acc: 0.094]\n",
      "2304 [D loss: (0.605)(R 0.584, F 0.626)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.321] [G acc: 0.141]\n",
      "2305 [D loss: (0.675)(R 0.683, F 0.667)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.014] [G acc: 0.234]\n",
      "2306 [D loss: (0.564)(R 0.577, F 0.551)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.207] [G acc: 0.188]\n",
      "2307 [D loss: (0.558)(R 0.517, F 0.599)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.230] [G acc: 0.094]\n",
      "2308 [D loss: (0.587)(R 0.557, F 0.617)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.227] [G acc: 0.125]\n",
      "2309 [D loss: (0.609)(R 0.694, F 0.523)] [D acc: (0.672)(0.500, 0.844)] [G loss: 1.184] [G acc: 0.109]\n",
      "2310 [D loss: (0.577)(R 0.626, F 0.529)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.195] [G acc: 0.078]\n",
      "2311 [D loss: (0.651)(R 0.633, F 0.668)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.200] [G acc: 0.078]\n",
      "2312 [D loss: (0.587)(R 0.629, F 0.546)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.213] [G acc: 0.125]\n",
      "2313 [D loss: (0.618)(R 0.661, F 0.575)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.129] [G acc: 0.141]\n",
      "2314 [D loss: (0.504)(R 0.507, F 0.502)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.252] [G acc: 0.156]\n",
      "2315 [D loss: (0.548)(R 0.534, F 0.562)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.117] [G acc: 0.125]\n",
      "2316 [D loss: (0.513)(R 0.449, F 0.576)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.219] [G acc: 0.109]\n",
      "2317 [D loss: (0.572)(R 0.544, F 0.601)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.301] [G acc: 0.094]\n",
      "2318 [D loss: (0.554)(R 0.633, F 0.475)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.152] [G acc: 0.172]\n",
      "2319 [D loss: (0.513)(R 0.484, F 0.542)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.243] [G acc: 0.109]\n",
      "2320 [D loss: (0.632)(R 0.541, F 0.724)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.258] [G acc: 0.141]\n",
      "2321 [D loss: (0.646)(R 0.738, F 0.554)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.171] [G acc: 0.141]\n",
      "2322 [D loss: (0.523)(R 0.543, F 0.503)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.213] [G acc: 0.188]\n",
      "2323 [D loss: (0.536)(R 0.494, F 0.578)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.248] [G acc: 0.062]\n",
      "2324 [D loss: (0.583)(R 0.572, F 0.594)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.252] [G acc: 0.062]\n",
      "2325 [D loss: (0.605)(R 0.659, F 0.551)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.181] [G acc: 0.109]\n",
      "2326 [D loss: (0.603)(R 0.595, F 0.611)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.158] [G acc: 0.094]\n",
      "2327 [D loss: (0.576)(R 0.554, F 0.597)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.164] [G acc: 0.156]\n",
      "2328 [D loss: (0.566)(R 0.631, F 0.501)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.079] [G acc: 0.188]\n",
      "2329 [D loss: (0.576)(R 0.453, F 0.699)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.127] [G acc: 0.078]\n",
      "2330 [D loss: (0.595)(R 0.578, F 0.611)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.102] [G acc: 0.203]\n",
      "2331 [D loss: (0.548)(R 0.498, F 0.599)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.153] [G acc: 0.125]\n",
      "2332 [D loss: (0.611)(R 0.612, F 0.610)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.284] [G acc: 0.078]\n",
      "2333 [D loss: (0.559)(R 0.595, F 0.524)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.253] [G acc: 0.141]\n",
      "2334 [D loss: (0.532)(R 0.411, F 0.652)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.271] [G acc: 0.109]\n",
      "2335 [D loss: (0.583)(R 0.569, F 0.596)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.426] [G acc: 0.062]\n",
      "2336 [D loss: (0.516)(R 0.587, F 0.445)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.328] [G acc: 0.094]\n",
      "2337 [D loss: (0.602)(R 0.609, F 0.595)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.236] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2338 [D loss: (0.591)(R 0.645, F 0.536)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.124] [G acc: 0.234]\n",
      "2339 [D loss: (0.564)(R 0.566, F 0.561)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.273] [G acc: 0.156]\n",
      "2340 [D loss: (0.594)(R 0.587, F 0.601)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.408] [G acc: 0.141]\n",
      "2341 [D loss: (0.574)(R 0.640, F 0.507)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.324] [G acc: 0.078]\n",
      "2342 [D loss: (0.586)(R 0.592, F 0.579)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.247] [G acc: 0.141]\n",
      "2343 [D loss: (0.583)(R 0.608, F 0.557)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.203] [G acc: 0.125]\n",
      "2344 [D loss: (0.616)(R 0.623, F 0.609)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.099] [G acc: 0.156]\n",
      "2345 [D loss: (0.560)(R 0.481, F 0.638)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.085] [G acc: 0.172]\n",
      "2346 [D loss: (0.600)(R 0.688, F 0.512)] [D acc: (0.625)(0.484, 0.766)] [G loss: 1.117] [G acc: 0.156]\n",
      "2347 [D loss: (0.554)(R 0.569, F 0.540)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.277] [G acc: 0.234]\n",
      "2348 [D loss: (0.637)(R 0.594, F 0.680)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.187] [G acc: 0.094]\n",
      "2349 [D loss: (0.545)(R 0.629, F 0.462)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.130] [G acc: 0.172]\n",
      "2350 [D loss: (0.519)(R 0.543, F 0.494)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.195] [G acc: 0.109]\n",
      "2351 [D loss: (0.604)(R 0.594, F 0.614)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.166] [G acc: 0.141]\n",
      "2352 [D loss: (0.457)(R 0.464, F 0.450)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.257] [G acc: 0.094]\n",
      "2353 [D loss: (0.556)(R 0.516, F 0.596)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.238] [G acc: 0.125]\n",
      "2354 [D loss: (0.561)(R 0.552, F 0.570)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.151] [G acc: 0.219]\n",
      "2355 [D loss: (0.524)(R 0.570, F 0.478)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.269] [G acc: 0.094]\n",
      "2356 [D loss: (0.524)(R 0.526, F 0.523)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.253] [G acc: 0.125]\n",
      "2357 [D loss: (0.594)(R 0.636, F 0.553)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.210] [G acc: 0.203]\n",
      "2358 [D loss: (0.405)(R 0.374, F 0.436)] [D acc: (0.820)(0.734, 0.906)] [G loss: 1.279] [G acc: 0.141]\n",
      "2359 [D loss: (0.594)(R 0.581, F 0.607)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.333] [G acc: 0.094]\n",
      "2360 [D loss: (0.597)(R 0.634, F 0.559)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.389] [G acc: 0.047]\n",
      "2361 [D loss: (0.552)(R 0.539, F 0.565)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.100] [G acc: 0.156]\n",
      "2362 [D loss: (0.580)(R 0.570, F 0.590)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.217] [G acc: 0.047]\n",
      "2363 [D loss: (0.672)(R 0.732, F 0.613)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.189] [G acc: 0.172]\n",
      "2364 [D loss: (0.586)(R 0.610, F 0.563)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.179] [G acc: 0.141]\n",
      "2365 [D loss: (0.551)(R 0.451, F 0.651)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.268] [G acc: 0.062]\n",
      "2366 [D loss: (0.571)(R 0.652, F 0.490)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.263] [G acc: 0.078]\n",
      "2367 [D loss: (0.573)(R 0.613, F 0.534)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.245] [G acc: 0.109]\n",
      "2368 [D loss: (0.586)(R 0.593, F 0.578)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.252] [G acc: 0.141]\n",
      "2369 [D loss: (0.577)(R 0.618, F 0.537)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.230] [G acc: 0.094]\n",
      "2370 [D loss: (0.544)(R 0.521, F 0.566)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.225] [G acc: 0.094]\n",
      "2371 [D loss: (0.519)(R 0.491, F 0.547)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.231] [G acc: 0.094]\n",
      "2372 [D loss: (0.495)(R 0.456, F 0.533)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.450] [G acc: 0.062]\n",
      "2373 [D loss: (0.622)(R 0.554, F 0.691)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.355] [G acc: 0.047]\n",
      "2374 [D loss: (0.653)(R 0.764, F 0.541)] [D acc: (0.617)(0.453, 0.781)] [G loss: 1.299] [G acc: 0.203]\n",
      "2375 [D loss: (0.537)(R 0.627, F 0.446)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.239] [G acc: 0.109]\n",
      "2376 [D loss: (0.554)(R 0.604, F 0.504)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.162] [G acc: 0.109]\n",
      "2377 [D loss: (0.569)(R 0.496, F 0.642)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.205] [G acc: 0.141]\n",
      "2378 [D loss: (0.552)(R 0.571, F 0.534)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.302] [G acc: 0.031]\n",
      "2379 [D loss: (0.600)(R 0.593, F 0.607)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.200] [G acc: 0.141]\n",
      "2380 [D loss: (0.558)(R 0.552, F 0.564)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.323] [G acc: 0.078]\n",
      "2381 [D loss: (0.584)(R 0.668, F 0.501)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.179] [G acc: 0.141]\n",
      "2382 [D loss: (0.515)(R 0.492, F 0.538)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.233] [G acc: 0.141]\n",
      "2383 [D loss: (0.560)(R 0.524, F 0.596)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.303] [G acc: 0.125]\n",
      "2384 [D loss: (0.535)(R 0.552, F 0.518)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.265] [G acc: 0.078]\n",
      "2385 [D loss: (0.595)(R 0.620, F 0.570)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.242] [G acc: 0.094]\n",
      "2386 [D loss: (0.623)(R 0.723, F 0.523)] [D acc: (0.594)(0.469, 0.719)] [G loss: 1.186] [G acc: 0.094]\n",
      "2387 [D loss: (0.557)(R 0.610, F 0.504)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.245] [G acc: 0.141]\n",
      "2388 [D loss: (0.657)(R 0.609, F 0.706)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.283] [G acc: 0.062]\n",
      "2389 [D loss: (0.525)(R 0.549, F 0.500)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.332] [G acc: 0.062]\n",
      "2390 [D loss: (0.553)(R 0.584, F 0.521)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.208] [G acc: 0.156]\n",
      "2391 [D loss: (0.600)(R 0.530, F 0.670)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.197] [G acc: 0.141]\n",
      "2392 [D loss: (0.577)(R 0.619, F 0.535)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.184] [G acc: 0.109]\n",
      "2393 [D loss: (0.536)(R 0.476, F 0.596)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.158] [G acc: 0.172]\n",
      "2394 [D loss: (0.573)(R 0.578, F 0.568)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.270] [G acc: 0.188]\n",
      "2395 [D loss: (0.517)(R 0.491, F 0.543)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.381] [G acc: 0.047]\n",
      "2396 [D loss: (0.550)(R 0.557, F 0.543)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.397] [G acc: 0.109]\n",
      "2397 [D loss: (0.662)(R 0.531, F 0.793)] [D acc: (0.609)(0.625, 0.594)] [G loss: 1.268] [G acc: 0.109]\n",
      "2398 [D loss: (0.590)(R 0.660, F 0.520)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.182] [G acc: 0.125]\n",
      "2399 [D loss: (0.522)(R 0.550, F 0.495)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.311] [G acc: 0.125]\n",
      "2400 [D loss: (0.568)(R 0.528, F 0.609)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.415] [G acc: 0.031]\n",
      "2401 [D loss: (0.604)(R 0.608, F 0.601)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.238] [G acc: 0.094]\n",
      "2402 [D loss: (0.511)(R 0.554, F 0.468)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.263] [G acc: 0.156]\n",
      "2403 [D loss: (0.546)(R 0.505, F 0.586)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.246] [G acc: 0.172]\n",
      "2404 [D loss: (0.541)(R 0.586, F 0.497)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.273] [G acc: 0.125]\n",
      "2405 [D loss: (0.547)(R 0.579, F 0.514)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.176] [G acc: 0.109]\n",
      "2406 [D loss: (0.594)(R 0.581, F 0.607)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.175] [G acc: 0.047]\n",
      "2407 [D loss: (0.619)(R 0.563, F 0.676)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.334] [G acc: 0.078]\n",
      "2408 [D loss: (0.503)(R 0.537, F 0.468)] [D acc: (0.812)(0.719, 0.906)] [G loss: 1.296] [G acc: 0.094]\n",
      "2409 [D loss: (0.539)(R 0.541, F 0.537)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.418] [G acc: 0.047]\n",
      "2410 [D loss: (0.614)(R 0.695, F 0.533)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.190] [G acc: 0.094]\n",
      "2411 [D loss: (0.586)(R 0.657, F 0.515)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.148] [G acc: 0.125]\n",
      "2412 [D loss: (0.609)(R 0.517, F 0.701)] [D acc: (0.695)(0.734, 0.656)] [G loss: 1.178] [G acc: 0.125]\n",
      "2413 [D loss: (0.577)(R 0.624, F 0.530)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.359] [G acc: 0.031]\n",
      "2414 [D loss: (0.498)(R 0.472, F 0.524)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.301] [G acc: 0.094]\n",
      "2415 [D loss: (0.510)(R 0.457, F 0.563)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.290] [G acc: 0.219]\n",
      "2416 [D loss: (0.553)(R 0.583, F 0.523)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.206] [G acc: 0.125]\n",
      "2417 [D loss: (0.588)(R 0.541, F 0.635)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.347] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2418 [D loss: (0.560)(R 0.520, F 0.600)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.263] [G acc: 0.078]\n",
      "2419 [D loss: (0.577)(R 0.590, F 0.565)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.271] [G acc: 0.094]\n",
      "2420 [D loss: (0.567)(R 0.622, F 0.513)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.299] [G acc: 0.109]\n",
      "2421 [D loss: (0.587)(R 0.677, F 0.496)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.246] [G acc: 0.188]\n",
      "2422 [D loss: (0.566)(R 0.428, F 0.704)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.293] [G acc: 0.156]\n",
      "2423 [D loss: (0.457)(R 0.448, F 0.466)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.245] [G acc: 0.156]\n",
      "2424 [D loss: (0.546)(R 0.496, F 0.596)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.383] [G acc: 0.062]\n",
      "2425 [D loss: (0.588)(R 0.623, F 0.552)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.315] [G acc: 0.062]\n",
      "2426 [D loss: (0.532)(R 0.604, F 0.460)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.417] [G acc: 0.109]\n",
      "2427 [D loss: (0.599)(R 0.527, F 0.670)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.290] [G acc: 0.141]\n",
      "2428 [D loss: (0.486)(R 0.485, F 0.488)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.195] [G acc: 0.188]\n",
      "2429 [D loss: (0.584)(R 0.556, F 0.612)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.340] [G acc: 0.094]\n",
      "2430 [D loss: (0.654)(R 0.648, F 0.661)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.281] [G acc: 0.109]\n",
      "2431 [D loss: (0.637)(R 0.593, F 0.681)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.331] [G acc: 0.109]\n",
      "2432 [D loss: (0.532)(R 0.629, F 0.435)] [D acc: (0.742)(0.594, 0.891)] [G loss: 1.305] [G acc: 0.109]\n",
      "2433 [D loss: (0.650)(R 0.663, F 0.636)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.268] [G acc: 0.094]\n",
      "2434 [D loss: (0.602)(R 0.646, F 0.558)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.303] [G acc: 0.109]\n",
      "2435 [D loss: (0.522)(R 0.564, F 0.481)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.241] [G acc: 0.094]\n",
      "2436 [D loss: (0.556)(R 0.530, F 0.581)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.310] [G acc: 0.078]\n",
      "2437 [D loss: (0.570)(R 0.582, F 0.558)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.284] [G acc: 0.109]\n",
      "2438 [D loss: (0.517)(R 0.424, F 0.609)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.210] [G acc: 0.109]\n",
      "2439 [D loss: (0.557)(R 0.579, F 0.534)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.286] [G acc: 0.125]\n",
      "2440 [D loss: (0.529)(R 0.545, F 0.513)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.163] [G acc: 0.109]\n",
      "2441 [D loss: (0.474)(R 0.466, F 0.482)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.277] [G acc: 0.078]\n",
      "2442 [D loss: (0.557)(R 0.627, F 0.488)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.150] [G acc: 0.188]\n",
      "2443 [D loss: (0.532)(R 0.473, F 0.591)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.407] [G acc: 0.078]\n",
      "2444 [D loss: (0.602)(R 0.611, F 0.593)] [D acc: (0.617)(0.531, 0.703)] [G loss: 1.245] [G acc: 0.094]\n",
      "2445 [D loss: (0.598)(R 0.602, F 0.593)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.214] [G acc: 0.141]\n",
      "2446 [D loss: (0.527)(R 0.518, F 0.536)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.214] [G acc: 0.141]\n",
      "2447 [D loss: (0.608)(R 0.647, F 0.569)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.215] [G acc: 0.172]\n",
      "2448 [D loss: (0.569)(R 0.660, F 0.478)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.200] [G acc: 0.141]\n",
      "2449 [D loss: (0.577)(R 0.546, F 0.609)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.199] [G acc: 0.078]\n",
      "2450 [D loss: (0.623)(R 0.538, F 0.708)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.325] [G acc: 0.109]\n",
      "2451 [D loss: (0.566)(R 0.612, F 0.521)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.140] [G acc: 0.094]\n",
      "2452 [D loss: (0.544)(R 0.498, F 0.590)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.190] [G acc: 0.125]\n",
      "2453 [D loss: (0.557)(R 0.540, F 0.573)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.295] [G acc: 0.125]\n",
      "2454 [D loss: (0.634)(R 0.703, F 0.566)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.198] [G acc: 0.094]\n",
      "2455 [D loss: (0.526)(R 0.574, F 0.478)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.226] [G acc: 0.141]\n",
      "2456 [D loss: (0.600)(R 0.603, F 0.597)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.122] [G acc: 0.172]\n",
      "2457 [D loss: (0.533)(R 0.553, F 0.514)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.123] [G acc: 0.156]\n",
      "2458 [D loss: (0.518)(R 0.503, F 0.533)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.364] [G acc: 0.109]\n",
      "2459 [D loss: (0.604)(R 0.543, F 0.664)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.310] [G acc: 0.047]\n",
      "2460 [D loss: (0.539)(R 0.587, F 0.491)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.210] [G acc: 0.172]\n",
      "2461 [D loss: (0.572)(R 0.507, F 0.636)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.450] [G acc: 0.062]\n",
      "2462 [D loss: (0.573)(R 0.604, F 0.542)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.462] [G acc: 0.094]\n",
      "2463 [D loss: (0.522)(R 0.508, F 0.535)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.345] [G acc: 0.156]\n",
      "2464 [D loss: (0.667)(R 0.713, F 0.621)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.160] [G acc: 0.141]\n",
      "2465 [D loss: (0.578)(R 0.626, F 0.529)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.155] [G acc: 0.234]\n",
      "2466 [D loss: (0.599)(R 0.639, F 0.560)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.207] [G acc: 0.047]\n",
      "2467 [D loss: (0.612)(R 0.609, F 0.615)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.228] [G acc: 0.078]\n",
      "2468 [D loss: (0.544)(R 0.606, F 0.482)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.317] [G acc: 0.109]\n",
      "2469 [D loss: (0.604)(R 0.612, F 0.595)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.179] [G acc: 0.156]\n",
      "2470 [D loss: (0.567)(R 0.521, F 0.612)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.166] [G acc: 0.109]\n",
      "2471 [D loss: (0.556)(R 0.545, F 0.567)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.440] [G acc: 0.031]\n",
      "2472 [D loss: (0.519)(R 0.508, F 0.529)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.289] [G acc: 0.109]\n",
      "2473 [D loss: (0.525)(R 0.515, F 0.535)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.173] [G acc: 0.125]\n",
      "2474 [D loss: (0.525)(R 0.489, F 0.560)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.233] [G acc: 0.094]\n",
      "2475 [D loss: (0.515)(R 0.584, F 0.445)] [D acc: (0.750)(0.609, 0.891)] [G loss: 1.258] [G acc: 0.172]\n",
      "2476 [D loss: (0.558)(R 0.568, F 0.547)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.355] [G acc: 0.078]\n",
      "2477 [D loss: (0.607)(R 0.602, F 0.611)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.349] [G acc: 0.156]\n",
      "2478 [D loss: (0.579)(R 0.615, F 0.544)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.234] [G acc: 0.156]\n",
      "2479 [D loss: (0.567)(R 0.531, F 0.602)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.357] [G acc: 0.062]\n",
      "2480 [D loss: (0.611)(R 0.682, F 0.539)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.392] [G acc: 0.078]\n",
      "2481 [D loss: (0.575)(R 0.564, F 0.586)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.327] [G acc: 0.094]\n",
      "2482 [D loss: (0.528)(R 0.570, F 0.487)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.321] [G acc: 0.141]\n",
      "2483 [D loss: (0.581)(R 0.641, F 0.521)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.310] [G acc: 0.078]\n",
      "2484 [D loss: (0.508)(R 0.543, F 0.473)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.178] [G acc: 0.078]\n",
      "2485 [D loss: (0.540)(R 0.520, F 0.560)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.162] [G acc: 0.141]\n",
      "2486 [D loss: (0.522)(R 0.497, F 0.548)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.366] [G acc: 0.094]\n",
      "2487 [D loss: (0.571)(R 0.623, F 0.520)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.323] [G acc: 0.047]\n",
      "2488 [D loss: (0.622)(R 0.615, F 0.630)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.268] [G acc: 0.094]\n",
      "2489 [D loss: (0.473)(R 0.487, F 0.459)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.315] [G acc: 0.172]\n",
      "2490 [D loss: (0.552)(R 0.483, F 0.621)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.347] [G acc: 0.141]\n",
      "2491 [D loss: (0.652)(R 0.722, F 0.581)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.329] [G acc: 0.031]\n",
      "2492 [D loss: (0.600)(R 0.538, F 0.662)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.199] [G acc: 0.125]\n",
      "2493 [D loss: (0.630)(R 0.695, F 0.566)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.255] [G acc: 0.125]\n",
      "2494 [D loss: (0.582)(R 0.599, F 0.564)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.180] [G acc: 0.109]\n",
      "2495 [D loss: (0.606)(R 0.569, F 0.642)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.212] [G acc: 0.125]\n",
      "2496 [D loss: (0.563)(R 0.621, F 0.504)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.240] [G acc: 0.109]\n",
      "2497 [D loss: (0.502)(R 0.533, F 0.471)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.246] [G acc: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498 [D loss: (0.600)(R 0.521, F 0.679)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.174] [G acc: 0.219]\n",
      "2499 [D loss: (0.562)(R 0.588, F 0.535)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.260] [G acc: 0.109]\n",
      "2500 [D loss: (0.551)(R 0.600, F 0.501)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.256] [G acc: 0.109]\n",
      "2501 [D loss: (0.560)(R 0.405, F 0.715)] [D acc: (0.758)(0.781, 0.734)] [G loss: 1.384] [G acc: 0.078]\n",
      "2502 [D loss: (0.605)(R 0.679, F 0.530)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.215] [G acc: 0.125]\n",
      "2503 [D loss: (0.586)(R 0.601, F 0.572)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.198] [G acc: 0.109]\n",
      "2504 [D loss: (0.592)(R 0.601, F 0.582)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.156] [G acc: 0.203]\n",
      "2505 [D loss: (0.604)(R 0.544, F 0.664)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.418] [G acc: 0.078]\n",
      "2506 [D loss: (0.604)(R 0.664, F 0.545)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.246] [G acc: 0.094]\n",
      "2507 [D loss: (0.592)(R 0.691, F 0.493)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.189] [G acc: 0.156]\n",
      "2508 [D loss: (0.612)(R 0.536, F 0.688)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.163] [G acc: 0.156]\n",
      "2509 [D loss: (0.479)(R 0.478, F 0.479)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.357] [G acc: 0.109]\n",
      "2510 [D loss: (0.567)(R 0.607, F 0.527)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.367] [G acc: 0.109]\n",
      "2511 [D loss: (0.542)(R 0.553, F 0.530)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.420] [G acc: 0.109]\n",
      "2512 [D loss: (0.543)(R 0.579, F 0.508)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.239] [G acc: 0.156]\n",
      "2513 [D loss: (0.581)(R 0.572, F 0.590)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.318] [G acc: 0.094]\n",
      "2514 [D loss: (0.581)(R 0.538, F 0.624)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.137] [G acc: 0.203]\n",
      "2515 [D loss: (0.519)(R 0.399, F 0.640)] [D acc: (0.797)(0.797, 0.797)] [G loss: 1.234] [G acc: 0.094]\n",
      "2516 [D loss: (0.604)(R 0.621, F 0.586)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.169] [G acc: 0.125]\n",
      "2517 [D loss: (0.530)(R 0.592, F 0.468)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.213] [G acc: 0.172]\n",
      "2518 [D loss: (0.562)(R 0.558, F 0.567)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.177] [G acc: 0.234]\n",
      "2519 [D loss: (0.664)(R 0.678, F 0.651)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.300] [G acc: 0.047]\n",
      "2520 [D loss: (0.635)(R 0.632, F 0.638)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.304] [G acc: 0.188]\n",
      "2521 [D loss: (0.670)(R 0.642, F 0.697)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.146] [G acc: 0.141]\n",
      "2522 [D loss: (0.602)(R 0.558, F 0.646)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.301] [G acc: 0.156]\n",
      "2523 [D loss: (0.568)(R 0.662, F 0.474)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.270] [G acc: 0.094]\n",
      "2524 [D loss: (0.527)(R 0.484, F 0.570)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.356] [G acc: 0.188]\n",
      "2525 [D loss: (0.596)(R 0.588, F 0.604)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.351] [G acc: 0.078]\n",
      "2526 [D loss: (0.622)(R 0.693, F 0.551)] [D acc: (0.656)(0.484, 0.828)] [G loss: 1.242] [G acc: 0.141]\n",
      "2527 [D loss: (0.505)(R 0.553, F 0.458)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.258] [G acc: 0.047]\n",
      "2528 [D loss: (0.526)(R 0.478, F 0.574)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.324] [G acc: 0.031]\n",
      "2529 [D loss: (0.520)(R 0.607, F 0.433)] [D acc: (0.750)(0.594, 0.906)] [G loss: 1.387] [G acc: 0.031]\n",
      "2530 [D loss: (0.562)(R 0.518, F 0.606)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.268] [G acc: 0.094]\n",
      "2531 [D loss: (0.608)(R 0.680, F 0.535)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.232] [G acc: 0.141]\n",
      "2532 [D loss: (0.538)(R 0.470, F 0.606)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.343] [G acc: 0.062]\n",
      "2533 [D loss: (0.560)(R 0.613, F 0.506)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.311] [G acc: 0.125]\n",
      "2534 [D loss: (0.518)(R 0.502, F 0.535)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.302] [G acc: 0.094]\n",
      "2535 [D loss: (0.537)(R 0.557, F 0.518)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.356] [G acc: 0.109]\n",
      "2536 [D loss: (0.559)(R 0.541, F 0.577)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.431] [G acc: 0.109]\n",
      "2537 [D loss: (0.565)(R 0.615, F 0.515)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.335] [G acc: 0.156]\n",
      "2538 [D loss: (0.610)(R 0.568, F 0.651)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.204] [G acc: 0.203]\n",
      "2539 [D loss: (0.644)(R 0.633, F 0.655)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.155] [G acc: 0.219]\n",
      "2540 [D loss: (0.542)(R 0.535, F 0.548)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.220] [G acc: 0.141]\n",
      "2541 [D loss: (0.524)(R 0.488, F 0.559)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.221] [G acc: 0.109]\n",
      "2542 [D loss: (0.479)(R 0.467, F 0.492)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.353] [G acc: 0.031]\n",
      "2543 [D loss: (0.542)(R 0.560, F 0.524)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.089] [G acc: 0.250]\n",
      "2544 [D loss: (0.615)(R 0.506, F 0.724)] [D acc: (0.672)(0.719, 0.625)] [G loss: 1.200] [G acc: 0.094]\n",
      "2545 [D loss: (0.610)(R 0.696, F 0.524)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.232] [G acc: 0.125]\n",
      "2546 [D loss: (0.570)(R 0.490, F 0.650)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.338] [G acc: 0.047]\n",
      "2547 [D loss: (0.559)(R 0.596, F 0.521)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.215] [G acc: 0.156]\n",
      "2548 [D loss: (0.508)(R 0.529, F 0.487)] [D acc: (0.773)(0.656, 0.891)] [G loss: 1.376] [G acc: 0.094]\n",
      "2549 [D loss: (0.530)(R 0.471, F 0.590)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.267] [G acc: 0.094]\n",
      "2550 [D loss: (0.512)(R 0.470, F 0.554)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.221] [G acc: 0.109]\n",
      "2551 [D loss: (0.569)(R 0.588, F 0.550)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.353] [G acc: 0.188]\n",
      "2552 [D loss: (0.575)(R 0.606, F 0.544)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.374] [G acc: 0.047]\n",
      "2553 [D loss: (0.556)(R 0.557, F 0.555)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.184] [G acc: 0.078]\n",
      "2554 [D loss: (0.570)(R 0.596, F 0.545)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.247] [G acc: 0.109]\n",
      "2555 [D loss: (0.597)(R 0.560, F 0.633)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.205] [G acc: 0.141]\n",
      "2556 [D loss: (0.568)(R 0.567, F 0.570)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.218] [G acc: 0.203]\n",
      "2557 [D loss: (0.583)(R 0.581, F 0.584)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.271] [G acc: 0.078]\n",
      "2558 [D loss: (0.582)(R 0.597, F 0.568)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.265] [G acc: 0.094]\n",
      "2559 [D loss: (0.531)(R 0.584, F 0.479)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.329] [G acc: 0.078]\n",
      "2560 [D loss: (0.558)(R 0.609, F 0.507)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.223] [G acc: 0.172]\n",
      "2561 [D loss: (0.580)(R 0.548, F 0.613)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.327] [G acc: 0.109]\n",
      "2562 [D loss: (0.549)(R 0.557, F 0.542)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.462] [G acc: 0.109]\n",
      "2563 [D loss: (0.484)(R 0.549, F 0.420)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.247] [G acc: 0.125]\n",
      "2564 [D loss: (0.567)(R 0.471, F 0.663)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.193] [G acc: 0.141]\n",
      "2565 [D loss: (0.569)(R 0.511, F 0.628)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.244] [G acc: 0.109]\n",
      "2566 [D loss: (0.581)(R 0.626, F 0.536)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.300] [G acc: 0.109]\n",
      "2567 [D loss: (0.571)(R 0.521, F 0.621)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.213] [G acc: 0.109]\n",
      "2568 [D loss: (0.549)(R 0.572, F 0.527)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.211] [G acc: 0.156]\n",
      "2569 [D loss: (0.643)(R 0.603, F 0.682)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.260] [G acc: 0.078]\n",
      "2570 [D loss: (0.646)(R 0.641, F 0.650)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.241] [G acc: 0.047]\n",
      "2571 [D loss: (0.483)(R 0.532, F 0.435)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.281] [G acc: 0.141]\n",
      "2572 [D loss: (0.515)(R 0.560, F 0.471)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.282] [G acc: 0.109]\n",
      "2573 [D loss: (0.598)(R 0.569, F 0.628)] [D acc: (0.625)(0.641, 0.609)] [G loss: 1.358] [G acc: 0.078]\n",
      "2574 [D loss: (0.654)(R 0.639, F 0.670)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.255] [G acc: 0.188]\n",
      "2575 [D loss: (0.547)(R 0.553, F 0.541)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.191] [G acc: 0.125]\n",
      "2576 [D loss: (0.593)(R 0.623, F 0.563)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.235] [G acc: 0.188]\n",
      "2577 [D loss: (0.600)(R 0.507, F 0.693)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.332] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2578 [D loss: (0.483)(R 0.463, F 0.503)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.430] [G acc: 0.109]\n",
      "2579 [D loss: (0.502)(R 0.490, F 0.513)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.295] [G acc: 0.109]\n",
      "2580 [D loss: (0.592)(R 0.615, F 0.568)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.345] [G acc: 0.141]\n",
      "2581 [D loss: (0.536)(R 0.575, F 0.497)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.221] [G acc: 0.141]\n",
      "2582 [D loss: (0.628)(R 0.580, F 0.676)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.357] [G acc: 0.094]\n",
      "2583 [D loss: (0.524)(R 0.545, F 0.503)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.156] [G acc: 0.094]\n",
      "2584 [D loss: (0.652)(R 0.619, F 0.685)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.295] [G acc: 0.094]\n",
      "2585 [D loss: (0.590)(R 0.674, F 0.507)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.281] [G acc: 0.094]\n",
      "2586 [D loss: (0.585)(R 0.622, F 0.547)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.220] [G acc: 0.141]\n",
      "2587 [D loss: (0.544)(R 0.516, F 0.573)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.405] [G acc: 0.031]\n",
      "2588 [D loss: (0.503)(R 0.533, F 0.472)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.331] [G acc: 0.141]\n",
      "2589 [D loss: (0.638)(R 0.455, F 0.820)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.221] [G acc: 0.109]\n",
      "2590 [D loss: (0.541)(R 0.550, F 0.533)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.388] [G acc: 0.062]\n",
      "2591 [D loss: (0.570)(R 0.572, F 0.569)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.299] [G acc: 0.125]\n",
      "2592 [D loss: (0.580)(R 0.558, F 0.601)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.207] [G acc: 0.156]\n",
      "2593 [D loss: (0.573)(R 0.613, F 0.533)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.290] [G acc: 0.141]\n",
      "2594 [D loss: (0.582)(R 0.628, F 0.537)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.258] [G acc: 0.141]\n",
      "2595 [D loss: (0.567)(R 0.577, F 0.557)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.351] [G acc: 0.141]\n",
      "2596 [D loss: (0.545)(R 0.541, F 0.548)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.327] [G acc: 0.125]\n",
      "2597 [D loss: (0.517)(R 0.586, F 0.448)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.255] [G acc: 0.109]\n",
      "2598 [D loss: (0.545)(R 0.573, F 0.518)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.328] [G acc: 0.141]\n",
      "2599 [D loss: (0.524)(R 0.474, F 0.574)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.319] [G acc: 0.156]\n",
      "2600 [D loss: (0.520)(R 0.516, F 0.525)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.354] [G acc: 0.172]\n",
      "2601 [D loss: (0.567)(R 0.578, F 0.555)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.452] [G acc: 0.078]\n",
      "2602 [D loss: (0.470)(R 0.471, F 0.470)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.309] [G acc: 0.156]\n",
      "2603 [D loss: (0.559)(R 0.560, F 0.559)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.357] [G acc: 0.141]\n",
      "2604 [D loss: (0.690)(R 0.682, F 0.698)] [D acc: (0.570)(0.562, 0.578)] [G loss: 1.369] [G acc: 0.094]\n",
      "2605 [D loss: (0.647)(R 0.701, F 0.592)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.248] [G acc: 0.078]\n",
      "2606 [D loss: (0.523)(R 0.588, F 0.457)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.364] [G acc: 0.125]\n",
      "2607 [D loss: (0.653)(R 0.522, F 0.785)] [D acc: (0.664)(0.719, 0.609)] [G loss: 1.341] [G acc: 0.031]\n",
      "2608 [D loss: (0.572)(R 0.647, F 0.497)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.355] [G acc: 0.125]\n",
      "2609 [D loss: (0.551)(R 0.563, F 0.539)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.229] [G acc: 0.172]\n",
      "2610 [D loss: (0.583)(R 0.671, F 0.494)] [D acc: (0.688)(0.500, 0.875)] [G loss: 1.375] [G acc: 0.078]\n",
      "2611 [D loss: (0.601)(R 0.615, F 0.588)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.243] [G acc: 0.109]\n",
      "2612 [D loss: (0.537)(R 0.534, F 0.540)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.228] [G acc: 0.078]\n",
      "2613 [D loss: (0.475)(R 0.437, F 0.512)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.334] [G acc: 0.078]\n",
      "2614 [D loss: (0.579)(R 0.533, F 0.624)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.221] [G acc: 0.156]\n",
      "2615 [D loss: (0.645)(R 0.628, F 0.663)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.366] [G acc: 0.109]\n",
      "2616 [D loss: (0.553)(R 0.552, F 0.554)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.274] [G acc: 0.109]\n",
      "2617 [D loss: (0.539)(R 0.492, F 0.586)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.330] [G acc: 0.172]\n",
      "2618 [D loss: (0.547)(R 0.528, F 0.566)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.200] [G acc: 0.109]\n",
      "2619 [D loss: (0.586)(R 0.619, F 0.552)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.277] [G acc: 0.125]\n",
      "2620 [D loss: (0.610)(R 0.652, F 0.568)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.330] [G acc: 0.062]\n",
      "2621 [D loss: (0.522)(R 0.419, F 0.624)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.332] [G acc: 0.094]\n",
      "2622 [D loss: (0.604)(R 0.639, F 0.570)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.215] [G acc: 0.109]\n",
      "2623 [D loss: (0.515)(R 0.513, F 0.518)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.238] [G acc: 0.125]\n",
      "2624 [D loss: (0.561)(R 0.529, F 0.593)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.326] [G acc: 0.094]\n",
      "2625 [D loss: (0.581)(R 0.577, F 0.584)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.171] [G acc: 0.109]\n",
      "2626 [D loss: (0.600)(R 0.677, F 0.523)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.153] [G acc: 0.141]\n",
      "2627 [D loss: (0.569)(R 0.512, F 0.627)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.320] [G acc: 0.156]\n",
      "2628 [D loss: (0.562)(R 0.607, F 0.516)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.087] [G acc: 0.141]\n",
      "2629 [D loss: (0.570)(R 0.546, F 0.594)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.244] [G acc: 0.078]\n",
      "2630 [D loss: (0.515)(R 0.564, F 0.466)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.278] [G acc: 0.125]\n",
      "2631 [D loss: (0.594)(R 0.591, F 0.596)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.318] [G acc: 0.156]\n",
      "2632 [D loss: (0.557)(R 0.542, F 0.571)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.188] [G acc: 0.172]\n",
      "2633 [D loss: (0.530)(R 0.555, F 0.505)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.176] [G acc: 0.188]\n",
      "2634 [D loss: (0.590)(R 0.574, F 0.605)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.200] [G acc: 0.156]\n",
      "2635 [D loss: (0.566)(R 0.564, F 0.567)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.269] [G acc: 0.141]\n",
      "2636 [D loss: (0.587)(R 0.674, F 0.501)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.210] [G acc: 0.125]\n",
      "2637 [D loss: (0.581)(R 0.545, F 0.618)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.162] [G acc: 0.125]\n",
      "2638 [D loss: (0.585)(R 0.613, F 0.557)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.170] [G acc: 0.094]\n",
      "2639 [D loss: (0.511)(R 0.489, F 0.532)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.149] [G acc: 0.125]\n",
      "2640 [D loss: (0.645)(R 0.716, F 0.573)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.203] [G acc: 0.125]\n",
      "2641 [D loss: (0.514)(R 0.482, F 0.545)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.277] [G acc: 0.156]\n",
      "2642 [D loss: (0.566)(R 0.555, F 0.577)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.252] [G acc: 0.125]\n",
      "2643 [D loss: (0.584)(R 0.606, F 0.562)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.124] [G acc: 0.156]\n",
      "2644 [D loss: (0.583)(R 0.564, F 0.602)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.089] [G acc: 0.125]\n",
      "2645 [D loss: (0.600)(R 0.629, F 0.571)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.153] [G acc: 0.078]\n",
      "2646 [D loss: (0.591)(R 0.644, F 0.537)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.169] [G acc: 0.125]\n",
      "2647 [D loss: (0.559)(R 0.595, F 0.522)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.334] [G acc: 0.062]\n",
      "2648 [D loss: (0.649)(R 0.660, F 0.637)] [D acc: (0.602)(0.531, 0.672)] [G loss: 1.206] [G acc: 0.141]\n",
      "2649 [D loss: (0.596)(R 0.641, F 0.551)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.149] [G acc: 0.078]\n",
      "2650 [D loss: (0.596)(R 0.604, F 0.587)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.223] [G acc: 0.062]\n",
      "2651 [D loss: (0.558)(R 0.565, F 0.551)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.174] [G acc: 0.094]\n",
      "2652 [D loss: (0.562)(R 0.530, F 0.594)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.230] [G acc: 0.094]\n",
      "2653 [D loss: (0.528)(R 0.526, F 0.531)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.228] [G acc: 0.047]\n",
      "2654 [D loss: (0.599)(R 0.634, F 0.564)] [D acc: (0.609)(0.531, 0.688)] [G loss: 1.263] [G acc: 0.094]\n",
      "2655 [D loss: (0.537)(R 0.563, F 0.510)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.238] [G acc: 0.156]\n",
      "2656 [D loss: (0.646)(R 0.620, F 0.673)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.294] [G acc: 0.062]\n",
      "2657 [D loss: (0.595)(R 0.635, F 0.555)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.209] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2658 [D loss: (0.589)(R 0.611, F 0.567)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.094] [G acc: 0.234]\n",
      "2659 [D loss: (0.520)(R 0.500, F 0.540)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.192] [G acc: 0.172]\n",
      "2660 [D loss: (0.532)(R 0.575, F 0.490)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.277] [G acc: 0.125]\n",
      "2661 [D loss: (0.598)(R 0.573, F 0.622)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.204] [G acc: 0.172]\n",
      "2662 [D loss: (0.685)(R 0.592, F 0.778)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.201] [G acc: 0.172]\n",
      "2663 [D loss: (0.551)(R 0.602, F 0.500)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.050] [G acc: 0.172]\n",
      "2664 [D loss: (0.577)(R 0.600, F 0.553)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.160] [G acc: 0.125]\n",
      "2665 [D loss: (0.527)(R 0.546, F 0.508)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.184] [G acc: 0.125]\n",
      "2666 [D loss: (0.620)(R 0.558, F 0.682)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.195] [G acc: 0.125]\n",
      "2667 [D loss: (0.596)(R 0.671, F 0.521)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.167] [G acc: 0.047]\n",
      "2668 [D loss: (0.568)(R 0.590, F 0.547)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.246] [G acc: 0.062]\n",
      "2669 [D loss: (0.519)(R 0.419, F 0.620)] [D acc: (0.742)(0.797, 0.688)] [G loss: 1.220] [G acc: 0.078]\n",
      "2670 [D loss: (0.462)(R 0.450, F 0.473)] [D acc: (0.828)(0.781, 0.875)] [G loss: 1.170] [G acc: 0.172]\n",
      "2671 [D loss: (0.616)(R 0.532, F 0.700)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.313] [G acc: 0.094]\n",
      "2672 [D loss: (0.560)(R 0.549, F 0.570)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.257] [G acc: 0.172]\n",
      "2673 [D loss: (0.509)(R 0.463, F 0.556)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.304] [G acc: 0.156]\n",
      "2674 [D loss: (0.603)(R 0.629, F 0.576)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.247] [G acc: 0.062]\n",
      "2675 [D loss: (0.593)(R 0.644, F 0.543)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.139] [G acc: 0.156]\n",
      "2676 [D loss: (0.574)(R 0.582, F 0.566)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.293] [G acc: 0.172]\n",
      "2677 [D loss: (0.614)(R 0.675, F 0.554)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.213] [G acc: 0.141]\n",
      "2678 [D loss: (0.595)(R 0.554, F 0.637)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.154] [G acc: 0.078]\n",
      "2679 [D loss: (0.601)(R 0.707, F 0.495)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.240] [G acc: 0.125]\n",
      "2680 [D loss: (0.601)(R 0.534, F 0.668)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.267] [G acc: 0.156]\n",
      "2681 [D loss: (0.500)(R 0.592, F 0.408)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.221] [G acc: 0.188]\n",
      "2682 [D loss: (0.642)(R 0.601, F 0.683)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.206] [G acc: 0.094]\n",
      "2683 [D loss: (0.588)(R 0.601, F 0.574)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.109] [G acc: 0.172]\n",
      "2684 [D loss: (0.457)(R 0.439, F 0.475)] [D acc: (0.836)(0.812, 0.859)] [G loss: 1.146] [G acc: 0.125]\n",
      "2685 [D loss: (0.656)(R 0.644, F 0.668)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.136] [G acc: 0.156]\n",
      "2686 [D loss: (0.543)(R 0.572, F 0.514)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.072] [G acc: 0.172]\n",
      "2687 [D loss: (0.573)(R 0.590, F 0.556)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.359] [G acc: 0.156]\n",
      "2688 [D loss: (0.592)(R 0.698, F 0.486)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.277] [G acc: 0.141]\n",
      "2689 [D loss: (0.629)(R 0.732, F 0.526)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.156] [G acc: 0.125]\n",
      "2690 [D loss: (0.638)(R 0.594, F 0.683)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.167] [G acc: 0.156]\n",
      "2691 [D loss: (0.613)(R 0.599, F 0.627)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.207] [G acc: 0.172]\n",
      "2692 [D loss: (0.574)(R 0.602, F 0.545)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.142] [G acc: 0.156]\n",
      "2693 [D loss: (0.511)(R 0.519, F 0.502)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.058] [G acc: 0.188]\n",
      "2694 [D loss: (0.559)(R 0.570, F 0.549)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.231] [G acc: 0.141]\n",
      "2695 [D loss: (0.566)(R 0.491, F 0.641)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.188] [G acc: 0.109]\n",
      "2696 [D loss: (0.589)(R 0.681, F 0.498)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.233] [G acc: 0.125]\n",
      "2697 [D loss: (0.636)(R 0.605, F 0.667)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.310] [G acc: 0.125]\n",
      "2698 [D loss: (0.541)(R 0.575, F 0.508)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.158] [G acc: 0.141]\n",
      "2699 [D loss: (0.563)(R 0.558, F 0.567)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.107] [G acc: 0.141]\n",
      "2700 [D loss: (0.583)(R 0.502, F 0.663)] [D acc: (0.695)(0.750, 0.641)] [G loss: 1.262] [G acc: 0.141]\n",
      "2701 [D loss: (0.528)(R 0.510, F 0.545)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.165] [G acc: 0.109]\n",
      "2702 [D loss: (0.614)(R 0.642, F 0.586)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.117] [G acc: 0.156]\n",
      "2703 [D loss: (0.505)(R 0.520, F 0.489)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.188] [G acc: 0.094]\n",
      "2704 [D loss: (0.532)(R 0.585, F 0.479)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.246] [G acc: 0.125]\n",
      "2705 [D loss: (0.507)(R 0.575, F 0.438)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.221] [G acc: 0.141]\n",
      "2706 [D loss: (0.639)(R 0.621, F 0.657)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.182] [G acc: 0.203]\n",
      "2707 [D loss: (0.610)(R 0.650, F 0.569)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.202] [G acc: 0.109]\n",
      "2708 [D loss: (0.560)(R 0.582, F 0.537)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.157] [G acc: 0.094]\n",
      "2709 [D loss: (0.551)(R 0.576, F 0.525)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.119] [G acc: 0.156]\n",
      "2710 [D loss: (0.553)(R 0.586, F 0.521)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.295] [G acc: 0.109]\n",
      "2711 [D loss: (0.557)(R 0.535, F 0.579)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.124] [G acc: 0.188]\n",
      "2712 [D loss: (0.534)(R 0.533, F 0.535)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.218] [G acc: 0.078]\n",
      "2713 [D loss: (0.580)(R 0.610, F 0.549)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.257] [G acc: 0.094]\n",
      "2714 [D loss: (0.563)(R 0.553, F 0.574)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.186] [G acc: 0.094]\n",
      "2715 [D loss: (0.622)(R 0.528, F 0.716)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.269] [G acc: 0.062]\n",
      "2716 [D loss: (0.576)(R 0.655, F 0.496)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.289] [G acc: 0.125]\n",
      "2717 [D loss: (0.563)(R 0.652, F 0.474)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.203] [G acc: 0.141]\n",
      "2718 [D loss: (0.543)(R 0.532, F 0.554)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.539] [G acc: 0.031]\n",
      "2719 [D loss: (0.574)(R 0.621, F 0.527)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.151] [G acc: 0.203]\n",
      "2720 [D loss: (0.571)(R 0.618, F 0.525)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.155] [G acc: 0.141]\n",
      "2721 [D loss: (0.607)(R 0.623, F 0.592)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.181] [G acc: 0.203]\n",
      "2722 [D loss: (0.528)(R 0.474, F 0.582)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.267] [G acc: 0.094]\n",
      "2723 [D loss: (0.568)(R 0.501, F 0.635)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.259] [G acc: 0.156]\n",
      "2724 [D loss: (0.517)(R 0.532, F 0.502)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.336] [G acc: 0.062]\n",
      "2725 [D loss: (0.624)(R 0.674, F 0.574)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.107] [G acc: 0.109]\n",
      "2726 [D loss: (0.560)(R 0.554, F 0.565)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.231] [G acc: 0.141]\n",
      "2727 [D loss: (0.592)(R 0.607, F 0.578)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.335] [G acc: 0.156]\n",
      "2728 [D loss: (0.562)(R 0.558, F 0.566)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.053] [G acc: 0.250]\n",
      "2729 [D loss: (0.624)(R 0.641, F 0.606)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.193] [G acc: 0.156]\n",
      "2730 [D loss: (0.515)(R 0.467, F 0.564)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.510] [G acc: 0.062]\n",
      "2731 [D loss: (0.541)(R 0.560, F 0.523)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.376] [G acc: 0.078]\n",
      "2732 [D loss: (0.539)(R 0.601, F 0.478)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.184] [G acc: 0.141]\n",
      "2733 [D loss: (0.507)(R 0.451, F 0.563)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.362] [G acc: 0.047]\n",
      "2734 [D loss: (0.660)(R 0.579, F 0.740)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.184] [G acc: 0.125]\n",
      "2735 [D loss: (0.616)(R 0.585, F 0.647)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.245] [G acc: 0.141]\n",
      "2736 [D loss: (0.589)(R 0.637, F 0.541)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.207] [G acc: 0.156]\n",
      "2737 [D loss: (0.549)(R 0.563, F 0.535)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.175] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2738 [D loss: (0.559)(R 0.580, F 0.538)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.295] [G acc: 0.141]\n",
      "2739 [D loss: (0.645)(R 0.583, F 0.707)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.246] [G acc: 0.156]\n",
      "2740 [D loss: (0.551)(R 0.526, F 0.577)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.246] [G acc: 0.125]\n",
      "2741 [D loss: (0.499)(R 0.485, F 0.512)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.203] [G acc: 0.156]\n",
      "2742 [D loss: (0.640)(R 0.612, F 0.668)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.129] [G acc: 0.141]\n",
      "2743 [D loss: (0.651)(R 0.606, F 0.695)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.250] [G acc: 0.125]\n",
      "2744 [D loss: (0.604)(R 0.642, F 0.566)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.337] [G acc: 0.109]\n",
      "2745 [D loss: (0.591)(R 0.609, F 0.573)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.135] [G acc: 0.219]\n",
      "2746 [D loss: (0.571)(R 0.443, F 0.700)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.362] [G acc: 0.047]\n",
      "2747 [D loss: (0.585)(R 0.629, F 0.542)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.258] [G acc: 0.062]\n",
      "2748 [D loss: (0.670)(R 0.743, F 0.596)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.172] [G acc: 0.172]\n",
      "2749 [D loss: (0.535)(R 0.610, F 0.459)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.136] [G acc: 0.188]\n",
      "2750 [D loss: (0.501)(R 0.462, F 0.541)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.236] [G acc: 0.125]\n",
      "2751 [D loss: (0.661)(R 0.558, F 0.764)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.293] [G acc: 0.078]\n",
      "2752 [D loss: (0.654)(R 0.766, F 0.542)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.251] [G acc: 0.125]\n",
      "2753 [D loss: (0.547)(R 0.667, F 0.426)] [D acc: (0.758)(0.594, 0.922)] [G loss: 1.163] [G acc: 0.188]\n",
      "2754 [D loss: (0.565)(R 0.544, F 0.587)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.146] [G acc: 0.172]\n",
      "2755 [D loss: (0.554)(R 0.577, F 0.532)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.153] [G acc: 0.156]\n",
      "2756 [D loss: (0.575)(R 0.593, F 0.556)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.177] [G acc: 0.156]\n",
      "2757 [D loss: (0.613)(R 0.634, F 0.592)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.215] [G acc: 0.109]\n",
      "2758 [D loss: (0.555)(R 0.582, F 0.528)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.070] [G acc: 0.172]\n",
      "2759 [D loss: (0.613)(R 0.601, F 0.624)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.299] [G acc: 0.094]\n",
      "2760 [D loss: (0.587)(R 0.541, F 0.634)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.241] [G acc: 0.141]\n",
      "2761 [D loss: (0.658)(R 0.716, F 0.599)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.273] [G acc: 0.094]\n",
      "2762 [D loss: (0.516)(R 0.582, F 0.450)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.123] [G acc: 0.156]\n",
      "2763 [D loss: (0.489)(R 0.464, F 0.515)] [D acc: (0.773)(0.781, 0.766)] [G loss: 1.317] [G acc: 0.109]\n",
      "2764 [D loss: (0.544)(R 0.517, F 0.571)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.290] [G acc: 0.078]\n",
      "2765 [D loss: (0.586)(R 0.577, F 0.594)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.333] [G acc: 0.078]\n",
      "2766 [D loss: (0.593)(R 0.606, F 0.581)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.239] [G acc: 0.062]\n",
      "2767 [D loss: (0.512)(R 0.507, F 0.516)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.283] [G acc: 0.156]\n",
      "2768 [D loss: (0.519)(R 0.567, F 0.472)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.323] [G acc: 0.094]\n",
      "2769 [D loss: (0.604)(R 0.518, F 0.690)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.307] [G acc: 0.078]\n",
      "2770 [D loss: (0.489)(R 0.490, F 0.489)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.410] [G acc: 0.125]\n",
      "2771 [D loss: (0.556)(R 0.610, F 0.502)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.350] [G acc: 0.078]\n",
      "2772 [D loss: (0.559)(R 0.578, F 0.539)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.374] [G acc: 0.047]\n",
      "2773 [D loss: (0.495)(R 0.558, F 0.432)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.320] [G acc: 0.156]\n",
      "2774 [D loss: (0.589)(R 0.664, F 0.514)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.392] [G acc: 0.125]\n",
      "2775 [D loss: (0.516)(R 0.403, F 0.628)] [D acc: (0.773)(0.781, 0.766)] [G loss: 1.420] [G acc: 0.078]\n",
      "2776 [D loss: (0.604)(R 0.617, F 0.591)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.407] [G acc: 0.031]\n",
      "2777 [D loss: (0.494)(R 0.596, F 0.392)] [D acc: (0.758)(0.594, 0.922)] [G loss: 1.312] [G acc: 0.125]\n",
      "2778 [D loss: (0.603)(R 0.640, F 0.566)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.316] [G acc: 0.094]\n",
      "2779 [D loss: (0.584)(R 0.724, F 0.443)] [D acc: (0.742)(0.594, 0.891)] [G loss: 1.204] [G acc: 0.141]\n",
      "2780 [D loss: (0.591)(R 0.560, F 0.623)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.239] [G acc: 0.109]\n",
      "2781 [D loss: (0.604)(R 0.517, F 0.692)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.203] [G acc: 0.172]\n",
      "2782 [D loss: (0.588)(R 0.586, F 0.590)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.305] [G acc: 0.062]\n",
      "2783 [D loss: (0.523)(R 0.512, F 0.533)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.200] [G acc: 0.266]\n",
      "2784 [D loss: (0.570)(R 0.448, F 0.691)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.363] [G acc: 0.094]\n",
      "2785 [D loss: (0.596)(R 0.666, F 0.527)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.153] [G acc: 0.078]\n",
      "2786 [D loss: (0.545)(R 0.526, F 0.563)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.257] [G acc: 0.094]\n",
      "2787 [D loss: (0.622)(R 0.695, F 0.550)] [D acc: (0.625)(0.484, 0.766)] [G loss: 1.385] [G acc: 0.156]\n",
      "2788 [D loss: (0.547)(R 0.588, F 0.506)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.212] [G acc: 0.156]\n",
      "2789 [D loss: (0.573)(R 0.567, F 0.580)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.300] [G acc: 0.078]\n",
      "2790 [D loss: (0.491)(R 0.518, F 0.463)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.280] [G acc: 0.125]\n",
      "2791 [D loss: (0.623)(R 0.567, F 0.678)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.386] [G acc: 0.031]\n",
      "2792 [D loss: (0.548)(R 0.607, F 0.489)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.301] [G acc: 0.078]\n",
      "2793 [D loss: (0.495)(R 0.509, F 0.482)] [D acc: (0.805)(0.719, 0.891)] [G loss: 1.321] [G acc: 0.141]\n",
      "2794 [D loss: (0.535)(R 0.528, F 0.541)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.308] [G acc: 0.188]\n",
      "2795 [D loss: (0.551)(R 0.530, F 0.571)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.151] [G acc: 0.203]\n",
      "2796 [D loss: (0.550)(R 0.569, F 0.532)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.377] [G acc: 0.047]\n",
      "2797 [D loss: (0.537)(R 0.556, F 0.518)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.278] [G acc: 0.156]\n",
      "2798 [D loss: (0.572)(R 0.572, F 0.573)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.270] [G acc: 0.109]\n",
      "2799 [D loss: (0.543)(R 0.519, F 0.568)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.317] [G acc: 0.234]\n",
      "2800 [D loss: (0.609)(R 0.641, F 0.577)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.283] [G acc: 0.172]\n",
      "2801 [D loss: (0.597)(R 0.614, F 0.579)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.442] [G acc: 0.141]\n",
      "2802 [D loss: (0.560)(R 0.558, F 0.562)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.282] [G acc: 0.156]\n",
      "2803 [D loss: (0.559)(R 0.604, F 0.515)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.285] [G acc: 0.141]\n",
      "2804 [D loss: (0.546)(R 0.512, F 0.580)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.219] [G acc: 0.125]\n",
      "2805 [D loss: (0.548)(R 0.434, F 0.662)] [D acc: (0.734)(0.797, 0.672)] [G loss: 1.363] [G acc: 0.109]\n",
      "2806 [D loss: (0.566)(R 0.641, F 0.491)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.276] [G acc: 0.141]\n",
      "2807 [D loss: (0.652)(R 0.581, F 0.722)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.324] [G acc: 0.078]\n",
      "2808 [D loss: (0.612)(R 0.731, F 0.494)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.269] [G acc: 0.125]\n",
      "2809 [D loss: (0.540)(R 0.489, F 0.591)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.252] [G acc: 0.094]\n",
      "2810 [D loss: (0.507)(R 0.535, F 0.479)] [D acc: (0.805)(0.672, 0.938)] [G loss: 1.428] [G acc: 0.125]\n",
      "2811 [D loss: (0.597)(R 0.536, F 0.659)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.575] [G acc: 0.047]\n",
      "2812 [D loss: (0.602)(R 0.695, F 0.508)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.327] [G acc: 0.156]\n",
      "2813 [D loss: (0.558)(R 0.630, F 0.487)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.290] [G acc: 0.141]\n",
      "2814 [D loss: (0.596)(R 0.585, F 0.607)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.290] [G acc: 0.094]\n",
      "2815 [D loss: (0.563)(R 0.567, F 0.560)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.329] [G acc: 0.062]\n",
      "2816 [D loss: (0.527)(R 0.491, F 0.563)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.306] [G acc: 0.141]\n",
      "2817 [D loss: (0.608)(R 0.655, F 0.560)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.251] [G acc: 0.156]\n",
      "2818 [D loss: (0.582)(R 0.553, F 0.610)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.541] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2819 [D loss: (0.637)(R 0.719, F 0.556)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.210] [G acc: 0.156]\n",
      "2820 [D loss: (0.559)(R 0.574, F 0.544)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.313] [G acc: 0.109]\n",
      "2821 [D loss: (0.552)(R 0.501, F 0.604)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.306] [G acc: 0.109]\n",
      "2822 [D loss: (0.538)(R 0.578, F 0.499)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.413] [G acc: 0.109]\n",
      "2823 [D loss: (0.510)(R 0.489, F 0.531)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.252] [G acc: 0.156]\n",
      "2824 [D loss: (0.553)(R 0.619, F 0.487)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.215] [G acc: 0.156]\n",
      "2825 [D loss: (0.561)(R 0.577, F 0.546)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.329] [G acc: 0.109]\n",
      "2826 [D loss: (0.605)(R 0.469, F 0.742)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.353] [G acc: 0.109]\n",
      "2827 [D loss: (0.628)(R 0.663, F 0.593)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.245] [G acc: 0.062]\n",
      "2828 [D loss: (0.666)(R 0.705, F 0.627)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.260] [G acc: 0.062]\n",
      "2829 [D loss: (0.527)(R 0.595, F 0.459)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.157] [G acc: 0.094]\n",
      "2830 [D loss: (0.568)(R 0.617, F 0.519)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.111] [G acc: 0.234]\n",
      "2831 [D loss: (0.557)(R 0.533, F 0.582)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.349] [G acc: 0.156]\n",
      "2832 [D loss: (0.632)(R 0.648, F 0.616)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.282] [G acc: 0.047]\n",
      "2833 [D loss: (0.621)(R 0.620, F 0.622)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.277] [G acc: 0.141]\n",
      "2834 [D loss: (0.663)(R 0.730, F 0.596)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.150] [G acc: 0.203]\n",
      "2835 [D loss: (0.578)(R 0.548, F 0.608)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.195] [G acc: 0.078]\n",
      "2836 [D loss: (0.581)(R 0.582, F 0.580)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.177] [G acc: 0.203]\n",
      "2837 [D loss: (0.574)(R 0.603, F 0.545)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.280] [G acc: 0.094]\n",
      "2838 [D loss: (0.561)(R 0.603, F 0.518)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.363] [G acc: 0.125]\n",
      "2839 [D loss: (0.620)(R 0.659, F 0.581)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.132] [G acc: 0.109]\n",
      "2840 [D loss: (0.659)(R 0.656, F 0.661)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.108] [G acc: 0.141]\n",
      "2841 [D loss: (0.570)(R 0.613, F 0.527)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.221] [G acc: 0.078]\n",
      "2842 [D loss: (0.484)(R 0.490, F 0.477)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.389] [G acc: 0.047]\n",
      "2843 [D loss: (0.554)(R 0.601, F 0.507)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.206] [G acc: 0.125]\n",
      "2844 [D loss: (0.652)(R 0.701, F 0.603)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.298] [G acc: 0.062]\n",
      "2845 [D loss: (0.536)(R 0.520, F 0.553)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.202] [G acc: 0.109]\n",
      "2846 [D loss: (0.561)(R 0.474, F 0.648)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.259] [G acc: 0.094]\n",
      "2847 [D loss: (0.563)(R 0.647, F 0.480)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.172] [G acc: 0.125]\n",
      "2848 [D loss: (0.653)(R 0.644, F 0.662)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.216] [G acc: 0.094]\n",
      "2849 [D loss: (0.555)(R 0.508, F 0.602)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.138] [G acc: 0.188]\n",
      "2850 [D loss: (0.586)(R 0.674, F 0.497)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.326] [G acc: 0.094]\n",
      "2851 [D loss: (0.477)(R 0.496, F 0.458)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.267] [G acc: 0.078]\n",
      "2852 [D loss: (0.624)(R 0.592, F 0.657)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.136] [G acc: 0.094]\n",
      "2853 [D loss: (0.532)(R 0.521, F 0.543)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.218] [G acc: 0.156]\n",
      "2854 [D loss: (0.593)(R 0.647, F 0.540)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.342] [G acc: 0.125]\n",
      "2855 [D loss: (0.505)(R 0.465, F 0.545)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.175] [G acc: 0.250]\n",
      "2856 [D loss: (0.463)(R 0.495, F 0.430)] [D acc: (0.805)(0.688, 0.922)] [G loss: 1.494] [G acc: 0.094]\n",
      "2857 [D loss: (0.612)(R 0.594, F 0.631)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.174] [G acc: 0.141]\n",
      "2858 [D loss: (0.673)(R 0.608, F 0.738)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.160] [G acc: 0.156]\n",
      "2859 [D loss: (0.595)(R 0.555, F 0.634)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.164] [G acc: 0.156]\n",
      "2860 [D loss: (0.551)(R 0.465, F 0.636)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.316] [G acc: 0.078]\n",
      "2861 [D loss: (0.563)(R 0.595, F 0.530)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.234] [G acc: 0.172]\n",
      "2862 [D loss: (0.554)(R 0.538, F 0.571)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.359] [G acc: 0.141]\n",
      "2863 [D loss: (0.616)(R 0.546, F 0.686)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.418] [G acc: 0.094]\n",
      "2864 [D loss: (0.576)(R 0.597, F 0.554)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.308] [G acc: 0.156]\n",
      "2865 [D loss: (0.534)(R 0.553, F 0.514)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.268] [G acc: 0.094]\n",
      "2866 [D loss: (0.558)(R 0.484, F 0.632)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.189] [G acc: 0.172]\n",
      "2867 [D loss: (0.576)(R 0.581, F 0.571)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.143] [G acc: 0.188]\n",
      "2868 [D loss: (0.532)(R 0.497, F 0.566)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.341] [G acc: 0.203]\n",
      "2869 [D loss: (0.610)(R 0.718, F 0.503)] [D acc: (0.648)(0.484, 0.812)] [G loss: 1.240] [G acc: 0.109]\n",
      "2870 [D loss: (0.639)(R 0.736, F 0.542)] [D acc: (0.633)(0.484, 0.781)] [G loss: 1.148] [G acc: 0.125]\n",
      "2871 [D loss: (0.488)(R 0.461, F 0.515)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.260] [G acc: 0.109]\n",
      "2872 [D loss: (0.620)(R 0.539, F 0.701)] [D acc: (0.625)(0.656, 0.594)] [G loss: 1.264] [G acc: 0.094]\n",
      "2873 [D loss: (0.571)(R 0.652, F 0.489)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.186] [G acc: 0.141]\n",
      "2874 [D loss: (0.587)(R 0.593, F 0.581)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.203] [G acc: 0.109]\n",
      "2875 [D loss: (0.625)(R 0.677, F 0.572)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.182] [G acc: 0.109]\n",
      "2876 [D loss: (0.584)(R 0.566, F 0.602)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.267] [G acc: 0.141]\n",
      "2877 [D loss: (0.600)(R 0.618, F 0.583)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.203] [G acc: 0.125]\n",
      "2878 [D loss: (0.565)(R 0.540, F 0.589)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.152] [G acc: 0.141]\n",
      "2879 [D loss: (0.605)(R 0.580, F 0.630)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.071] [G acc: 0.156]\n",
      "2880 [D loss: (0.544)(R 0.541, F 0.548)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.325] [G acc: 0.125]\n",
      "2881 [D loss: (0.593)(R 0.613, F 0.572)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.136] [G acc: 0.141]\n",
      "2882 [D loss: (0.627)(R 0.579, F 0.675)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.246] [G acc: 0.078]\n",
      "2883 [D loss: (0.550)(R 0.580, F 0.521)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.211] [G acc: 0.109]\n",
      "2884 [D loss: (0.488)(R 0.502, F 0.473)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.233] [G acc: 0.172]\n",
      "2885 [D loss: (0.557)(R 0.586, F 0.528)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.450] [G acc: 0.031]\n",
      "2886 [D loss: (0.449)(R 0.458, F 0.441)] [D acc: (0.820)(0.812, 0.828)] [G loss: 1.350] [G acc: 0.188]\n",
      "2887 [D loss: (0.632)(R 0.528, F 0.736)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.451] [G acc: 0.047]\n",
      "2888 [D loss: (0.587)(R 0.608, F 0.566)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.182] [G acc: 0.141]\n",
      "2889 [D loss: (0.598)(R 0.693, F 0.502)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.209] [G acc: 0.125]\n",
      "2890 [D loss: (0.538)(R 0.506, F 0.569)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.396] [G acc: 0.094]\n",
      "2891 [D loss: (0.634)(R 0.672, F 0.595)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.249] [G acc: 0.156]\n",
      "2892 [D loss: (0.543)(R 0.532, F 0.555)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.282] [G acc: 0.078]\n",
      "2893 [D loss: (0.557)(R 0.636, F 0.478)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.243] [G acc: 0.125]\n",
      "2894 [D loss: (0.563)(R 0.443, F 0.684)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.314] [G acc: 0.078]\n",
      "2895 [D loss: (0.567)(R 0.658, F 0.476)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.291] [G acc: 0.172]\n",
      "2896 [D loss: (0.545)(R 0.488, F 0.602)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.349] [G acc: 0.125]\n",
      "2897 [D loss: (0.610)(R 0.658, F 0.561)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.377] [G acc: 0.047]\n",
      "2898 [D loss: (0.538)(R 0.575, F 0.501)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.284] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2899 [D loss: (0.575)(R 0.575, F 0.575)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.369] [G acc: 0.062]\n",
      "2900 [D loss: (0.617)(R 0.534, F 0.701)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.287] [G acc: 0.078]\n",
      "2901 [D loss: (0.521)(R 0.585, F 0.458)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.360] [G acc: 0.016]\n",
      "2902 [D loss: (0.658)(R 0.618, F 0.698)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.354] [G acc: 0.078]\n",
      "2903 [D loss: (0.518)(R 0.529, F 0.507)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.280] [G acc: 0.125]\n",
      "2904 [D loss: (0.554)(R 0.643, F 0.466)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.229] [G acc: 0.141]\n",
      "2905 [D loss: (0.537)(R 0.532, F 0.542)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.312] [G acc: 0.125]\n",
      "2906 [D loss: (0.565)(R 0.639, F 0.492)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.324] [G acc: 0.172]\n",
      "2907 [D loss: (0.459)(R 0.443, F 0.476)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.404] [G acc: 0.094]\n",
      "2908 [D loss: (0.591)(R 0.625, F 0.557)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.409] [G acc: 0.062]\n",
      "2909 [D loss: (0.655)(R 0.682, F 0.627)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.149] [G acc: 0.141]\n",
      "2910 [D loss: (0.641)(R 0.552, F 0.729)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.250] [G acc: 0.109]\n",
      "2911 [D loss: (0.607)(R 0.736, F 0.479)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.209] [G acc: 0.109]\n",
      "2912 [D loss: (0.546)(R 0.576, F 0.516)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.218] [G acc: 0.156]\n",
      "2913 [D loss: (0.604)(R 0.546, F 0.662)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.203] [G acc: 0.094]\n",
      "2914 [D loss: (0.579)(R 0.623, F 0.535)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.109] [G acc: 0.172]\n",
      "2915 [D loss: (0.543)(R 0.558, F 0.527)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.165] [G acc: 0.109]\n",
      "2916 [D loss: (0.618)(R 0.609, F 0.627)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.360] [G acc: 0.047]\n",
      "2917 [D loss: (0.571)(R 0.614, F 0.529)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.335] [G acc: 0.031]\n",
      "2918 [D loss: (0.673)(R 0.652, F 0.694)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.118] [G acc: 0.172]\n",
      "2919 [D loss: (0.588)(R 0.616, F 0.561)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.152] [G acc: 0.109]\n",
      "2920 [D loss: (0.543)(R 0.548, F 0.539)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.236] [G acc: 0.109]\n",
      "2921 [D loss: (0.526)(R 0.475, F 0.577)] [D acc: (0.781)(0.766, 0.797)] [G loss: 1.159] [G acc: 0.172]\n",
      "2922 [D loss: (0.592)(R 0.589, F 0.596)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.243] [G acc: 0.094]\n",
      "2923 [D loss: (0.596)(R 0.634, F 0.558)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.171] [G acc: 0.141]\n",
      "2924 [D loss: (0.620)(R 0.634, F 0.607)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.286] [G acc: 0.125]\n",
      "2925 [D loss: (0.556)(R 0.591, F 0.522)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.217] [G acc: 0.125]\n",
      "2926 [D loss: (0.642)(R 0.624, F 0.661)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.356] [G acc: 0.062]\n",
      "2927 [D loss: (0.569)(R 0.639, F 0.500)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.216] [G acc: 0.125]\n",
      "2928 [D loss: (0.532)(R 0.489, F 0.576)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.230] [G acc: 0.188]\n",
      "2929 [D loss: (0.642)(R 0.597, F 0.687)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.115] [G acc: 0.156]\n",
      "2930 [D loss: (0.571)(R 0.592, F 0.550)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.223] [G acc: 0.156]\n",
      "2931 [D loss: (0.551)(R 0.480, F 0.623)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.139] [G acc: 0.156]\n",
      "2932 [D loss: (0.523)(R 0.546, F 0.500)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.341] [G acc: 0.078]\n",
      "2933 [D loss: (0.612)(R 0.615, F 0.610)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.349] [G acc: 0.109]\n",
      "2934 [D loss: (0.552)(R 0.591, F 0.512)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.289] [G acc: 0.078]\n",
      "2935 [D loss: (0.569)(R 0.608, F 0.530)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.348] [G acc: 0.031]\n",
      "2936 [D loss: (0.624)(R 0.615, F 0.634)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.261] [G acc: 0.078]\n",
      "2937 [D loss: (0.597)(R 0.657, F 0.536)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.265] [G acc: 0.078]\n",
      "2938 [D loss: (0.560)(R 0.545, F 0.574)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.259] [G acc: 0.125]\n",
      "2939 [D loss: (0.621)(R 0.663, F 0.579)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.206] [G acc: 0.062]\n",
      "2940 [D loss: (0.566)(R 0.568, F 0.564)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.191] [G acc: 0.141]\n",
      "2941 [D loss: (0.566)(R 0.574, F 0.558)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.230] [G acc: 0.062]\n",
      "2942 [D loss: (0.606)(R 0.639, F 0.572)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.259] [G acc: 0.078]\n",
      "2943 [D loss: (0.607)(R 0.613, F 0.601)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.147] [G acc: 0.109]\n",
      "2944 [D loss: (0.572)(R 0.593, F 0.551)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.244] [G acc: 0.125]\n",
      "2945 [D loss: (0.527)(R 0.513, F 0.542)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.143] [G acc: 0.141]\n",
      "2946 [D loss: (0.580)(R 0.563, F 0.597)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.297] [G acc: 0.125]\n",
      "2947 [D loss: (0.555)(R 0.506, F 0.604)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.187] [G acc: 0.125]\n",
      "2948 [D loss: (0.602)(R 0.644, F 0.561)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.230] [G acc: 0.141]\n",
      "2949 [D loss: (0.568)(R 0.590, F 0.545)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.293] [G acc: 0.062]\n",
      "2950 [D loss: (0.527)(R 0.563, F 0.490)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.183] [G acc: 0.109]\n",
      "2951 [D loss: (0.547)(R 0.516, F 0.578)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.419] [G acc: 0.047]\n",
      "2952 [D loss: (0.549)(R 0.532, F 0.565)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.325] [G acc: 0.047]\n",
      "2953 [D loss: (0.584)(R 0.582, F 0.587)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.260] [G acc: 0.031]\n",
      "2954 [D loss: (0.578)(R 0.673, F 0.482)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.212] [G acc: 0.094]\n",
      "2955 [D loss: (0.575)(R 0.577, F 0.573)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.295] [G acc: 0.094]\n",
      "2956 [D loss: (0.529)(R 0.549, F 0.510)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.255] [G acc: 0.141]\n",
      "2957 [D loss: (0.505)(R 0.409, F 0.601)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.384] [G acc: 0.078]\n",
      "2958 [D loss: (0.592)(R 0.694, F 0.491)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.234] [G acc: 0.156]\n",
      "2959 [D loss: (0.635)(R 0.588, F 0.682)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.448] [G acc: 0.078]\n",
      "2960 [D loss: (0.568)(R 0.577, F 0.559)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.218] [G acc: 0.047]\n",
      "2961 [D loss: (0.576)(R 0.622, F 0.530)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.177] [G acc: 0.188]\n",
      "2962 [D loss: (0.546)(R 0.544, F 0.549)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.209] [G acc: 0.109]\n",
      "2963 [D loss: (0.601)(R 0.643, F 0.559)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.241] [G acc: 0.109]\n",
      "2964 [D loss: (0.592)(R 0.622, F 0.562)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.181] [G acc: 0.141]\n",
      "2965 [D loss: (0.546)(R 0.515, F 0.577)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.190] [G acc: 0.156]\n",
      "2966 [D loss: (0.579)(R 0.523, F 0.635)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.205] [G acc: 0.078]\n",
      "2967 [D loss: (0.555)(R 0.466, F 0.644)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.317] [G acc: 0.078]\n",
      "2968 [D loss: (0.565)(R 0.631, F 0.500)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.454] [G acc: 0.078]\n",
      "2969 [D loss: (0.563)(R 0.610, F 0.516)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.396] [G acc: 0.109]\n",
      "2970 [D loss: (0.557)(R 0.576, F 0.539)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.296] [G acc: 0.156]\n",
      "2971 [D loss: (0.492)(R 0.496, F 0.488)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.284] [G acc: 0.156]\n",
      "2972 [D loss: (0.530)(R 0.495, F 0.565)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.289] [G acc: 0.156]\n",
      "2973 [D loss: (0.526)(R 0.556, F 0.496)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.423] [G acc: 0.062]\n",
      "2974 [D loss: (0.576)(R 0.573, F 0.578)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.277] [G acc: 0.125]\n",
      "2975 [D loss: (0.633)(R 0.579, F 0.687)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.271] [G acc: 0.172]\n",
      "2976 [D loss: (0.549)(R 0.577, F 0.521)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.252] [G acc: 0.156]\n",
      "2977 [D loss: (0.516)(R 0.539, F 0.493)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.167] [G acc: 0.188]\n",
      "2978 [D loss: (0.538)(R 0.580, F 0.497)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.232] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2979 [D loss: (0.552)(R 0.546, F 0.558)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.191] [G acc: 0.141]\n",
      "2980 [D loss: (0.582)(R 0.577, F 0.587)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.099] [G acc: 0.156]\n",
      "2981 [D loss: (0.570)(R 0.574, F 0.566)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.244] [G acc: 0.141]\n",
      "2982 [D loss: (0.569)(R 0.510, F 0.628)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.359] [G acc: 0.125]\n",
      "2983 [D loss: (0.537)(R 0.566, F 0.507)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.291] [G acc: 0.094]\n",
      "2984 [D loss: (0.545)(R 0.543, F 0.547)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.108] [G acc: 0.203]\n",
      "2985 [D loss: (0.573)(R 0.553, F 0.593)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.376] [G acc: 0.062]\n",
      "2986 [D loss: (0.571)(R 0.574, F 0.569)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.376] [G acc: 0.109]\n",
      "2987 [D loss: (0.546)(R 0.524, F 0.567)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.231] [G acc: 0.172]\n",
      "2988 [D loss: (0.520)(R 0.474, F 0.565)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.393] [G acc: 0.062]\n",
      "2989 [D loss: (0.494)(R 0.571, F 0.417)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.439] [G acc: 0.062]\n",
      "2990 [D loss: (0.460)(R 0.470, F 0.450)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.460] [G acc: 0.125]\n",
      "2991 [D loss: (0.549)(R 0.606, F 0.491)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.365] [G acc: 0.109]\n",
      "2992 [D loss: (0.472)(R 0.533, F 0.411)] [D acc: (0.805)(0.688, 0.922)] [G loss: 1.327] [G acc: 0.109]\n",
      "2993 [D loss: (0.530)(R 0.509, F 0.550)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.391] [G acc: 0.156]\n",
      "2994 [D loss: (0.621)(R 0.716, F 0.526)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.321] [G acc: 0.078]\n",
      "2995 [D loss: (0.468)(R 0.473, F 0.462)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.291] [G acc: 0.156]\n",
      "2996 [D loss: (0.559)(R 0.449, F 0.669)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.390] [G acc: 0.109]\n",
      "2997 [D loss: (0.597)(R 0.642, F 0.552)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.506] [G acc: 0.062]\n",
      "2998 [D loss: (0.634)(R 0.612, F 0.657)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.248] [G acc: 0.094]\n",
      "2999 [D loss: (0.533)(R 0.548, F 0.518)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.442] [G acc: 0.047]\n",
      "3000 [D loss: (0.531)(R 0.529, F 0.533)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.240] [G acc: 0.219]\n",
      "3001 [D loss: (0.572)(R 0.564, F 0.579)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.244] [G acc: 0.172]\n",
      "3002 [D loss: (0.523)(R 0.603, F 0.443)] [D acc: (0.773)(0.641, 0.906)] [G loss: 1.437] [G acc: 0.109]\n",
      "3003 [D loss: (0.520)(R 0.487, F 0.553)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.358] [G acc: 0.141]\n",
      "3004 [D loss: (0.495)(R 0.508, F 0.482)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.455] [G acc: 0.109]\n",
      "3005 [D loss: (0.573)(R 0.611, F 0.536)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.317] [G acc: 0.141]\n",
      "3006 [D loss: (0.534)(R 0.618, F 0.451)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.211] [G acc: 0.094]\n",
      "3007 [D loss: (0.547)(R 0.535, F 0.560)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.344] [G acc: 0.125]\n",
      "3008 [D loss: (0.489)(R 0.545, F 0.433)] [D acc: (0.797)(0.672, 0.922)] [G loss: 1.340] [G acc: 0.078]\n",
      "3009 [D loss: (0.637)(R 0.566, F 0.708)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.290] [G acc: 0.156]\n",
      "3010 [D loss: (0.612)(R 0.715, F 0.510)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.154] [G acc: 0.203]\n",
      "3011 [D loss: (0.575)(R 0.574, F 0.575)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.196] [G acc: 0.188]\n",
      "3012 [D loss: (0.564)(R 0.504, F 0.624)] [D acc: (0.672)(0.719, 0.625)] [G loss: 1.466] [G acc: 0.062]\n",
      "3013 [D loss: (0.674)(R 0.772, F 0.576)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.241] [G acc: 0.125]\n",
      "3014 [D loss: (0.552)(R 0.575, F 0.530)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.225] [G acc: 0.188]\n",
      "3015 [D loss: (0.540)(R 0.489, F 0.591)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.122] [G acc: 0.188]\n",
      "3016 [D loss: (0.458)(R 0.407, F 0.508)] [D acc: (0.781)(0.797, 0.766)] [G loss: 1.449] [G acc: 0.141]\n",
      "3017 [D loss: (0.521)(R 0.501, F 0.542)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.223] [G acc: 0.172]\n",
      "3018 [D loss: (0.604)(R 0.705, F 0.503)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.355] [G acc: 0.047]\n",
      "3019 [D loss: (0.600)(R 0.643, F 0.557)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.169] [G acc: 0.094]\n",
      "3020 [D loss: (0.504)(R 0.471, F 0.537)] [D acc: (0.766)(0.781, 0.750)] [G loss: 1.286] [G acc: 0.172]\n",
      "3021 [D loss: (0.462)(R 0.435, F 0.489)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.186] [G acc: 0.156]\n",
      "3022 [D loss: (0.535)(R 0.533, F 0.538)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.392] [G acc: 0.094]\n",
      "3023 [D loss: (0.569)(R 0.555, F 0.584)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.277] [G acc: 0.141]\n",
      "3024 [D loss: (0.559)(R 0.652, F 0.467)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.337] [G acc: 0.141]\n",
      "3025 [D loss: (0.549)(R 0.478, F 0.620)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.410] [G acc: 0.109]\n",
      "3026 [D loss: (0.533)(R 0.516, F 0.549)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.390] [G acc: 0.094]\n",
      "3027 [D loss: (0.601)(R 0.574, F 0.628)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.434] [G acc: 0.125]\n",
      "3028 [D loss: (0.557)(R 0.624, F 0.491)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.246] [G acc: 0.141]\n",
      "3029 [D loss: (0.545)(R 0.481, F 0.610)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.331] [G acc: 0.109]\n",
      "3030 [D loss: (0.489)(R 0.450, F 0.527)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.365] [G acc: 0.031]\n",
      "3031 [D loss: (0.514)(R 0.577, F 0.451)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.400] [G acc: 0.156]\n",
      "3032 [D loss: (0.598)(R 0.526, F 0.671)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.186] [G acc: 0.156]\n",
      "3033 [D loss: (0.501)(R 0.551, F 0.451)] [D acc: (0.797)(0.719, 0.875)] [G loss: 1.342] [G acc: 0.125]\n",
      "3034 [D loss: (0.527)(R 0.512, F 0.541)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.299] [G acc: 0.125]\n",
      "3035 [D loss: (0.582)(R 0.552, F 0.611)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.358] [G acc: 0.172]\n",
      "3036 [D loss: (0.597)(R 0.589, F 0.606)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.395] [G acc: 0.078]\n",
      "3037 [D loss: (0.575)(R 0.679, F 0.471)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.342] [G acc: 0.078]\n",
      "3038 [D loss: (0.524)(R 0.503, F 0.544)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.346] [G acc: 0.156]\n",
      "3039 [D loss: (0.602)(R 0.590, F 0.615)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.276] [G acc: 0.141]\n",
      "3040 [D loss: (0.552)(R 0.602, F 0.502)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.180] [G acc: 0.156]\n",
      "3041 [D loss: (0.554)(R 0.603, F 0.505)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.480] [G acc: 0.125]\n",
      "3042 [D loss: (0.651)(R 0.630, F 0.671)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.338] [G acc: 0.125]\n",
      "3043 [D loss: (0.540)(R 0.577, F 0.504)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.354] [G acc: 0.094]\n",
      "3044 [D loss: (0.583)(R 0.615, F 0.551)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.341] [G acc: 0.141]\n",
      "3045 [D loss: (0.555)(R 0.521, F 0.590)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.308] [G acc: 0.109]\n",
      "3046 [D loss: (0.554)(R 0.587, F 0.520)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.393] [G acc: 0.141]\n",
      "3047 [D loss: (0.558)(R 0.561, F 0.554)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.543] [G acc: 0.047]\n",
      "3048 [D loss: (0.572)(R 0.585, F 0.560)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.273] [G acc: 0.141]\n",
      "3049 [D loss: (0.562)(R 0.592, F 0.532)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.270] [G acc: 0.141]\n",
      "3050 [D loss: (0.554)(R 0.605, F 0.502)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.289] [G acc: 0.078]\n",
      "3051 [D loss: (0.495)(R 0.526, F 0.465)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.283] [G acc: 0.234]\n",
      "3052 [D loss: (0.542)(R 0.556, F 0.528)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.412] [G acc: 0.125]\n",
      "3053 [D loss: (0.597)(R 0.581, F 0.612)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.302] [G acc: 0.125]\n",
      "3054 [D loss: (0.592)(R 0.593, F 0.591)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.414] [G acc: 0.062]\n",
      "3055 [D loss: (0.546)(R 0.489, F 0.603)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.278] [G acc: 0.109]\n",
      "3056 [D loss: (0.498)(R 0.566, F 0.430)] [D acc: (0.789)(0.656, 0.922)] [G loss: 1.257] [G acc: 0.141]\n",
      "3057 [D loss: (0.560)(R 0.565, F 0.556)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.330] [G acc: 0.078]\n",
      "3058 [D loss: (0.447)(R 0.472, F 0.422)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.332] [G acc: 0.109]\n",
      "3059 [D loss: (0.626)(R 0.563, F 0.688)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.492] [G acc: 0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060 [D loss: (0.656)(R 0.766, F 0.546)] [D acc: (0.633)(0.484, 0.781)] [G loss: 1.193] [G acc: 0.125]\n",
      "3061 [D loss: (0.637)(R 0.591, F 0.682)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.275] [G acc: 0.172]\n",
      "3062 [D loss: (0.622)(R 0.630, F 0.613)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.249] [G acc: 0.141]\n",
      "3063 [D loss: (0.551)(R 0.636, F 0.467)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.264] [G acc: 0.125]\n",
      "3064 [D loss: (0.646)(R 0.702, F 0.590)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.101] [G acc: 0.172]\n",
      "3065 [D loss: (0.531)(R 0.515, F 0.547)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.318] [G acc: 0.109]\n",
      "3066 [D loss: (0.525)(R 0.437, F 0.612)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.220] [G acc: 0.156]\n",
      "3067 [D loss: (0.541)(R 0.570, F 0.512)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.234] [G acc: 0.141]\n",
      "3068 [D loss: (0.573)(R 0.654, F 0.493)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.243] [G acc: 0.172]\n",
      "3069 [D loss: (0.683)(R 0.691, F 0.676)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.280] [G acc: 0.094]\n",
      "3070 [D loss: (0.525)(R 0.547, F 0.503)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.243] [G acc: 0.094]\n",
      "3071 [D loss: (0.584)(R 0.540, F 0.627)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.325] [G acc: 0.156]\n",
      "3072 [D loss: (0.520)(R 0.473, F 0.566)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.276] [G acc: 0.156]\n",
      "3073 [D loss: (0.594)(R 0.600, F 0.588)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.231] [G acc: 0.125]\n",
      "3074 [D loss: (0.538)(R 0.580, F 0.496)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.104] [G acc: 0.219]\n",
      "3075 [D loss: (0.504)(R 0.472, F 0.536)] [D acc: (0.734)(0.766, 0.703)] [G loss: 1.291] [G acc: 0.125]\n",
      "3076 [D loss: (0.590)(R 0.521, F 0.659)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.282] [G acc: 0.109]\n",
      "3077 [D loss: (0.562)(R 0.516, F 0.608)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.415] [G acc: 0.125]\n",
      "3078 [D loss: (0.623)(R 0.681, F 0.566)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.472] [G acc: 0.109]\n",
      "3079 [D loss: (0.574)(R 0.612, F 0.537)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.379] [G acc: 0.141]\n",
      "3080 [D loss: (0.703)(R 0.725, F 0.682)] [D acc: (0.594)(0.547, 0.641)] [G loss: 1.429] [G acc: 0.078]\n",
      "3081 [D loss: (0.651)(R 0.703, F 0.599)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.233] [G acc: 0.125]\n",
      "3082 [D loss: (0.522)(R 0.572, F 0.472)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.261] [G acc: 0.109]\n",
      "3083 [D loss: (0.567)(R 0.588, F 0.546)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.185] [G acc: 0.109]\n",
      "3084 [D loss: (0.536)(R 0.572, F 0.501)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.179] [G acc: 0.172]\n",
      "3085 [D loss: (0.562)(R 0.554, F 0.571)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.094] [G acc: 0.203]\n",
      "3086 [D loss: (0.561)(R 0.567, F 0.554)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.185] [G acc: 0.156]\n",
      "3087 [D loss: (0.555)(R 0.601, F 0.509)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.282] [G acc: 0.156]\n",
      "3088 [D loss: (0.575)(R 0.513, F 0.638)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.363] [G acc: 0.047]\n",
      "3089 [D loss: (0.510)(R 0.609, F 0.411)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.277] [G acc: 0.203]\n",
      "3090 [D loss: (0.588)(R 0.554, F 0.621)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.267] [G acc: 0.125]\n",
      "3091 [D loss: (0.547)(R 0.511, F 0.584)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.257] [G acc: 0.172]\n",
      "3092 [D loss: (0.607)(R 0.616, F 0.599)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.349] [G acc: 0.125]\n",
      "3093 [D loss: (0.605)(R 0.665, F 0.546)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.246] [G acc: 0.062]\n",
      "3094 [D loss: (0.597)(R 0.574, F 0.619)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.246] [G acc: 0.125]\n",
      "3095 [D loss: (0.598)(R 0.704, F 0.492)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.153] [G acc: 0.156]\n",
      "3096 [D loss: (0.576)(R 0.679, F 0.472)] [D acc: (0.672)(0.500, 0.844)] [G loss: 1.257] [G acc: 0.125]\n",
      "3097 [D loss: (0.578)(R 0.547, F 0.609)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.205] [G acc: 0.109]\n",
      "3098 [D loss: (0.478)(R 0.448, F 0.507)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.159] [G acc: 0.203]\n",
      "3099 [D loss: (0.548)(R 0.554, F 0.542)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.311] [G acc: 0.125]\n",
      "3100 [D loss: (0.592)(R 0.576, F 0.607)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.216] [G acc: 0.156]\n",
      "3101 [D loss: (0.610)(R 0.584, F 0.635)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.170] [G acc: 0.172]\n",
      "3102 [D loss: (0.616)(R 0.655, F 0.576)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.158] [G acc: 0.156]\n",
      "3103 [D loss: (0.581)(R 0.578, F 0.584)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.233] [G acc: 0.109]\n",
      "3104 [D loss: (0.547)(R 0.588, F 0.506)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.286] [G acc: 0.125]\n",
      "3105 [D loss: (0.588)(R 0.532, F 0.643)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.298] [G acc: 0.172]\n",
      "3106 [D loss: (0.607)(R 0.690, F 0.524)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.199] [G acc: 0.078]\n",
      "3107 [D loss: (0.559)(R 0.550, F 0.568)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.384] [G acc: 0.047]\n",
      "3108 [D loss: (0.564)(R 0.533, F 0.594)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.177] [G acc: 0.156]\n",
      "3109 [D loss: (0.534)(R 0.559, F 0.510)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.270] [G acc: 0.094]\n",
      "3110 [D loss: (0.487)(R 0.346, F 0.628)] [D acc: (0.797)(0.844, 0.750)] [G loss: 1.524] [G acc: 0.094]\n",
      "3111 [D loss: (0.554)(R 0.529, F 0.578)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.494] [G acc: 0.047]\n",
      "3112 [D loss: (0.507)(R 0.464, F 0.549)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.365] [G acc: 0.109]\n",
      "3113 [D loss: (0.580)(R 0.602, F 0.559)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.612] [G acc: 0.047]\n",
      "3114 [D loss: (0.623)(R 0.646, F 0.600)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.358] [G acc: 0.078]\n",
      "3115 [D loss: (0.557)(R 0.640, F 0.474)] [D acc: (0.742)(0.594, 0.891)] [G loss: 1.395] [G acc: 0.078]\n",
      "3116 [D loss: (0.612)(R 0.623, F 0.600)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.290] [G acc: 0.109]\n",
      "3117 [D loss: (0.641)(R 0.714, F 0.569)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.238] [G acc: 0.141]\n",
      "3118 [D loss: (0.503)(R 0.463, F 0.543)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.270] [G acc: 0.109]\n",
      "3119 [D loss: (0.563)(R 0.609, F 0.517)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.374] [G acc: 0.062]\n",
      "3120 [D loss: (0.588)(R 0.482, F 0.694)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.483] [G acc: 0.094]\n",
      "3121 [D loss: (0.610)(R 0.687, F 0.533)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.395] [G acc: 0.109]\n",
      "3122 [D loss: (0.522)(R 0.564, F 0.480)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.245] [G acc: 0.125]\n",
      "3123 [D loss: (0.554)(R 0.603, F 0.504)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.282] [G acc: 0.125]\n",
      "3124 [D loss: (0.543)(R 0.425, F 0.660)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.315] [G acc: 0.109]\n",
      "3125 [D loss: (0.594)(R 0.635, F 0.553)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.301] [G acc: 0.062]\n",
      "3126 [D loss: (0.633)(R 0.593, F 0.673)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.215] [G acc: 0.172]\n",
      "3127 [D loss: (0.622)(R 0.667, F 0.576)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.216] [G acc: 0.141]\n",
      "3128 [D loss: (0.569)(R 0.666, F 0.472)] [D acc: (0.750)(0.578, 0.922)] [G loss: 1.287] [G acc: 0.062]\n",
      "3129 [D loss: (0.508)(R 0.517, F 0.500)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.311] [G acc: 0.125]\n",
      "3130 [D loss: (0.546)(R 0.614, F 0.478)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.228] [G acc: 0.203]\n",
      "3131 [D loss: (0.562)(R 0.500, F 0.625)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.437] [G acc: 0.062]\n",
      "3132 [D loss: (0.561)(R 0.631, F 0.491)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.236] [G acc: 0.172]\n",
      "3133 [D loss: (0.540)(R 0.521, F 0.560)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.243] [G acc: 0.125]\n",
      "3134 [D loss: (0.606)(R 0.638, F 0.574)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.106] [G acc: 0.172]\n",
      "3135 [D loss: (0.522)(R 0.539, F 0.505)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.247] [G acc: 0.141]\n",
      "3136 [D loss: (0.574)(R 0.559, F 0.590)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.110] [G acc: 0.172]\n",
      "3137 [D loss: (0.563)(R 0.565, F 0.562)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.273] [G acc: 0.125]\n",
      "3138 [D loss: (0.626)(R 0.686, F 0.566)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.195] [G acc: 0.141]\n",
      "3139 [D loss: (0.541)(R 0.543, F 0.540)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.318] [G acc: 0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3140 [D loss: (0.593)(R 0.650, F 0.536)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.399] [G acc: 0.062]\n",
      "3141 [D loss: (0.668)(R 0.706, F 0.631)] [D acc: (0.617)(0.422, 0.812)] [G loss: 1.199] [G acc: 0.141]\n",
      "3142 [D loss: (0.599)(R 0.673, F 0.525)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.128] [G acc: 0.172]\n",
      "3143 [D loss: (0.564)(R 0.544, F 0.585)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.186] [G acc: 0.141]\n",
      "3144 [D loss: (0.500)(R 0.540, F 0.460)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.237] [G acc: 0.203]\n",
      "3145 [D loss: (0.535)(R 0.553, F 0.516)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.222] [G acc: 0.156]\n",
      "3146 [D loss: (0.546)(R 0.429, F 0.664)] [D acc: (0.789)(0.812, 0.766)] [G loss: 1.286] [G acc: 0.141]\n",
      "3147 [D loss: (0.539)(R 0.627, F 0.451)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.292] [G acc: 0.188]\n",
      "3148 [D loss: (0.559)(R 0.541, F 0.576)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.353] [G acc: 0.125]\n",
      "3149 [D loss: (0.574)(R 0.560, F 0.589)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.349] [G acc: 0.125]\n",
      "3150 [D loss: (0.530)(R 0.512, F 0.547)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.307] [G acc: 0.094]\n",
      "3151 [D loss: (0.634)(R 0.620, F 0.648)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.460] [G acc: 0.047]\n",
      "3152 [D loss: (0.536)(R 0.548, F 0.524)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.334] [G acc: 0.125]\n",
      "3153 [D loss: (0.566)(R 0.643, F 0.489)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.411] [G acc: 0.109]\n",
      "3154 [D loss: (0.557)(R 0.543, F 0.570)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.336] [G acc: 0.172]\n",
      "3155 [D loss: (0.693)(R 0.666, F 0.721)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.389] [G acc: 0.062]\n",
      "3156 [D loss: (0.584)(R 0.716, F 0.451)] [D acc: (0.711)(0.516, 0.906)] [G loss: 1.199] [G acc: 0.141]\n",
      "3157 [D loss: (0.622)(R 0.634, F 0.610)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.335] [G acc: 0.062]\n",
      "3158 [D loss: (0.560)(R 0.627, F 0.493)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.248] [G acc: 0.141]\n",
      "3159 [D loss: (0.440)(R 0.413, F 0.467)] [D acc: (0.828)(0.828, 0.828)] [G loss: 1.234] [G acc: 0.141]\n",
      "3160 [D loss: (0.555)(R 0.518, F 0.592)] [D acc: (0.789)(0.797, 0.781)] [G loss: 1.273] [G acc: 0.078]\n",
      "3161 [D loss: (0.588)(R 0.555, F 0.621)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.320] [G acc: 0.125]\n",
      "3162 [D loss: (0.531)(R 0.491, F 0.571)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.177] [G acc: 0.219]\n",
      "3163 [D loss: (0.588)(R 0.624, F 0.552)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.242] [G acc: 0.062]\n",
      "3164 [D loss: (0.685)(R 0.643, F 0.727)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.107] [G acc: 0.156]\n",
      "3165 [D loss: (0.578)(R 0.668, F 0.488)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.254] [G acc: 0.078]\n",
      "3166 [D loss: (0.532)(R 0.596, F 0.468)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.319] [G acc: 0.047]\n",
      "3167 [D loss: (0.629)(R 0.537, F 0.721)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.253] [G acc: 0.094]\n",
      "3168 [D loss: (0.584)(R 0.656, F 0.511)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.328] [G acc: 0.125]\n",
      "3169 [D loss: (0.588)(R 0.622, F 0.553)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.173] [G acc: 0.156]\n",
      "3170 [D loss: (0.594)(R 0.558, F 0.629)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.350] [G acc: 0.047]\n",
      "3171 [D loss: (0.518)(R 0.565, F 0.472)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.212] [G acc: 0.094]\n",
      "3172 [D loss: (0.509)(R 0.557, F 0.461)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.229] [G acc: 0.188]\n",
      "3173 [D loss: (0.560)(R 0.525, F 0.596)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.351] [G acc: 0.078]\n",
      "3174 [D loss: (0.548)(R 0.537, F 0.559)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.237] [G acc: 0.109]\n",
      "3175 [D loss: (0.578)(R 0.578, F 0.579)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.269] [G acc: 0.172]\n",
      "3176 [D loss: (0.580)(R 0.688, F 0.472)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.265] [G acc: 0.109]\n",
      "3177 [D loss: (0.547)(R 0.481, F 0.614)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.347] [G acc: 0.062]\n",
      "3178 [D loss: (0.558)(R 0.641, F 0.475)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.141] [G acc: 0.078]\n",
      "3179 [D loss: (0.536)(R 0.487, F 0.586)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.352] [G acc: 0.062]\n",
      "3180 [D loss: (0.537)(R 0.483, F 0.591)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.374] [G acc: 0.062]\n",
      "3181 [D loss: (0.577)(R 0.655, F 0.499)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.328] [G acc: 0.094]\n",
      "3182 [D loss: (0.592)(R 0.572, F 0.612)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.327] [G acc: 0.078]\n",
      "3183 [D loss: (0.472)(R 0.530, F 0.414)] [D acc: (0.805)(0.688, 0.922)] [G loss: 1.317] [G acc: 0.094]\n",
      "3184 [D loss: (0.526)(R 0.543, F 0.509)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.435] [G acc: 0.125]\n",
      "3185 [D loss: (0.640)(R 0.559, F 0.721)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.273] [G acc: 0.094]\n",
      "3186 [D loss: (0.608)(R 0.608, F 0.608)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.194] [G acc: 0.156]\n",
      "3187 [D loss: (0.688)(R 0.660, F 0.717)] [D acc: (0.594)(0.531, 0.656)] [G loss: 1.303] [G acc: 0.078]\n",
      "3188 [D loss: (0.561)(R 0.573, F 0.549)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.280] [G acc: 0.094]\n",
      "3189 [D loss: (0.566)(R 0.611, F 0.522)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.227] [G acc: 0.094]\n",
      "3190 [D loss: (0.546)(R 0.623, F 0.469)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.149] [G acc: 0.094]\n",
      "3191 [D loss: (0.535)(R 0.552, F 0.517)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.265] [G acc: 0.109]\n",
      "3192 [D loss: (0.612)(R 0.637, F 0.588)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.355] [G acc: 0.078]\n",
      "3193 [D loss: (0.569)(R 0.604, F 0.534)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.376] [G acc: 0.047]\n",
      "3194 [D loss: (0.546)(R 0.584, F 0.508)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.188] [G acc: 0.141]\n",
      "3195 [D loss: (0.535)(R 0.466, F 0.605)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.398] [G acc: 0.109]\n",
      "3196 [D loss: (0.578)(R 0.670, F 0.486)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.184] [G acc: 0.188]\n",
      "3197 [D loss: (0.582)(R 0.550, F 0.614)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.106] [G acc: 0.203]\n",
      "3198 [D loss: (0.591)(R 0.540, F 0.642)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.265] [G acc: 0.156]\n",
      "3199 [D loss: (0.528)(R 0.580, F 0.477)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.161] [G acc: 0.125]\n",
      "3200 [D loss: (0.502)(R 0.537, F 0.466)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.094] [G acc: 0.219]\n",
      "3201 [D loss: (0.564)(R 0.576, F 0.552)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.160] [G acc: 0.203]\n",
      "3202 [D loss: (0.614)(R 0.586, F 0.643)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.237] [G acc: 0.172]\n",
      "3203 [D loss: (0.608)(R 0.700, F 0.516)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.173] [G acc: 0.156]\n",
      "3204 [D loss: (0.573)(R 0.594, F 0.553)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.325] [G acc: 0.141]\n",
      "3205 [D loss: (0.597)(R 0.502, F 0.692)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.192] [G acc: 0.078]\n",
      "3206 [D loss: (0.564)(R 0.534, F 0.593)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.288] [G acc: 0.094]\n",
      "3207 [D loss: (0.556)(R 0.561, F 0.552)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.255] [G acc: 0.141]\n",
      "3208 [D loss: (0.546)(R 0.567, F 0.524)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.212] [G acc: 0.219]\n",
      "3209 [D loss: (0.456)(R 0.394, F 0.518)] [D acc: (0.781)(0.781, 0.781)] [G loss: 1.231] [G acc: 0.156]\n",
      "3210 [D loss: (0.549)(R 0.461, F 0.637)] [D acc: (0.742)(0.766, 0.719)] [G loss: 1.243] [G acc: 0.172]\n",
      "3211 [D loss: (0.587)(R 0.550, F 0.623)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.456] [G acc: 0.047]\n",
      "3212 [D loss: (0.599)(R 0.702, F 0.497)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.308] [G acc: 0.172]\n",
      "3213 [D loss: (0.504)(R 0.501, F 0.507)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.271] [G acc: 0.172]\n",
      "3214 [D loss: (0.620)(R 0.689, F 0.551)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.297] [G acc: 0.172]\n",
      "3215 [D loss: (0.577)(R 0.473, F 0.682)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.292] [G acc: 0.172]\n",
      "3216 [D loss: (0.540)(R 0.551, F 0.530)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.198] [G acc: 0.156]\n",
      "3217 [D loss: (0.533)(R 0.532, F 0.533)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.291] [G acc: 0.188]\n",
      "3218 [D loss: (0.562)(R 0.560, F 0.563)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.199] [G acc: 0.188]\n",
      "3219 [D loss: (0.450)(R 0.445, F 0.455)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.322] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3220 [D loss: (0.592)(R 0.598, F 0.585)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.201] [G acc: 0.141]\n",
      "3221 [D loss: (0.530)(R 0.539, F 0.520)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.228] [G acc: 0.203]\n",
      "3222 [D loss: (0.517)(R 0.505, F 0.529)] [D acc: (0.781)(0.766, 0.797)] [G loss: 1.296] [G acc: 0.156]\n",
      "3223 [D loss: (0.552)(R 0.480, F 0.624)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.363] [G acc: 0.109]\n",
      "3224 [D loss: (0.562)(R 0.628, F 0.495)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.229] [G acc: 0.141]\n",
      "3225 [D loss: (0.548)(R 0.524, F 0.571)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.373] [G acc: 0.109]\n",
      "3226 [D loss: (0.601)(R 0.627, F 0.575)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.253] [G acc: 0.141]\n",
      "3227 [D loss: (0.621)(R 0.615, F 0.628)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.202] [G acc: 0.078]\n",
      "3228 [D loss: (0.569)(R 0.585, F 0.554)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.145] [G acc: 0.156]\n",
      "3229 [D loss: (0.592)(R 0.613, F 0.571)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.274] [G acc: 0.141]\n",
      "3230 [D loss: (0.611)(R 0.604, F 0.617)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.183] [G acc: 0.109]\n",
      "3231 [D loss: (0.633)(R 0.647, F 0.618)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.217] [G acc: 0.141]\n",
      "3232 [D loss: (0.616)(R 0.636, F 0.596)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.146] [G acc: 0.078]\n",
      "3233 [D loss: (0.548)(R 0.600, F 0.496)] [D acc: (0.719)(0.562, 0.875)] [G loss: 1.170] [G acc: 0.109]\n",
      "3234 [D loss: (0.546)(R 0.532, F 0.561)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.129] [G acc: 0.062]\n",
      "3235 [D loss: (0.541)(R 0.528, F 0.554)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.264] [G acc: 0.141]\n",
      "3236 [D loss: (0.559)(R 0.599, F 0.518)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.119] [G acc: 0.125]\n",
      "3237 [D loss: (0.578)(R 0.539, F 0.617)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.134] [G acc: 0.125]\n",
      "3238 [D loss: (0.511)(R 0.504, F 0.519)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.234] [G acc: 0.094]\n",
      "3239 [D loss: (0.564)(R 0.561, F 0.567)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.167] [G acc: 0.125]\n",
      "3240 [D loss: (0.620)(R 0.623, F 0.617)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.317] [G acc: 0.031]\n",
      "3241 [D loss: (0.531)(R 0.482, F 0.580)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.296] [G acc: 0.094]\n",
      "3242 [D loss: (0.557)(R 0.593, F 0.520)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.318] [G acc: 0.078]\n",
      "3243 [D loss: (0.520)(R 0.546, F 0.494)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.347] [G acc: 0.109]\n",
      "3244 [D loss: (0.531)(R 0.535, F 0.528)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.391] [G acc: 0.062]\n",
      "3245 [D loss: (0.448)(R 0.452, F 0.445)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.337] [G acc: 0.141]\n",
      "3246 [D loss: (0.638)(R 0.596, F 0.680)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.282] [G acc: 0.109]\n",
      "3247 [D loss: (0.648)(R 0.679, F 0.617)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.292] [G acc: 0.188]\n",
      "3248 [D loss: (0.541)(R 0.551, F 0.530)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.180] [G acc: 0.141]\n",
      "3249 [D loss: (0.614)(R 0.604, F 0.624)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.111] [G acc: 0.188]\n",
      "3250 [D loss: (0.495)(R 0.538, F 0.451)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.179] [G acc: 0.141]\n",
      "3251 [D loss: (0.538)(R 0.471, F 0.605)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.372] [G acc: 0.109]\n",
      "3252 [D loss: (0.546)(R 0.606, F 0.485)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.108] [G acc: 0.188]\n",
      "3253 [D loss: (0.545)(R 0.576, F 0.514)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.129] [G acc: 0.203]\n",
      "3254 [D loss: (0.625)(R 0.647, F 0.603)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.210] [G acc: 0.125]\n",
      "3255 [D loss: (0.575)(R 0.507, F 0.643)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.191] [G acc: 0.125]\n",
      "3256 [D loss: (0.546)(R 0.585, F 0.506)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.259] [G acc: 0.062]\n",
      "3257 [D loss: (0.459)(R 0.464, F 0.454)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.236] [G acc: 0.188]\n",
      "3258 [D loss: (0.565)(R 0.557, F 0.574)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.294] [G acc: 0.094]\n",
      "3259 [D loss: (0.473)(R 0.444, F 0.502)] [D acc: (0.805)(0.766, 0.844)] [G loss: 1.321] [G acc: 0.125]\n",
      "3260 [D loss: (0.527)(R 0.535, F 0.518)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.351] [G acc: 0.078]\n",
      "3261 [D loss: (0.606)(R 0.702, F 0.510)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.108] [G acc: 0.141]\n",
      "3262 [D loss: (0.583)(R 0.601, F 0.564)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.216] [G acc: 0.156]\n",
      "3263 [D loss: (0.603)(R 0.553, F 0.652)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.191] [G acc: 0.094]\n",
      "3264 [D loss: (0.594)(R 0.603, F 0.585)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.215] [G acc: 0.156]\n",
      "3265 [D loss: (0.596)(R 0.669, F 0.523)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.214] [G acc: 0.141]\n",
      "3266 [D loss: (0.598)(R 0.673, F 0.523)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.275] [G acc: 0.062]\n",
      "3267 [D loss: (0.581)(R 0.604, F 0.558)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.290] [G acc: 0.062]\n",
      "3268 [D loss: (0.659)(R 0.746, F 0.572)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.307] [G acc: 0.094]\n",
      "3269 [D loss: (0.566)(R 0.573, F 0.560)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.416] [G acc: 0.047]\n",
      "3270 [D loss: (0.581)(R 0.628, F 0.534)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.176] [G acc: 0.172]\n",
      "3271 [D loss: (0.530)(R 0.535, F 0.526)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.154] [G acc: 0.188]\n",
      "3272 [D loss: (0.584)(R 0.559, F 0.609)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.280] [G acc: 0.094]\n",
      "3273 [D loss: (0.692)(R 0.701, F 0.684)] [D acc: (0.602)(0.500, 0.703)] [G loss: 1.166] [G acc: 0.109]\n",
      "3274 [D loss: (0.614)(R 0.672, F 0.557)] [D acc: (0.633)(0.484, 0.781)] [G loss: 1.158] [G acc: 0.094]\n",
      "3275 [D loss: (0.608)(R 0.673, F 0.543)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.114] [G acc: 0.156]\n",
      "3276 [D loss: (0.609)(R 0.581, F 0.637)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.121] [G acc: 0.156]\n",
      "3277 [D loss: (0.553)(R 0.509, F 0.596)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.289] [G acc: 0.094]\n",
      "3278 [D loss: (0.590)(R 0.556, F 0.624)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.230] [G acc: 0.094]\n",
      "3279 [D loss: (0.655)(R 0.719, F 0.591)] [D acc: (0.633)(0.453, 0.812)] [G loss: 1.103] [G acc: 0.094]\n",
      "3280 [D loss: (0.529)(R 0.538, F 0.521)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.019] [G acc: 0.156]\n",
      "3281 [D loss: (0.633)(R 0.601, F 0.665)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.049] [G acc: 0.141]\n",
      "3282 [D loss: (0.532)(R 0.571, F 0.493)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.166] [G acc: 0.172]\n",
      "3283 [D loss: (0.558)(R 0.510, F 0.605)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.052] [G acc: 0.234]\n",
      "3284 [D loss: (0.554)(R 0.452, F 0.656)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.325] [G acc: 0.109]\n",
      "3285 [D loss: (0.578)(R 0.619, F 0.536)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.116] [G acc: 0.109]\n",
      "3286 [D loss: (0.523)(R 0.552, F 0.493)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.176] [G acc: 0.156]\n",
      "3287 [D loss: (0.599)(R 0.515, F 0.683)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.225] [G acc: 0.094]\n",
      "3288 [D loss: (0.583)(R 0.638, F 0.527)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.200] [G acc: 0.125]\n",
      "3289 [D loss: (0.628)(R 0.586, F 0.669)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.140] [G acc: 0.094]\n",
      "3290 [D loss: (0.612)(R 0.671, F 0.553)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.384] [G acc: 0.062]\n",
      "3291 [D loss: (0.505)(R 0.538, F 0.472)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.296] [G acc: 0.094]\n",
      "3292 [D loss: (0.648)(R 0.716, F 0.579)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.192] [G acc: 0.094]\n",
      "3293 [D loss: (0.607)(R 0.677, F 0.536)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.179] [G acc: 0.078]\n",
      "3294 [D loss: (0.607)(R 0.656, F 0.557)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.191] [G acc: 0.047]\n",
      "3295 [D loss: (0.625)(R 0.658, F 0.592)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.115] [G acc: 0.109]\n",
      "3296 [D loss: (0.556)(R 0.539, F 0.573)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.134] [G acc: 0.219]\n",
      "3297 [D loss: (0.547)(R 0.422, F 0.673)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.265] [G acc: 0.078]\n",
      "3298 [D loss: (0.543)(R 0.603, F 0.484)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.152] [G acc: 0.078]\n",
      "3299 [D loss: (0.481)(R 0.476, F 0.486)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.384] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300 [D loss: (0.587)(R 0.477, F 0.697)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.255] [G acc: 0.062]\n",
      "3301 [D loss: (0.598)(R 0.615, F 0.582)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.133] [G acc: 0.141]\n",
      "3302 [D loss: (0.567)(R 0.602, F 0.532)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.101] [G acc: 0.062]\n",
      "3303 [D loss: (0.500)(R 0.500, F 0.500)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.095] [G acc: 0.203]\n",
      "3304 [D loss: (0.643)(R 0.721, F 0.566)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.179] [G acc: 0.125]\n",
      "3305 [D loss: (0.575)(R 0.496, F 0.654)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.165] [G acc: 0.188]\n",
      "3306 [D loss: (0.546)(R 0.604, F 0.488)] [D acc: (0.773)(0.656, 0.891)] [G loss: 1.124] [G acc: 0.250]\n",
      "3307 [D loss: (0.488)(R 0.483, F 0.493)] [D acc: (0.781)(0.766, 0.797)] [G loss: 1.215] [G acc: 0.109]\n",
      "3308 [D loss: (0.540)(R 0.502, F 0.579)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.478] [G acc: 0.078]\n",
      "3309 [D loss: (0.569)(R 0.552, F 0.586)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.276] [G acc: 0.125]\n",
      "3310 [D loss: (0.512)(R 0.470, F 0.553)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.342] [G acc: 0.125]\n",
      "3311 [D loss: (0.514)(R 0.569, F 0.459)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.241] [G acc: 0.203]\n",
      "3312 [D loss: (0.573)(R 0.594, F 0.552)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.319] [G acc: 0.125]\n",
      "3313 [D loss: (0.542)(R 0.499, F 0.586)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.323] [G acc: 0.234]\n",
      "3314 [D loss: (0.470)(R 0.541, F 0.399)] [D acc: (0.805)(0.688, 0.922)] [G loss: 1.171] [G acc: 0.156]\n",
      "3315 [D loss: (0.550)(R 0.585, F 0.515)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.313] [G acc: 0.109]\n",
      "3316 [D loss: (0.530)(R 0.506, F 0.554)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.216] [G acc: 0.156]\n",
      "3317 [D loss: (0.580)(R 0.614, F 0.545)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.338] [G acc: 0.125]\n",
      "3318 [D loss: (0.588)(R 0.548, F 0.627)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.496] [G acc: 0.109]\n",
      "3319 [D loss: (0.595)(R 0.672, F 0.518)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.294] [G acc: 0.125]\n",
      "3320 [D loss: (0.495)(R 0.509, F 0.482)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.185] [G acc: 0.109]\n",
      "3321 [D loss: (0.550)(R 0.489, F 0.610)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.260] [G acc: 0.156]\n",
      "3322 [D loss: (0.510)(R 0.515, F 0.505)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.289] [G acc: 0.141]\n",
      "3323 [D loss: (0.496)(R 0.517, F 0.475)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.372] [G acc: 0.125]\n",
      "3324 [D loss: (0.607)(R 0.551, F 0.663)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.185] [G acc: 0.156]\n",
      "3325 [D loss: (0.541)(R 0.588, F 0.493)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.345] [G acc: 0.141]\n",
      "3326 [D loss: (0.553)(R 0.526, F 0.579)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.257] [G acc: 0.156]\n",
      "3327 [D loss: (0.546)(R 0.632, F 0.460)] [D acc: (0.742)(0.594, 0.891)] [G loss: 1.266] [G acc: 0.172]\n",
      "3328 [D loss: (0.637)(R 0.628, F 0.646)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.292] [G acc: 0.094]\n",
      "3329 [D loss: (0.555)(R 0.522, F 0.588)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.224] [G acc: 0.062]\n",
      "3330 [D loss: (0.569)(R 0.508, F 0.630)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.231] [G acc: 0.109]\n",
      "3331 [D loss: (0.518)(R 0.542, F 0.495)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.313] [G acc: 0.109]\n",
      "3332 [D loss: (0.537)(R 0.591, F 0.484)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.262] [G acc: 0.094]\n",
      "3333 [D loss: (0.600)(R 0.674, F 0.526)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.201] [G acc: 0.172]\n",
      "3334 [D loss: (0.517)(R 0.526, F 0.509)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.287] [G acc: 0.188]\n",
      "3335 [D loss: (0.618)(R 0.565, F 0.671)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.179] [G acc: 0.156]\n",
      "3336 [D loss: (0.613)(R 0.633, F 0.592)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.196] [G acc: 0.344]\n",
      "3337 [D loss: (0.599)(R 0.621, F 0.577)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.243] [G acc: 0.141]\n",
      "3338 [D loss: (0.511)(R 0.440, F 0.583)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.348] [G acc: 0.125]\n",
      "3339 [D loss: (0.548)(R 0.543, F 0.552)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.240] [G acc: 0.125]\n",
      "3340 [D loss: (0.552)(R 0.586, F 0.518)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.273] [G acc: 0.156]\n",
      "3341 [D loss: (0.573)(R 0.514, F 0.631)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.359] [G acc: 0.078]\n",
      "3342 [D loss: (0.488)(R 0.478, F 0.498)] [D acc: (0.797)(0.734, 0.859)] [G loss: 1.234] [G acc: 0.156]\n",
      "3343 [D loss: (0.551)(R 0.576, F 0.527)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.258] [G acc: 0.109]\n",
      "3344 [D loss: (0.541)(R 0.486, F 0.597)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.260] [G acc: 0.172]\n",
      "3345 [D loss: (0.564)(R 0.628, F 0.500)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.287] [G acc: 0.047]\n",
      "3346 [D loss: (0.519)(R 0.515, F 0.523)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.356] [G acc: 0.078]\n",
      "3347 [D loss: (0.628)(R 0.673, F 0.583)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.317] [G acc: 0.141]\n",
      "3348 [D loss: (0.544)(R 0.615, F 0.473)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.295] [G acc: 0.109]\n",
      "3349 [D loss: (0.485)(R 0.492, F 0.477)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.204] [G acc: 0.109]\n",
      "3350 [D loss: (0.555)(R 0.623, F 0.486)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.367] [G acc: 0.109]\n",
      "3351 [D loss: (0.522)(R 0.541, F 0.502)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.189] [G acc: 0.172]\n",
      "3352 [D loss: (0.559)(R 0.554, F 0.564)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.409] [G acc: 0.109]\n",
      "3353 [D loss: (0.530)(R 0.547, F 0.514)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.166] [G acc: 0.125]\n",
      "3354 [D loss: (0.575)(R 0.549, F 0.601)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.320] [G acc: 0.078]\n",
      "3355 [D loss: (0.499)(R 0.492, F 0.507)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.338] [G acc: 0.078]\n",
      "3356 [D loss: (0.563)(R 0.669, F 0.457)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.174] [G acc: 0.172]\n",
      "3357 [D loss: (0.558)(R 0.504, F 0.612)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.274] [G acc: 0.094]\n",
      "3358 [D loss: (0.574)(R 0.595, F 0.552)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.321] [G acc: 0.094]\n",
      "3359 [D loss: (0.469)(R 0.513, F 0.425)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.288] [G acc: 0.156]\n",
      "3360 [D loss: (0.630)(R 0.515, F 0.746)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.285] [G acc: 0.062]\n",
      "3361 [D loss: (0.531)(R 0.560, F 0.502)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.249] [G acc: 0.094]\n",
      "3362 [D loss: (0.519)(R 0.527, F 0.512)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.357] [G acc: 0.141]\n",
      "3363 [D loss: (0.518)(R 0.485, F 0.552)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.560] [G acc: 0.062]\n",
      "3364 [D loss: (0.605)(R 0.621, F 0.589)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.343] [G acc: 0.078]\n",
      "3365 [D loss: (0.558)(R 0.497, F 0.619)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.384] [G acc: 0.031]\n",
      "3366 [D loss: (0.616)(R 0.657, F 0.575)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.285] [G acc: 0.109]\n",
      "3367 [D loss: (0.609)(R 0.616, F 0.601)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.212] [G acc: 0.125]\n",
      "3368 [D loss: (0.515)(R 0.555, F 0.476)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.190] [G acc: 0.125]\n",
      "3369 [D loss: (0.507)(R 0.576, F 0.438)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.262] [G acc: 0.172]\n",
      "3370 [D loss: (0.642)(R 0.580, F 0.705)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.201] [G acc: 0.125]\n",
      "3371 [D loss: (0.632)(R 0.666, F 0.598)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.393] [G acc: 0.062]\n",
      "3372 [D loss: (0.554)(R 0.561, F 0.548)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.232] [G acc: 0.047]\n",
      "3373 [D loss: (0.541)(R 0.596, F 0.486)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.325] [G acc: 0.078]\n",
      "3374 [D loss: (0.505)(R 0.520, F 0.491)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.357] [G acc: 0.094]\n",
      "3375 [D loss: (0.583)(R 0.570, F 0.595)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.169] [G acc: 0.141]\n",
      "3376 [D loss: (0.539)(R 0.565, F 0.514)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.283] [G acc: 0.078]\n",
      "3377 [D loss: (0.593)(R 0.639, F 0.547)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.185] [G acc: 0.141]\n",
      "3378 [D loss: (0.482)(R 0.477, F 0.487)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.241] [G acc: 0.125]\n",
      "3379 [D loss: (0.598)(R 0.586, F 0.611)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.129] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380 [D loss: (0.570)(R 0.651, F 0.488)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.079] [G acc: 0.188]\n",
      "3381 [D loss: (0.536)(R 0.365, F 0.708)] [D acc: (0.781)(0.859, 0.703)] [G loss: 1.301] [G acc: 0.172]\n",
      "3382 [D loss: (0.574)(R 0.637, F 0.511)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.165] [G acc: 0.188]\n",
      "3383 [D loss: (0.560)(R 0.632, F 0.487)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.261] [G acc: 0.109]\n",
      "3384 [D loss: (0.434)(R 0.386, F 0.482)] [D acc: (0.820)(0.797, 0.844)] [G loss: 1.392] [G acc: 0.125]\n",
      "3385 [D loss: (0.555)(R 0.561, F 0.548)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.438] [G acc: 0.078]\n",
      "3386 [D loss: (0.554)(R 0.516, F 0.593)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.467] [G acc: 0.109]\n",
      "3387 [D loss: (0.564)(R 0.610, F 0.517)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.303] [G acc: 0.062]\n",
      "3388 [D loss: (0.591)(R 0.592, F 0.590)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.230] [G acc: 0.094]\n",
      "3389 [D loss: (0.533)(R 0.579, F 0.487)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.222] [G acc: 0.156]\n",
      "3390 [D loss: (0.545)(R 0.512, F 0.579)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.248] [G acc: 0.172]\n",
      "3391 [D loss: (0.551)(R 0.543, F 0.558)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.326] [G acc: 0.109]\n",
      "3392 [D loss: (0.586)(R 0.643, F 0.530)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.254] [G acc: 0.078]\n",
      "3393 [D loss: (0.551)(R 0.460, F 0.643)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.272] [G acc: 0.125]\n",
      "3394 [D loss: (0.582)(R 0.570, F 0.595)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.300] [G acc: 0.109]\n",
      "3395 [D loss: (0.471)(R 0.520, F 0.421)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.201] [G acc: 0.188]\n",
      "3396 [D loss: (0.551)(R 0.531, F 0.572)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.459] [G acc: 0.094]\n",
      "3397 [D loss: (0.566)(R 0.569, F 0.564)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.534] [G acc: 0.062]\n",
      "3398 [D loss: (0.535)(R 0.629, F 0.440)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.509] [G acc: 0.109]\n",
      "3399 [D loss: (0.659)(R 0.572, F 0.746)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.373] [G acc: 0.094]\n",
      "3400 [D loss: (0.553)(R 0.608, F 0.498)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.378] [G acc: 0.047]\n",
      "3401 [D loss: (0.570)(R 0.613, F 0.527)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.193] [G acc: 0.219]\n",
      "3402 [D loss: (0.549)(R 0.527, F 0.570)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.272] [G acc: 0.156]\n",
      "3403 [D loss: (0.555)(R 0.546, F 0.565)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.379] [G acc: 0.078]\n",
      "3404 [D loss: (0.576)(R 0.569, F 0.584)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.257] [G acc: 0.125]\n",
      "3405 [D loss: (0.548)(R 0.539, F 0.558)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.325] [G acc: 0.031]\n",
      "3406 [D loss: (0.591)(R 0.589, F 0.592)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.273] [G acc: 0.031]\n",
      "3407 [D loss: (0.561)(R 0.598, F 0.525)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.232] [G acc: 0.172]\n",
      "3408 [D loss: (0.586)(R 0.557, F 0.615)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.251] [G acc: 0.078]\n",
      "3409 [D loss: (0.551)(R 0.625, F 0.478)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.309] [G acc: 0.062]\n",
      "3410 [D loss: (0.540)(R 0.627, F 0.454)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.290] [G acc: 0.109]\n",
      "3411 [D loss: (0.585)(R 0.467, F 0.703)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.304] [G acc: 0.062]\n",
      "3412 [D loss: (0.533)(R 0.532, F 0.535)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.331] [G acc: 0.094]\n",
      "3413 [D loss: (0.532)(R 0.525, F 0.538)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.386] [G acc: 0.125]\n",
      "3414 [D loss: (0.594)(R 0.607, F 0.580)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.165] [G acc: 0.141]\n",
      "3415 [D loss: (0.586)(R 0.567, F 0.606)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.164] [G acc: 0.094]\n",
      "3416 [D loss: (0.533)(R 0.592, F 0.475)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.321] [G acc: 0.125]\n",
      "3417 [D loss: (0.622)(R 0.578, F 0.667)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.277] [G acc: 0.094]\n",
      "3418 [D loss: (0.611)(R 0.709, F 0.512)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.228] [G acc: 0.031]\n",
      "3419 [D loss: (0.599)(R 0.653, F 0.544)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.129] [G acc: 0.125]\n",
      "3420 [D loss: (0.496)(R 0.502, F 0.489)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.213] [G acc: 0.125]\n",
      "3421 [D loss: (0.485)(R 0.511, F 0.458)] [D acc: (0.789)(0.688, 0.891)] [G loss: 1.241] [G acc: 0.156]\n",
      "3422 [D loss: (0.549)(R 0.475, F 0.623)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.352] [G acc: 0.078]\n",
      "3423 [D loss: (0.553)(R 0.578, F 0.528)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.389] [G acc: 0.125]\n",
      "3424 [D loss: (0.528)(R 0.568, F 0.487)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.354] [G acc: 0.156]\n",
      "3425 [D loss: (0.538)(R 0.588, F 0.488)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.304] [G acc: 0.094]\n",
      "3426 [D loss: (0.483)(R 0.471, F 0.496)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.288] [G acc: 0.156]\n",
      "3427 [D loss: (0.548)(R 0.515, F 0.580)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.363] [G acc: 0.094]\n",
      "3428 [D loss: (0.597)(R 0.600, F 0.593)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.327] [G acc: 0.094]\n",
      "3429 [D loss: (0.626)(R 0.616, F 0.636)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.274] [G acc: 0.125]\n",
      "3430 [D loss: (0.491)(R 0.517, F 0.465)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.191] [G acc: 0.188]\n",
      "3431 [D loss: (0.540)(R 0.577, F 0.504)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.273] [G acc: 0.078]\n",
      "3432 [D loss: (0.584)(R 0.578, F 0.591)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.313] [G acc: 0.172]\n",
      "3433 [D loss: (0.511)(R 0.474, F 0.549)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.296] [G acc: 0.094]\n",
      "3434 [D loss: (0.666)(R 0.645, F 0.687)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.322] [G acc: 0.094]\n",
      "3435 [D loss: (0.557)(R 0.645, F 0.470)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.309] [G acc: 0.094]\n",
      "3436 [D loss: (0.607)(R 0.667, F 0.546)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.011] [G acc: 0.203]\n",
      "3437 [D loss: (0.648)(R 0.520, F 0.775)] [D acc: (0.672)(0.734, 0.609)] [G loss: 1.240] [G acc: 0.047]\n",
      "3438 [D loss: (0.540)(R 0.648, F 0.432)] [D acc: (0.727)(0.547, 0.906)] [G loss: 1.321] [G acc: 0.094]\n",
      "3439 [D loss: (0.574)(R 0.639, F 0.508)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.098] [G acc: 0.156]\n",
      "3440 [D loss: (0.578)(R 0.579, F 0.577)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.278] [G acc: 0.109]\n",
      "3441 [D loss: (0.583)(R 0.603, F 0.564)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.235] [G acc: 0.109]\n",
      "3442 [D loss: (0.548)(R 0.530, F 0.567)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.202] [G acc: 0.094]\n",
      "3443 [D loss: (0.536)(R 0.510, F 0.561)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.317] [G acc: 0.141]\n",
      "3444 [D loss: (0.529)(R 0.491, F 0.567)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.224] [G acc: 0.141]\n",
      "3445 [D loss: (0.665)(R 0.548, F 0.782)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.251] [G acc: 0.109]\n",
      "3446 [D loss: (0.650)(R 0.835, F 0.465)] [D acc: (0.680)(0.484, 0.875)] [G loss: 1.186] [G acc: 0.156]\n",
      "3447 [D loss: (0.586)(R 0.547, F 0.626)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.248] [G acc: 0.125]\n",
      "3448 [D loss: (0.476)(R 0.531, F 0.421)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.249] [G acc: 0.109]\n",
      "3449 [D loss: (0.566)(R 0.597, F 0.536)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.119] [G acc: 0.156]\n",
      "3450 [D loss: (0.619)(R 0.611, F 0.627)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.219] [G acc: 0.047]\n",
      "3451 [D loss: (0.623)(R 0.709, F 0.537)] [D acc: (0.641)(0.484, 0.797)] [G loss: 1.244] [G acc: 0.047]\n",
      "3452 [D loss: (0.557)(R 0.574, F 0.540)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.291] [G acc: 0.062]\n",
      "3453 [D loss: (0.547)(R 0.502, F 0.592)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.134] [G acc: 0.125]\n",
      "3454 [D loss: (0.616)(R 0.626, F 0.606)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.200] [G acc: 0.141]\n",
      "3455 [D loss: (0.576)(R 0.649, F 0.504)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.180] [G acc: 0.203]\n",
      "3456 [D loss: (0.544)(R 0.477, F 0.611)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.297] [G acc: 0.078]\n",
      "3457 [D loss: (0.492)(R 0.498, F 0.486)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.142] [G acc: 0.141]\n",
      "3458 [D loss: (0.496)(R 0.506, F 0.487)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.320] [G acc: 0.141]\n",
      "3459 [D loss: (0.652)(R 0.640, F 0.664)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.217] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3460 [D loss: (0.596)(R 0.703, F 0.490)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.121] [G acc: 0.141]\n",
      "3461 [D loss: (0.577)(R 0.577, F 0.576)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.140] [G acc: 0.188]\n",
      "3462 [D loss: (0.637)(R 0.683, F 0.591)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.215] [G acc: 0.172]\n",
      "3463 [D loss: (0.594)(R 0.595, F 0.593)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.127] [G acc: 0.188]\n",
      "3464 [D loss: (0.606)(R 0.648, F 0.564)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.137] [G acc: 0.188]\n",
      "3465 [D loss: (0.626)(R 0.577, F 0.675)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.198] [G acc: 0.141]\n",
      "3466 [D loss: (0.552)(R 0.549, F 0.555)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.203] [G acc: 0.109]\n",
      "3467 [D loss: (0.675)(R 0.642, F 0.707)] [D acc: (0.586)(0.578, 0.594)] [G loss: 1.322] [G acc: 0.125]\n",
      "3468 [D loss: (0.560)(R 0.619, F 0.500)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.165] [G acc: 0.141]\n",
      "3469 [D loss: (0.580)(R 0.595, F 0.565)] [D acc: (0.609)(0.578, 0.641)] [G loss: 1.124] [G acc: 0.172]\n",
      "3470 [D loss: (0.545)(R 0.543, F 0.547)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.148] [G acc: 0.172]\n",
      "3471 [D loss: (0.554)(R 0.531, F 0.577)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.303] [G acc: 0.078]\n",
      "3472 [D loss: (0.630)(R 0.581, F 0.678)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.135] [G acc: 0.109]\n",
      "3473 [D loss: (0.525)(R 0.532, F 0.517)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.278] [G acc: 0.141]\n",
      "3474 [D loss: (0.558)(R 0.511, F 0.604)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.242] [G acc: 0.156]\n",
      "3475 [D loss: (0.598)(R 0.535, F 0.661)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.322] [G acc: 0.094]\n",
      "3476 [D loss: (0.553)(R 0.590, F 0.516)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.359] [G acc: 0.109]\n",
      "3477 [D loss: (0.537)(R 0.553, F 0.521)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.204] [G acc: 0.109]\n",
      "3478 [D loss: (0.619)(R 0.530, F 0.708)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.295] [G acc: 0.062]\n",
      "3479 [D loss: (0.550)(R 0.587, F 0.514)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.314] [G acc: 0.047]\n",
      "3480 [D loss: (0.610)(R 0.575, F 0.644)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.267] [G acc: 0.062]\n",
      "3481 [D loss: (0.538)(R 0.587, F 0.490)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.274] [G acc: 0.109]\n",
      "3482 [D loss: (0.596)(R 0.651, F 0.541)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.242] [G acc: 0.141]\n",
      "3483 [D loss: (0.522)(R 0.536, F 0.508)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.204] [G acc: 0.094]\n",
      "3484 [D loss: (0.551)(R 0.670, F 0.433)] [D acc: (0.773)(0.625, 0.922)] [G loss: 1.180] [G acc: 0.188]\n",
      "3485 [D loss: (0.520)(R 0.542, F 0.498)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.235] [G acc: 0.125]\n",
      "3486 [D loss: (0.486)(R 0.451, F 0.520)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.249] [G acc: 0.172]\n",
      "3487 [D loss: (0.527)(R 0.491, F 0.563)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.265] [G acc: 0.078]\n",
      "3488 [D loss: (0.578)(R 0.526, F 0.631)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.380] [G acc: 0.078]\n",
      "3489 [D loss: (0.579)(R 0.593, F 0.565)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.291] [G acc: 0.062]\n",
      "3490 [D loss: (0.593)(R 0.647, F 0.539)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.313] [G acc: 0.062]\n",
      "3491 [D loss: (0.571)(R 0.550, F 0.592)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.135] [G acc: 0.156]\n",
      "3492 [D loss: (0.614)(R 0.647, F 0.580)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.220] [G acc: 0.172]\n",
      "3493 [D loss: (0.598)(R 0.539, F 0.656)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.144] [G acc: 0.141]\n",
      "3494 [D loss: (0.557)(R 0.612, F 0.501)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.403] [G acc: 0.109]\n",
      "3495 [D loss: (0.591)(R 0.668, F 0.514)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.365] [G acc: 0.156]\n",
      "3496 [D loss: (0.501)(R 0.476, F 0.526)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.447] [G acc: 0.047]\n",
      "3497 [D loss: (0.540)(R 0.530, F 0.551)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.337] [G acc: 0.047]\n",
      "3498 [D loss: (0.619)(R 0.560, F 0.678)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.369] [G acc: 0.078]\n",
      "3499 [D loss: (0.528)(R 0.582, F 0.474)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.275] [G acc: 0.125]\n",
      "3500 [D loss: (0.527)(R 0.502, F 0.551)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.276] [G acc: 0.109]\n",
      "3501 [D loss: (0.589)(R 0.570, F 0.607)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.248] [G acc: 0.078]\n",
      "3502 [D loss: (0.636)(R 0.691, F 0.581)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.220] [G acc: 0.094]\n",
      "3503 [D loss: (0.499)(R 0.435, F 0.563)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.210] [G acc: 0.141]\n",
      "3504 [D loss: (0.541)(R 0.518, F 0.563)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.228] [G acc: 0.141]\n",
      "3505 [D loss: (0.540)(R 0.512, F 0.568)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.272] [G acc: 0.078]\n",
      "3506 [D loss: (0.607)(R 0.655, F 0.558)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.290] [G acc: 0.125]\n",
      "3507 [D loss: (0.540)(R 0.531, F 0.548)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.333] [G acc: 0.109]\n",
      "3508 [D loss: (0.588)(R 0.647, F 0.529)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.490] [G acc: 0.078]\n",
      "3509 [D loss: (0.594)(R 0.663, F 0.524)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.263] [G acc: 0.188]\n",
      "3510 [D loss: (0.585)(R 0.498, F 0.673)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.257] [G acc: 0.094]\n",
      "3511 [D loss: (0.560)(R 0.644, F 0.476)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.304] [G acc: 0.141]\n",
      "3512 [D loss: (0.608)(R 0.588, F 0.628)] [D acc: (0.609)(0.625, 0.594)] [G loss: 1.338] [G acc: 0.047]\n",
      "3513 [D loss: (0.576)(R 0.609, F 0.544)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.126] [G acc: 0.125]\n",
      "3514 [D loss: (0.599)(R 0.611, F 0.587)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.187] [G acc: 0.172]\n",
      "3515 [D loss: (0.555)(R 0.610, F 0.499)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.162] [G acc: 0.094]\n",
      "3516 [D loss: (0.540)(R 0.453, F 0.627)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.207] [G acc: 0.078]\n",
      "3517 [D loss: (0.582)(R 0.679, F 0.484)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.196] [G acc: 0.141]\n",
      "3518 [D loss: (0.546)(R 0.563, F 0.528)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.261] [G acc: 0.109]\n",
      "3519 [D loss: (0.507)(R 0.457, F 0.558)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.295] [G acc: 0.078]\n",
      "3520 [D loss: (0.523)(R 0.534, F 0.513)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.301] [G acc: 0.109]\n",
      "3521 [D loss: (0.554)(R 0.627, F 0.480)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.316] [G acc: 0.078]\n",
      "3522 [D loss: (0.570)(R 0.504, F 0.635)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.305] [G acc: 0.109]\n",
      "3523 [D loss: (0.563)(R 0.569, F 0.557)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.240] [G acc: 0.125]\n",
      "3524 [D loss: (0.490)(R 0.411, F 0.569)] [D acc: (0.766)(0.781, 0.750)] [G loss: 1.239] [G acc: 0.094]\n",
      "3525 [D loss: (0.571)(R 0.562, F 0.580)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.223] [G acc: 0.172]\n",
      "3526 [D loss: (0.490)(R 0.431, F 0.548)] [D acc: (0.773)(0.781, 0.766)] [G loss: 1.189] [G acc: 0.219]\n",
      "3527 [D loss: (0.634)(R 0.606, F 0.662)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.341] [G acc: 0.094]\n",
      "3528 [D loss: (0.484)(R 0.500, F 0.468)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.283] [G acc: 0.141]\n",
      "3529 [D loss: (0.576)(R 0.539, F 0.613)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.376] [G acc: 0.109]\n",
      "3530 [D loss: (0.561)(R 0.622, F 0.500)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.360] [G acc: 0.094]\n",
      "3531 [D loss: (0.553)(R 0.574, F 0.531)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.240] [G acc: 0.172]\n",
      "3532 [D loss: (0.584)(R 0.652, F 0.515)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.217] [G acc: 0.094]\n",
      "3533 [D loss: (0.580)(R 0.520, F 0.641)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.218] [G acc: 0.094]\n",
      "3534 [D loss: (0.522)(R 0.521, F 0.524)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.216] [G acc: 0.125]\n",
      "3535 [D loss: (0.594)(R 0.577, F 0.611)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.289] [G acc: 0.078]\n",
      "3536 [D loss: (0.559)(R 0.619, F 0.499)] [D acc: (0.727)(0.547, 0.906)] [G loss: 1.261] [G acc: 0.188]\n",
      "3537 [D loss: (0.502)(R 0.501, F 0.503)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.375] [G acc: 0.125]\n",
      "3538 [D loss: (0.525)(R 0.512, F 0.539)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.382] [G acc: 0.109]\n",
      "3539 [D loss: (0.537)(R 0.634, F 0.441)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.384] [G acc: 0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3540 [D loss: (0.672)(R 0.611, F 0.733)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.144] [G acc: 0.125]\n",
      "3541 [D loss: (0.543)(R 0.557, F 0.528)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.259] [G acc: 0.109]\n",
      "3542 [D loss: (0.558)(R 0.568, F 0.548)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.329] [G acc: 0.062]\n",
      "3543 [D loss: (0.480)(R 0.497, F 0.463)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.269] [G acc: 0.062]\n",
      "3544 [D loss: (0.566)(R 0.642, F 0.490)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.301] [G acc: 0.078]\n",
      "3545 [D loss: (0.519)(R 0.475, F 0.564)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.442] [G acc: 0.047]\n",
      "3546 [D loss: (0.631)(R 0.652, F 0.609)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.277] [G acc: 0.047]\n",
      "3547 [D loss: (0.581)(R 0.588, F 0.573)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.246] [G acc: 0.062]\n",
      "3548 [D loss: (0.552)(R 0.567, F 0.536)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.399] [G acc: 0.094]\n",
      "3549 [D loss: (0.573)(R 0.589, F 0.557)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.254] [G acc: 0.062]\n",
      "3550 [D loss: (0.529)(R 0.580, F 0.477)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.424] [G acc: 0.094]\n",
      "3551 [D loss: (0.582)(R 0.647, F 0.517)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.327] [G acc: 0.094]\n",
      "3552 [D loss: (0.499)(R 0.560, F 0.437)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.395] [G acc: 0.078]\n",
      "3553 [D loss: (0.496)(R 0.425, F 0.566)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.252] [G acc: 0.125]\n",
      "3554 [D loss: (0.560)(R 0.547, F 0.574)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.273] [G acc: 0.078]\n",
      "3555 [D loss: (0.609)(R 0.615, F 0.603)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.297] [G acc: 0.031]\n",
      "3556 [D loss: (0.583)(R 0.590, F 0.576)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.369] [G acc: 0.078]\n",
      "3557 [D loss: (0.586)(R 0.701, F 0.472)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.252] [G acc: 0.156]\n",
      "3558 [D loss: (0.531)(R 0.571, F 0.490)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.304] [G acc: 0.109]\n",
      "3559 [D loss: (0.564)(R 0.407, F 0.721)] [D acc: (0.703)(0.797, 0.609)] [G loss: 1.286] [G acc: 0.094]\n",
      "3560 [D loss: (0.597)(R 0.682, F 0.513)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.259] [G acc: 0.141]\n",
      "3561 [D loss: (0.509)(R 0.547, F 0.470)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.168] [G acc: 0.109]\n",
      "3562 [D loss: (0.506)(R 0.499, F 0.513)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.325] [G acc: 0.172]\n",
      "3563 [D loss: (0.497)(R 0.525, F 0.469)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.373] [G acc: 0.125]\n",
      "3564 [D loss: (0.458)(R 0.442, F 0.474)] [D acc: (0.797)(0.750, 0.844)] [G loss: 1.405] [G acc: 0.109]\n",
      "3565 [D loss: (0.539)(R 0.425, F 0.653)] [D acc: (0.781)(0.766, 0.797)] [G loss: 1.472] [G acc: 0.094]\n",
      "3566 [D loss: (0.481)(R 0.504, F 0.457)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.263] [G acc: 0.172]\n",
      "3567 [D loss: (0.468)(R 0.472, F 0.465)] [D acc: (0.797)(0.719, 0.875)] [G loss: 1.419] [G acc: 0.094]\n",
      "3568 [D loss: (0.565)(R 0.583, F 0.548)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.383] [G acc: 0.109]\n",
      "3569 [D loss: (0.468)(R 0.580, F 0.356)] [D acc: (0.781)(0.641, 0.922)] [G loss: 1.416] [G acc: 0.078]\n",
      "3570 [D loss: (0.453)(R 0.459, F 0.447)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.509] [G acc: 0.078]\n",
      "3571 [D loss: (0.461)(R 0.469, F 0.454)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.362] [G acc: 0.125]\n",
      "3572 [D loss: (0.531)(R 0.482, F 0.579)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.381] [G acc: 0.203]\n",
      "3573 [D loss: (0.597)(R 0.570, F 0.623)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.314] [G acc: 0.109]\n",
      "3574 [D loss: (0.544)(R 0.558, F 0.531)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.335] [G acc: 0.125]\n",
      "3575 [D loss: (0.505)(R 0.445, F 0.564)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.430] [G acc: 0.109]\n",
      "3576 [D loss: (0.558)(R 0.552, F 0.563)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.358] [G acc: 0.062]\n",
      "3577 [D loss: (0.563)(R 0.600, F 0.527)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.428] [G acc: 0.078]\n",
      "3578 [D loss: (0.531)(R 0.517, F 0.546)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.419] [G acc: 0.125]\n",
      "3579 [D loss: (0.506)(R 0.537, F 0.475)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.399] [G acc: 0.156]\n",
      "3580 [D loss: (0.611)(R 0.587, F 0.635)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.321] [G acc: 0.109]\n",
      "3581 [D loss: (0.549)(R 0.608, F 0.490)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.343] [G acc: 0.078]\n",
      "3582 [D loss: (0.470)(R 0.445, F 0.496)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.174] [G acc: 0.109]\n",
      "3583 [D loss: (0.517)(R 0.457, F 0.576)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.231] [G acc: 0.094]\n",
      "3584 [D loss: (0.584)(R 0.636, F 0.532)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.358] [G acc: 0.078]\n",
      "3585 [D loss: (0.527)(R 0.517, F 0.537)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.380] [G acc: 0.078]\n",
      "3586 [D loss: (0.522)(R 0.455, F 0.589)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.368] [G acc: 0.078]\n",
      "3587 [D loss: (0.593)(R 0.553, F 0.632)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.311] [G acc: 0.047]\n",
      "3588 [D loss: (0.582)(R 0.702, F 0.463)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.382] [G acc: 0.078]\n",
      "3589 [D loss: (0.622)(R 0.618, F 0.627)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.293] [G acc: 0.062]\n",
      "3590 [D loss: (0.554)(R 0.532, F 0.575)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.389] [G acc: 0.109]\n",
      "3591 [D loss: (0.511)(R 0.510, F 0.513)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.238] [G acc: 0.156]\n",
      "3592 [D loss: (0.485)(R 0.514, F 0.456)] [D acc: (0.773)(0.656, 0.891)] [G loss: 1.249] [G acc: 0.094]\n",
      "3593 [D loss: (0.542)(R 0.568, F 0.515)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.315] [G acc: 0.125]\n",
      "3594 [D loss: (0.602)(R 0.701, F 0.503)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.287] [G acc: 0.094]\n",
      "3595 [D loss: (0.598)(R 0.466, F 0.731)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.330] [G acc: 0.031]\n",
      "3596 [D loss: (0.563)(R 0.684, F 0.443)] [D acc: (0.711)(0.531, 0.891)] [G loss: 1.347] [G acc: 0.125]\n",
      "3597 [D loss: (0.622)(R 0.627, F 0.618)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.247] [G acc: 0.094]\n",
      "3598 [D loss: (0.579)(R 0.611, F 0.546)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.316] [G acc: 0.109]\n",
      "3599 [D loss: (0.454)(R 0.450, F 0.457)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.168] [G acc: 0.188]\n",
      "3600 [D loss: (0.564)(R 0.603, F 0.525)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.287] [G acc: 0.156]\n",
      "3601 [D loss: (0.529)(R 0.531, F 0.527)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.292] [G acc: 0.141]\n",
      "3602 [D loss: (0.572)(R 0.566, F 0.577)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.294] [G acc: 0.188]\n",
      "3603 [D loss: (0.586)(R 0.566, F 0.606)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.272] [G acc: 0.078]\n",
      "3604 [D loss: (0.638)(R 0.678, F 0.598)] [D acc: (0.609)(0.484, 0.734)] [G loss: 1.188] [G acc: 0.156]\n",
      "3605 [D loss: (0.598)(R 0.560, F 0.637)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.262] [G acc: 0.109]\n",
      "3606 [D loss: (0.570)(R 0.669, F 0.472)] [D acc: (0.648)(0.469, 0.828)] [G loss: 1.244] [G acc: 0.125]\n",
      "3607 [D loss: (0.557)(R 0.551, F 0.564)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.344] [G acc: 0.078]\n",
      "3608 [D loss: (0.558)(R 0.595, F 0.521)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.128] [G acc: 0.141]\n",
      "3609 [D loss: (0.526)(R 0.547, F 0.506)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.253] [G acc: 0.125]\n",
      "3610 [D loss: (0.549)(R 0.563, F 0.535)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.158] [G acc: 0.125]\n",
      "3611 [D loss: (0.578)(R 0.656, F 0.499)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.208] [G acc: 0.125]\n",
      "3612 [D loss: (0.571)(R 0.450, F 0.691)] [D acc: (0.711)(0.750, 0.672)] [G loss: 1.235] [G acc: 0.109]\n",
      "3613 [D loss: (0.584)(R 0.576, F 0.592)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.223] [G acc: 0.125]\n",
      "3614 [D loss: (0.527)(R 0.571, F 0.484)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.314] [G acc: 0.141]\n",
      "3615 [D loss: (0.582)(R 0.588, F 0.576)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.322] [G acc: 0.078]\n",
      "3616 [D loss: (0.586)(R 0.608, F 0.564)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.343] [G acc: 0.125]\n",
      "3617 [D loss: (0.528)(R 0.526, F 0.529)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.291] [G acc: 0.062]\n",
      "3618 [D loss: (0.591)(R 0.623, F 0.559)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.128] [G acc: 0.219]\n",
      "3619 [D loss: (0.554)(R 0.485, F 0.622)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.247] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3620 [D loss: (0.598)(R 0.659, F 0.538)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.193] [G acc: 0.172]\n",
      "3621 [D loss: (0.574)(R 0.652, F 0.497)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.215] [G acc: 0.109]\n",
      "3622 [D loss: (0.650)(R 0.557, F 0.743)] [D acc: (0.688)(0.719, 0.656)] [G loss: 1.133] [G acc: 0.156]\n",
      "3623 [D loss: (0.568)(R 0.536, F 0.600)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.263] [G acc: 0.109]\n",
      "3624 [D loss: (0.520)(R 0.579, F 0.461)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.130] [G acc: 0.141]\n",
      "3625 [D loss: (0.683)(R 0.553, F 0.812)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.208] [G acc: 0.219]\n",
      "3626 [D loss: (0.524)(R 0.543, F 0.505)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.300] [G acc: 0.078]\n",
      "3627 [D loss: (0.548)(R 0.570, F 0.525)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.323] [G acc: 0.078]\n",
      "3628 [D loss: (0.576)(R 0.556, F 0.596)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.316] [G acc: 0.062]\n",
      "3629 [D loss: (0.598)(R 0.720, F 0.475)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.232] [G acc: 0.203]\n",
      "3630 [D loss: (0.484)(R 0.473, F 0.496)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.292] [G acc: 0.172]\n",
      "3631 [D loss: (0.598)(R 0.601, F 0.595)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.314] [G acc: 0.141]\n",
      "3632 [D loss: (0.538)(R 0.599, F 0.477)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.391] [G acc: 0.109]\n",
      "3633 [D loss: (0.503)(R 0.500, F 0.507)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.295] [G acc: 0.125]\n",
      "3634 [D loss: (0.540)(R 0.521, F 0.559)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.250] [G acc: 0.172]\n",
      "3635 [D loss: (0.620)(R 0.573, F 0.667)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.322] [G acc: 0.078]\n",
      "3636 [D loss: (0.625)(R 0.634, F 0.617)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.190] [G acc: 0.141]\n",
      "3637 [D loss: (0.572)(R 0.551, F 0.594)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.242] [G acc: 0.141]\n",
      "3638 [D loss: (0.507)(R 0.564, F 0.450)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.184] [G acc: 0.156]\n",
      "3639 [D loss: (0.649)(R 0.682, F 0.616)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.214] [G acc: 0.109]\n",
      "3640 [D loss: (0.667)(R 0.725, F 0.609)] [D acc: (0.594)(0.516, 0.672)] [G loss: 1.171] [G acc: 0.109]\n",
      "3641 [D loss: (0.548)(R 0.527, F 0.568)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.307] [G acc: 0.125]\n",
      "3642 [D loss: (0.534)(R 0.517, F 0.551)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.086] [G acc: 0.203]\n",
      "3643 [D loss: (0.540)(R 0.517, F 0.563)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.196] [G acc: 0.125]\n",
      "3644 [D loss: (0.498)(R 0.453, F 0.543)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.254] [G acc: 0.156]\n",
      "3645 [D loss: (0.594)(R 0.517, F 0.670)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.211] [G acc: 0.125]\n",
      "3646 [D loss: (0.573)(R 0.575, F 0.571)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.183] [G acc: 0.156]\n",
      "3647 [D loss: (0.480)(R 0.522, F 0.438)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.268] [G acc: 0.203]\n",
      "3648 [D loss: (0.549)(R 0.470, F 0.628)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.363] [G acc: 0.141]\n",
      "3649 [D loss: (0.531)(R 0.564, F 0.498)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.478] [G acc: 0.109]\n",
      "3650 [D loss: (0.479)(R 0.486, F 0.473)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.446] [G acc: 0.125]\n",
      "3651 [D loss: (0.557)(R 0.516, F 0.598)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.425] [G acc: 0.078]\n",
      "3652 [D loss: (0.522)(R 0.539, F 0.505)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.599] [G acc: 0.031]\n",
      "3653 [D loss: (0.591)(R 0.660, F 0.522)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.386] [G acc: 0.156]\n",
      "3654 [D loss: (0.605)(R 0.717, F 0.493)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.260] [G acc: 0.125]\n",
      "3655 [D loss: (0.566)(R 0.591, F 0.542)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.304] [G acc: 0.094]\n",
      "3656 [D loss: (0.553)(R 0.497, F 0.610)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.430] [G acc: 0.078]\n",
      "3657 [D loss: (0.492)(R 0.501, F 0.482)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.330] [G acc: 0.078]\n",
      "3658 [D loss: (0.487)(R 0.525, F 0.448)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.289] [G acc: 0.234]\n",
      "3659 [D loss: (0.654)(R 0.533, F 0.774)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.360] [G acc: 0.109]\n",
      "3660 [D loss: (0.630)(R 0.749, F 0.511)] [D acc: (0.648)(0.469, 0.828)] [G loss: 1.262] [G acc: 0.188]\n",
      "3661 [D loss: (0.559)(R 0.594, F 0.524)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.421] [G acc: 0.141]\n",
      "3662 [D loss: (0.548)(R 0.567, F 0.530)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.455] [G acc: 0.094]\n",
      "3663 [D loss: (0.532)(R 0.600, F 0.463)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.220] [G acc: 0.109]\n",
      "3664 [D loss: (0.501)(R 0.520, F 0.481)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.393] [G acc: 0.125]\n",
      "3665 [D loss: (0.609)(R 0.544, F 0.675)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.339] [G acc: 0.078]\n",
      "3666 [D loss: (0.538)(R 0.639, F 0.437)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.362] [G acc: 0.141]\n",
      "3667 [D loss: (0.517)(R 0.590, F 0.444)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.283] [G acc: 0.125]\n",
      "3668 [D loss: (0.658)(R 0.441, F 0.875)] [D acc: (0.648)(0.734, 0.562)] [G loss: 1.237] [G acc: 0.078]\n",
      "3669 [D loss: (0.568)(R 0.688, F 0.448)] [D acc: (0.664)(0.500, 0.828)] [G loss: 1.216] [G acc: 0.141]\n",
      "3670 [D loss: (0.546)(R 0.527, F 0.565)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.134] [G acc: 0.141]\n",
      "3671 [D loss: (0.548)(R 0.516, F 0.580)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.175] [G acc: 0.172]\n",
      "3672 [D loss: (0.516)(R 0.567, F 0.466)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.204] [G acc: 0.156]\n",
      "3673 [D loss: (0.570)(R 0.578, F 0.563)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.256] [G acc: 0.109]\n",
      "3674 [D loss: (0.549)(R 0.536, F 0.561)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.260] [G acc: 0.078]\n",
      "3675 [D loss: (0.561)(R 0.498, F 0.623)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.323] [G acc: 0.141]\n",
      "3676 [D loss: (0.559)(R 0.541, F 0.577)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.367] [G acc: 0.172]\n",
      "3677 [D loss: (0.554)(R 0.630, F 0.479)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.192] [G acc: 0.156]\n",
      "3678 [D loss: (0.541)(R 0.596, F 0.487)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.228] [G acc: 0.125]\n",
      "3679 [D loss: (0.555)(R 0.491, F 0.618)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.393] [G acc: 0.078]\n",
      "3680 [D loss: (0.593)(R 0.595, F 0.591)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.387] [G acc: 0.078]\n",
      "3681 [D loss: (0.538)(R 0.650, F 0.427)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.357] [G acc: 0.094]\n",
      "3682 [D loss: (0.493)(R 0.528, F 0.457)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.247] [G acc: 0.156]\n",
      "3683 [D loss: (0.581)(R 0.534, F 0.628)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.191] [G acc: 0.156]\n",
      "3684 [D loss: (0.574)(R 0.531, F 0.617)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.282] [G acc: 0.125]\n",
      "3685 [D loss: (0.589)(R 0.603, F 0.574)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.303] [G acc: 0.172]\n",
      "3686 [D loss: (0.617)(R 0.644, F 0.591)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.233] [G acc: 0.109]\n",
      "3687 [D loss: (0.548)(R 0.530, F 0.565)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.368] [G acc: 0.047]\n",
      "3688 [D loss: (0.598)(R 0.658, F 0.538)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.203] [G acc: 0.141]\n",
      "3689 [D loss: (0.552)(R 0.555, F 0.550)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.235] [G acc: 0.094]\n",
      "3690 [D loss: (0.545)(R 0.574, F 0.516)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.265] [G acc: 0.125]\n",
      "3691 [D loss: (0.494)(R 0.518, F 0.471)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.201] [G acc: 0.172]\n",
      "3692 [D loss: (0.559)(R 0.567, F 0.550)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.124] [G acc: 0.203]\n",
      "3693 [D loss: (0.580)(R 0.605, F 0.554)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.216] [G acc: 0.094]\n",
      "3694 [D loss: (0.539)(R 0.417, F 0.661)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.296] [G acc: 0.094]\n",
      "3695 [D loss: (0.542)(R 0.600, F 0.485)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.306] [G acc: 0.125]\n",
      "3696 [D loss: (0.519)(R 0.528, F 0.511)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.163] [G acc: 0.156]\n",
      "3697 [D loss: (0.614)(R 0.628, F 0.599)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.225] [G acc: 0.094]\n",
      "3698 [D loss: (0.534)(R 0.569, F 0.499)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.212] [G acc: 0.125]\n",
      "3699 [D loss: (0.552)(R 0.584, F 0.520)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.297] [G acc: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700 [D loss: (0.679)(R 0.720, F 0.638)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.225] [G acc: 0.156]\n",
      "3701 [D loss: (0.674)(R 0.689, F 0.659)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.246] [G acc: 0.094]\n",
      "3702 [D loss: (0.655)(R 0.590, F 0.721)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.142] [G acc: 0.062]\n",
      "3703 [D loss: (0.607)(R 0.654, F 0.559)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.158] [G acc: 0.109]\n",
      "3704 [D loss: (0.563)(R 0.522, F 0.604)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.124] [G acc: 0.156]\n",
      "3705 [D loss: (0.552)(R 0.598, F 0.506)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.099] [G acc: 0.141]\n",
      "3706 [D loss: (0.651)(R 0.666, F 0.636)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.175] [G acc: 0.172]\n",
      "3707 [D loss: (0.587)(R 0.581, F 0.592)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.055] [G acc: 0.219]\n",
      "3708 [D loss: (0.537)(R 0.562, F 0.511)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.225] [G acc: 0.109]\n",
      "3709 [D loss: (0.592)(R 0.616, F 0.568)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.277] [G acc: 0.047]\n",
      "3710 [D loss: (0.608)(R 0.547, F 0.669)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.299] [G acc: 0.031]\n",
      "3711 [D loss: (0.507)(R 0.529, F 0.485)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.190] [G acc: 0.109]\n",
      "3712 [D loss: (0.618)(R 0.610, F 0.626)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.127] [G acc: 0.141]\n",
      "3713 [D loss: (0.533)(R 0.531, F 0.536)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.255] [G acc: 0.125]\n",
      "3714 [D loss: (0.546)(R 0.565, F 0.527)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.271] [G acc: 0.188]\n",
      "3715 [D loss: (0.540)(R 0.586, F 0.493)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.135] [G acc: 0.156]\n",
      "3716 [D loss: (0.591)(R 0.536, F 0.647)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.214] [G acc: 0.094]\n",
      "3717 [D loss: (0.558)(R 0.642, F 0.473)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.208] [G acc: 0.141]\n",
      "3718 [D loss: (0.540)(R 0.530, F 0.549)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.132] [G acc: 0.109]\n",
      "3719 [D loss: (0.573)(R 0.605, F 0.542)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.379] [G acc: 0.125]\n",
      "3720 [D loss: (0.535)(R 0.618, F 0.452)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.277] [G acc: 0.109]\n",
      "3721 [D loss: (0.575)(R 0.659, F 0.492)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.206] [G acc: 0.125]\n",
      "3722 [D loss: (0.533)(R 0.472, F 0.593)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.274] [G acc: 0.109]\n",
      "3723 [D loss: (0.498)(R 0.497, F 0.499)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.213] [G acc: 0.172]\n",
      "3724 [D loss: (0.689)(R 0.698, F 0.680)] [D acc: (0.625)(0.500, 0.750)] [G loss: 1.145] [G acc: 0.141]\n",
      "3725 [D loss: (0.550)(R 0.548, F 0.551)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.125] [G acc: 0.109]\n",
      "3726 [D loss: (0.533)(R 0.527, F 0.538)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.181] [G acc: 0.125]\n",
      "3727 [D loss: (0.540)(R 0.469, F 0.611)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.425] [G acc: 0.062]\n",
      "3728 [D loss: (0.558)(R 0.590, F 0.527)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.337] [G acc: 0.109]\n",
      "3729 [D loss: (0.600)(R 0.629, F 0.572)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.327] [G acc: 0.156]\n",
      "3730 [D loss: (0.582)(R 0.589, F 0.575)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.302] [G acc: 0.188]\n",
      "3731 [D loss: (0.638)(R 0.650, F 0.626)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.132] [G acc: 0.172]\n",
      "3732 [D loss: (0.516)(R 0.574, F 0.458)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.225] [G acc: 0.125]\n",
      "3733 [D loss: (0.607)(R 0.581, F 0.633)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.391] [G acc: 0.062]\n",
      "3734 [D loss: (0.537)(R 0.518, F 0.556)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.185] [G acc: 0.109]\n",
      "3735 [D loss: (0.619)(R 0.711, F 0.527)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.276] [G acc: 0.172]\n",
      "3736 [D loss: (0.562)(R 0.529, F 0.596)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.232] [G acc: 0.078]\n",
      "3737 [D loss: (0.524)(R 0.451, F 0.598)] [D acc: (0.750)(0.797, 0.703)] [G loss: 1.113] [G acc: 0.172]\n",
      "3738 [D loss: (0.555)(R 0.557, F 0.553)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.268] [G acc: 0.094]\n",
      "3739 [D loss: (0.541)(R 0.584, F 0.498)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.286] [G acc: 0.109]\n",
      "3740 [D loss: (0.509)(R 0.455, F 0.564)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.238] [G acc: 0.094]\n",
      "3741 [D loss: (0.581)(R 0.535, F 0.626)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.316] [G acc: 0.094]\n",
      "3742 [D loss: (0.650)(R 0.585, F 0.716)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.237] [G acc: 0.125]\n",
      "3743 [D loss: (0.559)(R 0.571, F 0.547)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.302] [G acc: 0.031]\n",
      "3744 [D loss: (0.560)(R 0.578, F 0.542)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.260] [G acc: 0.125]\n",
      "3745 [D loss: (0.555)(R 0.559, F 0.551)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.284] [G acc: 0.109]\n",
      "3746 [D loss: (0.542)(R 0.539, F 0.545)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.170] [G acc: 0.188]\n",
      "3747 [D loss: (0.562)(R 0.559, F 0.565)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.398] [G acc: 0.094]\n",
      "3748 [D loss: (0.596)(R 0.622, F 0.569)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.200] [G acc: 0.125]\n",
      "3749 [D loss: (0.583)(R 0.576, F 0.591)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.212] [G acc: 0.094]\n",
      "3750 [D loss: (0.604)(R 0.685, F 0.524)] [D acc: (0.688)(0.516, 0.859)] [G loss: 1.223] [G acc: 0.141]\n",
      "3751 [D loss: (0.587)(R 0.582, F 0.593)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.345] [G acc: 0.062]\n",
      "3752 [D loss: (0.535)(R 0.491, F 0.578)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.222] [G acc: 0.156]\n",
      "3753 [D loss: (0.576)(R 0.567, F 0.585)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.232] [G acc: 0.156]\n",
      "3754 [D loss: (0.594)(R 0.589, F 0.598)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.172] [G acc: 0.141]\n",
      "3755 [D loss: (0.505)(R 0.542, F 0.468)] [D acc: (0.766)(0.625, 0.906)] [G loss: 1.201] [G acc: 0.188]\n",
      "3756 [D loss: (0.586)(R 0.522, F 0.651)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.137] [G acc: 0.141]\n",
      "3757 [D loss: (0.619)(R 0.611, F 0.627)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.345] [G acc: 0.062]\n",
      "3758 [D loss: (0.494)(R 0.529, F 0.458)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.286] [G acc: 0.109]\n",
      "3759 [D loss: (0.551)(R 0.614, F 0.488)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.315] [G acc: 0.156]\n",
      "3760 [D loss: (0.532)(R 0.527, F 0.538)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.399] [G acc: 0.141]\n",
      "3761 [D loss: (0.548)(R 0.532, F 0.565)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.268] [G acc: 0.156]\n",
      "3762 [D loss: (0.676)(R 0.764, F 0.587)] [D acc: (0.586)(0.422, 0.750)] [G loss: 1.186] [G acc: 0.125]\n",
      "3763 [D loss: (0.507)(R 0.464, F 0.550)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.239] [G acc: 0.156]\n",
      "3764 [D loss: (0.590)(R 0.579, F 0.601)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.375] [G acc: 0.062]\n",
      "3765 [D loss: (0.625)(R 0.691, F 0.560)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.140] [G acc: 0.281]\n",
      "3766 [D loss: (0.537)(R 0.478, F 0.596)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.173] [G acc: 0.141]\n",
      "3767 [D loss: (0.578)(R 0.440, F 0.716)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.181] [G acc: 0.172]\n",
      "3768 [D loss: (0.549)(R 0.560, F 0.537)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.244] [G acc: 0.203]\n",
      "3769 [D loss: (0.592)(R 0.656, F 0.527)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.242] [G acc: 0.156]\n",
      "3770 [D loss: (0.603)(R 0.598, F 0.608)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.384] [G acc: 0.062]\n",
      "3771 [D loss: (0.525)(R 0.597, F 0.453)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.237] [G acc: 0.156]\n",
      "3772 [D loss: (0.560)(R 0.454, F 0.667)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.310] [G acc: 0.016]\n",
      "3773 [D loss: (0.580)(R 0.658, F 0.502)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.356] [G acc: 0.109]\n",
      "3774 [D loss: (0.604)(R 0.657, F 0.551)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.262] [G acc: 0.172]\n",
      "3775 [D loss: (0.599)(R 0.526, F 0.672)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.339] [G acc: 0.141]\n",
      "3776 [D loss: (0.597)(R 0.634, F 0.559)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.230] [G acc: 0.078]\n",
      "3777 [D loss: (0.563)(R 0.558, F 0.567)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.284] [G acc: 0.188]\n",
      "3778 [D loss: (0.651)(R 0.615, F 0.687)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.203] [G acc: 0.125]\n",
      "3779 [D loss: (0.604)(R 0.722, F 0.485)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.085] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3780 [D loss: (0.672)(R 0.807, F 0.537)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.121] [G acc: 0.109]\n",
      "3781 [D loss: (0.570)(R 0.590, F 0.549)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.277] [G acc: 0.125]\n",
      "3782 [D loss: (0.566)(R 0.524, F 0.609)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.124] [G acc: 0.094]\n",
      "3783 [D loss: (0.499)(R 0.500, F 0.498)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.276] [G acc: 0.078]\n",
      "3784 [D loss: (0.617)(R 0.637, F 0.597)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.183] [G acc: 0.062]\n",
      "3785 [D loss: (0.544)(R 0.614, F 0.475)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.314] [G acc: 0.078]\n",
      "3786 [D loss: (0.559)(R 0.598, F 0.520)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.145] [G acc: 0.188]\n",
      "3787 [D loss: (0.648)(R 0.676, F 0.620)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.147] [G acc: 0.125]\n",
      "3788 [D loss: (0.573)(R 0.623, F 0.523)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.227] [G acc: 0.141]\n",
      "3789 [D loss: (0.638)(R 0.638, F 0.638)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.211] [G acc: 0.094]\n",
      "3790 [D loss: (0.465)(R 0.485, F 0.445)] [D acc: (0.797)(0.750, 0.844)] [G loss: 1.228] [G acc: 0.109]\n",
      "3791 [D loss: (0.668)(R 0.569, F 0.767)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.194] [G acc: 0.094]\n",
      "3792 [D loss: (0.547)(R 0.609, F 0.485)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.260] [G acc: 0.078]\n",
      "3793 [D loss: (0.570)(R 0.601, F 0.540)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.265] [G acc: 0.031]\n",
      "3794 [D loss: (0.532)(R 0.528, F 0.537)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.168] [G acc: 0.141]\n",
      "3795 [D loss: (0.578)(R 0.627, F 0.530)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.264] [G acc: 0.094]\n",
      "3796 [D loss: (0.540)(R 0.515, F 0.565)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.238] [G acc: 0.141]\n",
      "3797 [D loss: (0.514)(R 0.527, F 0.502)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.179] [G acc: 0.172]\n",
      "3798 [D loss: (0.594)(R 0.600, F 0.589)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.214] [G acc: 0.109]\n",
      "3799 [D loss: (0.555)(R 0.560, F 0.550)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.237] [G acc: 0.156]\n",
      "3800 [D loss: (0.631)(R 0.714, F 0.548)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.210] [G acc: 0.141]\n",
      "3801 [D loss: (0.537)(R 0.538, F 0.537)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.306] [G acc: 0.094]\n",
      "3802 [D loss: (0.522)(R 0.470, F 0.574)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.182] [G acc: 0.141]\n",
      "3803 [D loss: (0.527)(R 0.589, F 0.466)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.202] [G acc: 0.141]\n",
      "3804 [D loss: (0.499)(R 0.516, F 0.482)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.417] [G acc: 0.078]\n",
      "3805 [D loss: (0.575)(R 0.576, F 0.574)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.582] [G acc: 0.047]\n",
      "3806 [D loss: (0.551)(R 0.559, F 0.543)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.374] [G acc: 0.078]\n",
      "3807 [D loss: (0.615)(R 0.670, F 0.561)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.145] [G acc: 0.141]\n",
      "3808 [D loss: (0.515)(R 0.546, F 0.485)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.222] [G acc: 0.141]\n",
      "3809 [D loss: (0.544)(R 0.347, F 0.742)] [D acc: (0.781)(0.797, 0.766)] [G loss: 1.309] [G acc: 0.109]\n",
      "3810 [D loss: (0.541)(R 0.605, F 0.476)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.288] [G acc: 0.094]\n",
      "3811 [D loss: (0.580)(R 0.650, F 0.510)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.295] [G acc: 0.172]\n",
      "3812 [D loss: (0.502)(R 0.488, F 0.516)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.325] [G acc: 0.094]\n",
      "3813 [D loss: (0.558)(R 0.599, F 0.516)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.253] [G acc: 0.047]\n",
      "3814 [D loss: (0.592)(R 0.543, F 0.642)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.234] [G acc: 0.094]\n",
      "3815 [D loss: (0.593)(R 0.735, F 0.451)] [D acc: (0.727)(0.531, 0.922)] [G loss: 1.192] [G acc: 0.141]\n",
      "3816 [D loss: (0.466)(R 0.444, F 0.489)] [D acc: (0.781)(0.766, 0.797)] [G loss: 1.364] [G acc: 0.062]\n",
      "3817 [D loss: (0.531)(R 0.523, F 0.539)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.269] [G acc: 0.125]\n",
      "3818 [D loss: (0.528)(R 0.506, F 0.551)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.255] [G acc: 0.094]\n",
      "3819 [D loss: (0.530)(R 0.540, F 0.519)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.358] [G acc: 0.094]\n",
      "3820 [D loss: (0.491)(R 0.455, F 0.528)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.302] [G acc: 0.109]\n",
      "3821 [D loss: (0.604)(R 0.463, F 0.745)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.289] [G acc: 0.109]\n",
      "3822 [D loss: (0.518)(R 0.631, F 0.405)] [D acc: (0.766)(0.609, 0.922)] [G loss: 1.190] [G acc: 0.141]\n",
      "3823 [D loss: (0.579)(R 0.506, F 0.651)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.270] [G acc: 0.141]\n",
      "3824 [D loss: (0.622)(R 0.685, F 0.559)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.344] [G acc: 0.125]\n",
      "3825 [D loss: (0.515)(R 0.588, F 0.443)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.248] [G acc: 0.141]\n",
      "3826 [D loss: (0.640)(R 0.491, F 0.789)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.311] [G acc: 0.062]\n",
      "3827 [D loss: (0.578)(R 0.673, F 0.483)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.289] [G acc: 0.078]\n",
      "3828 [D loss: (0.550)(R 0.591, F 0.509)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.277] [G acc: 0.125]\n",
      "3829 [D loss: (0.544)(R 0.552, F 0.536)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.255] [G acc: 0.125]\n",
      "3830 [D loss: (0.598)(R 0.590, F 0.605)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.233] [G acc: 0.125]\n",
      "3831 [D loss: (0.579)(R 0.608, F 0.550)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.283] [G acc: 0.125]\n",
      "3832 [D loss: (0.619)(R 0.577, F 0.661)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.370] [G acc: 0.047]\n",
      "3833 [D loss: (0.535)(R 0.409, F 0.660)] [D acc: (0.734)(0.766, 0.703)] [G loss: 1.182] [G acc: 0.141]\n",
      "3834 [D loss: (0.526)(R 0.569, F 0.483)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.224] [G acc: 0.109]\n",
      "3835 [D loss: (0.525)(R 0.556, F 0.494)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.242] [G acc: 0.109]\n",
      "3836 [D loss: (0.603)(R 0.724, F 0.483)] [D acc: (0.641)(0.469, 0.812)] [G loss: 1.222] [G acc: 0.188]\n",
      "3837 [D loss: (0.555)(R 0.488, F 0.622)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.198] [G acc: 0.188]\n",
      "3838 [D loss: (0.559)(R 0.531, F 0.587)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.314] [G acc: 0.141]\n",
      "3839 [D loss: (0.516)(R 0.533, F 0.498)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.288] [G acc: 0.078]\n",
      "3840 [D loss: (0.575)(R 0.612, F 0.538)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.268] [G acc: 0.109]\n",
      "3841 [D loss: (0.528)(R 0.548, F 0.508)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.337] [G acc: 0.125]\n",
      "3842 [D loss: (0.581)(R 0.518, F 0.644)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.177] [G acc: 0.125]\n",
      "3843 [D loss: (0.602)(R 0.603, F 0.602)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.288] [G acc: 0.094]\n",
      "3844 [D loss: (0.554)(R 0.603, F 0.505)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.235] [G acc: 0.094]\n",
      "3845 [D loss: (0.461)(R 0.474, F 0.449)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.340] [G acc: 0.125]\n",
      "3846 [D loss: (0.520)(R 0.562, F 0.478)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.222] [G acc: 0.188]\n",
      "3847 [D loss: (0.575)(R 0.607, F 0.543)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.233] [G acc: 0.094]\n",
      "3848 [D loss: (0.503)(R 0.533, F 0.474)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.284] [G acc: 0.109]\n",
      "3849 [D loss: (0.604)(R 0.456, F 0.752)] [D acc: (0.648)(0.703, 0.594)] [G loss: 1.173] [G acc: 0.078]\n",
      "3850 [D loss: (0.619)(R 0.612, F 0.625)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.287] [G acc: 0.062]\n",
      "3851 [D loss: (0.564)(R 0.631, F 0.496)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.255] [G acc: 0.078]\n",
      "3852 [D loss: (0.610)(R 0.567, F 0.654)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.227] [G acc: 0.141]\n",
      "3853 [D loss: (0.610)(R 0.649, F 0.572)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.212] [G acc: 0.078]\n",
      "3854 [D loss: (0.506)(R 0.533, F 0.480)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.249] [G acc: 0.109]\n",
      "3855 [D loss: (0.574)(R 0.591, F 0.556)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.221] [G acc: 0.031]\n",
      "3856 [D loss: (0.540)(R 0.460, F 0.619)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.343] [G acc: 0.109]\n",
      "3857 [D loss: (0.505)(R 0.481, F 0.528)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.338] [G acc: 0.156]\n",
      "3858 [D loss: (0.497)(R 0.608, F 0.386)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.329] [G acc: 0.062]\n",
      "3859 [D loss: (0.528)(R 0.513, F 0.543)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.295] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3860 [D loss: (0.580)(R 0.597, F 0.562)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.308] [G acc: 0.125]\n",
      "3861 [D loss: (0.522)(R 0.532, F 0.512)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.256] [G acc: 0.203]\n",
      "3862 [D loss: (0.629)(R 0.618, F 0.641)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.259] [G acc: 0.109]\n",
      "3863 [D loss: (0.597)(R 0.591, F 0.602)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.342] [G acc: 0.078]\n",
      "3864 [D loss: (0.572)(R 0.669, F 0.476)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.311] [G acc: 0.156]\n",
      "3865 [D loss: (0.583)(R 0.528, F 0.637)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.203] [G acc: 0.156]\n",
      "3866 [D loss: (0.602)(R 0.637, F 0.567)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.264] [G acc: 0.109]\n",
      "3867 [D loss: (0.486)(R 0.509, F 0.463)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.361] [G acc: 0.078]\n",
      "3868 [D loss: (0.559)(R 0.541, F 0.578)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.350] [G acc: 0.062]\n",
      "3869 [D loss: (0.654)(R 0.765, F 0.543)] [D acc: (0.664)(0.500, 0.828)] [G loss: 1.237] [G acc: 0.078]\n",
      "3870 [D loss: (0.530)(R 0.600, F 0.460)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.198] [G acc: 0.062]\n",
      "3871 [D loss: (0.612)(R 0.704, F 0.520)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.144] [G acc: 0.172]\n",
      "3872 [D loss: (0.520)(R 0.447, F 0.593)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.218] [G acc: 0.109]\n",
      "3873 [D loss: (0.542)(R 0.520, F 0.564)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.221] [G acc: 0.141]\n",
      "3874 [D loss: (0.549)(R 0.559, F 0.538)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.230] [G acc: 0.125]\n",
      "3875 [D loss: (0.615)(R 0.591, F 0.639)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.190] [G acc: 0.094]\n",
      "3876 [D loss: (0.472)(R 0.437, F 0.506)] [D acc: (0.797)(0.750, 0.844)] [G loss: 1.359] [G acc: 0.094]\n",
      "3877 [D loss: (0.587)(R 0.552, F 0.623)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.183] [G acc: 0.141]\n",
      "3878 [D loss: (0.587)(R 0.715, F 0.458)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.349] [G acc: 0.125]\n",
      "3879 [D loss: (0.685)(R 0.667, F 0.703)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.146] [G acc: 0.172]\n",
      "3880 [D loss: (0.570)(R 0.673, F 0.467)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.292] [G acc: 0.109]\n",
      "3881 [D loss: (0.508)(R 0.491, F 0.524)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.196] [G acc: 0.156]\n",
      "3882 [D loss: (0.545)(R 0.508, F 0.581)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.414] [G acc: 0.031]\n",
      "3883 [D loss: (0.549)(R 0.597, F 0.502)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.503] [G acc: 0.062]\n",
      "3884 [D loss: (0.523)(R 0.573, F 0.473)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.265] [G acc: 0.188]\n",
      "3885 [D loss: (0.630)(R 0.658, F 0.602)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.395] [G acc: 0.047]\n",
      "3886 [D loss: (0.613)(R 0.657, F 0.569)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.262] [G acc: 0.062]\n",
      "3887 [D loss: (0.591)(R 0.633, F 0.550)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.119] [G acc: 0.188]\n",
      "3888 [D loss: (0.530)(R 0.504, F 0.557)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.152] [G acc: 0.109]\n",
      "3889 [D loss: (0.631)(R 0.591, F 0.671)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.223] [G acc: 0.109]\n",
      "3890 [D loss: (0.614)(R 0.640, F 0.588)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.232] [G acc: 0.125]\n",
      "3891 [D loss: (0.606)(R 0.576, F 0.636)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.170] [G acc: 0.141]\n",
      "3892 [D loss: (0.595)(R 0.613, F 0.577)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.226] [G acc: 0.188]\n",
      "3893 [D loss: (0.575)(R 0.598, F 0.553)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.146] [G acc: 0.109]\n",
      "3894 [D loss: (0.599)(R 0.631, F 0.568)] [D acc: (0.672)(0.500, 0.844)] [G loss: 1.336] [G acc: 0.109]\n",
      "3895 [D loss: (0.532)(R 0.580, F 0.484)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.468] [G acc: 0.062]\n",
      "3896 [D loss: (0.570)(R 0.543, F 0.597)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.180] [G acc: 0.141]\n",
      "3897 [D loss: (0.554)(R 0.535, F 0.573)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.133] [G acc: 0.094]\n",
      "3898 [D loss: (0.535)(R 0.582, F 0.489)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.353] [G acc: 0.031]\n",
      "3899 [D loss: (0.504)(R 0.478, F 0.530)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.228] [G acc: 0.094]\n",
      "3900 [D loss: (0.598)(R 0.583, F 0.614)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.275] [G acc: 0.047]\n",
      "3901 [D loss: (0.690)(R 0.634, F 0.745)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.238] [G acc: 0.094]\n",
      "3902 [D loss: (0.664)(R 0.688, F 0.641)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.194] [G acc: 0.172]\n",
      "3903 [D loss: (0.581)(R 0.690, F 0.472)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.252] [G acc: 0.141]\n",
      "3904 [D loss: (0.563)(R 0.564, F 0.562)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.351] [G acc: 0.125]\n",
      "3905 [D loss: (0.543)(R 0.556, F 0.531)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.248] [G acc: 0.094]\n",
      "3906 [D loss: (0.555)(R 0.435, F 0.676)] [D acc: (0.719)(0.766, 0.672)] [G loss: 1.314] [G acc: 0.078]\n",
      "3907 [D loss: (0.557)(R 0.652, F 0.463)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.383] [G acc: 0.047]\n",
      "3908 [D loss: (0.489)(R 0.522, F 0.457)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.281] [G acc: 0.188]\n",
      "3909 [D loss: (0.539)(R 0.504, F 0.574)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.476] [G acc: 0.109]\n",
      "3910 [D loss: (0.459)(R 0.469, F 0.449)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.437] [G acc: 0.141]\n",
      "3911 [D loss: (0.574)(R 0.589, F 0.558)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.367] [G acc: 0.125]\n",
      "3912 [D loss: (0.564)(R 0.543, F 0.584)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.273] [G acc: 0.188]\n",
      "3913 [D loss: (0.641)(R 0.561, F 0.720)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.443] [G acc: 0.094]\n",
      "3914 [D loss: (0.480)(R 0.575, F 0.386)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.377] [G acc: 0.125]\n",
      "3915 [D loss: (0.538)(R 0.583, F 0.493)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.216] [G acc: 0.281]\n",
      "3916 [D loss: (0.568)(R 0.606, F 0.530)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.277] [G acc: 0.125]\n",
      "3917 [D loss: (0.608)(R 0.615, F 0.600)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.281] [G acc: 0.125]\n",
      "3918 [D loss: (0.592)(R 0.547, F 0.637)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.328] [G acc: 0.125]\n",
      "3919 [D loss: (0.575)(R 0.633, F 0.516)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.261] [G acc: 0.203]\n",
      "3920 [D loss: (0.465)(R 0.505, F 0.424)] [D acc: (0.805)(0.719, 0.891)] [G loss: 1.182] [G acc: 0.125]\n",
      "3921 [D loss: (0.572)(R 0.548, F 0.597)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.271] [G acc: 0.125]\n",
      "3922 [D loss: (0.643)(R 0.652, F 0.633)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.139] [G acc: 0.219]\n",
      "3923 [D loss: (0.570)(R 0.651, F 0.489)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.274] [G acc: 0.141]\n",
      "3924 [D loss: (0.568)(R 0.540, F 0.595)] [D acc: (0.742)(0.766, 0.719)] [G loss: 1.313] [G acc: 0.094]\n",
      "3925 [D loss: (0.632)(R 0.574, F 0.690)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.113] [G acc: 0.125]\n",
      "3926 [D loss: (0.617)(R 0.695, F 0.539)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.211] [G acc: 0.156]\n",
      "3927 [D loss: (0.553)(R 0.538, F 0.567)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.211] [G acc: 0.141]\n",
      "3928 [D loss: (0.545)(R 0.533, F 0.557)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.296] [G acc: 0.109]\n",
      "3929 [D loss: (0.554)(R 0.528, F 0.579)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.244] [G acc: 0.125]\n",
      "3930 [D loss: (0.526)(R 0.550, F 0.502)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.215] [G acc: 0.125]\n",
      "3931 [D loss: (0.581)(R 0.547, F 0.614)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.307] [G acc: 0.094]\n",
      "3932 [D loss: (0.523)(R 0.496, F 0.551)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.200] [G acc: 0.156]\n",
      "3933 [D loss: (0.665)(R 0.678, F 0.653)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.292] [G acc: 0.078]\n",
      "3934 [D loss: (0.604)(R 0.696, F 0.513)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.248] [G acc: 0.141]\n",
      "3935 [D loss: (0.580)(R 0.573, F 0.588)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.135] [G acc: 0.156]\n",
      "3936 [D loss: (0.633)(R 0.693, F 0.573)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.242] [G acc: 0.125]\n",
      "3937 [D loss: (0.528)(R 0.546, F 0.510)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.270] [G acc: 0.141]\n",
      "3938 [D loss: (0.662)(R 0.627, F 0.696)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.107] [G acc: 0.109]\n",
      "3939 [D loss: (0.626)(R 0.598, F 0.654)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.255] [G acc: 0.078]\n",
      "3940 [D loss: (0.508)(R 0.533, F 0.482)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.251] [G acc: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3941 [D loss: (0.616)(R 0.590, F 0.642)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.081] [G acc: 0.188]\n",
      "3942 [D loss: (0.528)(R 0.580, F 0.477)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.068] [G acc: 0.188]\n",
      "3943 [D loss: (0.634)(R 0.535, F 0.733)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.236] [G acc: 0.125]\n",
      "3944 [D loss: (0.619)(R 0.633, F 0.605)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.221] [G acc: 0.172]\n",
      "3945 [D loss: (0.571)(R 0.610, F 0.532)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.167] [G acc: 0.094]\n",
      "3946 [D loss: (0.575)(R 0.507, F 0.642)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.308] [G acc: 0.078]\n",
      "3947 [D loss: (0.604)(R 0.643, F 0.565)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.285] [G acc: 0.078]\n",
      "3948 [D loss: (0.630)(R 0.660, F 0.600)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.204] [G acc: 0.188]\n",
      "3949 [D loss: (0.601)(R 0.684, F 0.518)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.127] [G acc: 0.094]\n",
      "3950 [D loss: (0.532)(R 0.507, F 0.557)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.266] [G acc: 0.109]\n",
      "3951 [D loss: (0.584)(R 0.640, F 0.528)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.222] [G acc: 0.078]\n",
      "3952 [D loss: (0.525)(R 0.529, F 0.522)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.156] [G acc: 0.203]\n",
      "3953 [D loss: (0.614)(R 0.488, F 0.740)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.195] [G acc: 0.062]\n",
      "3954 [D loss: (0.658)(R 0.729, F 0.586)] [D acc: (0.609)(0.453, 0.766)] [G loss: 1.179] [G acc: 0.125]\n",
      "3955 [D loss: (0.506)(R 0.500, F 0.511)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.190] [G acc: 0.125]\n",
      "3956 [D loss: (0.606)(R 0.509, F 0.703)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.285] [G acc: 0.141]\n",
      "3957 [D loss: (0.545)(R 0.558, F 0.533)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.261] [G acc: 0.125]\n",
      "3958 [D loss: (0.601)(R 0.631, F 0.572)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.170] [G acc: 0.203]\n",
      "3959 [D loss: (0.554)(R 0.507, F 0.602)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.236] [G acc: 0.141]\n",
      "3960 [D loss: (0.654)(R 0.692, F 0.616)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.244] [G acc: 0.094]\n",
      "3961 [D loss: (0.593)(R 0.648, F 0.538)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.158] [G acc: 0.188]\n",
      "3962 [D loss: (0.485)(R 0.406, F 0.565)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.186] [G acc: 0.156]\n",
      "3963 [D loss: (0.531)(R 0.478, F 0.585)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.270] [G acc: 0.109]\n",
      "3964 [D loss: (0.507)(R 0.548, F 0.467)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.227] [G acc: 0.141]\n",
      "3965 [D loss: (0.541)(R 0.581, F 0.502)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.155] [G acc: 0.141]\n",
      "3966 [D loss: (0.495)(R 0.465, F 0.524)] [D acc: (0.766)(0.781, 0.750)] [G loss: 1.222] [G acc: 0.141]\n",
      "3967 [D loss: (0.484)(R 0.469, F 0.500)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.347] [G acc: 0.109]\n",
      "3968 [D loss: (0.526)(R 0.430, F 0.623)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.243] [G acc: 0.109]\n",
      "3969 [D loss: (0.538)(R 0.567, F 0.509)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.346] [G acc: 0.094]\n",
      "3970 [D loss: (0.659)(R 0.727, F 0.592)] [D acc: (0.617)(0.516, 0.719)] [G loss: 1.278] [G acc: 0.109]\n",
      "3971 [D loss: (0.583)(R 0.668, F 0.498)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.166] [G acc: 0.125]\n",
      "3972 [D loss: (0.541)(R 0.531, F 0.550)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.258] [G acc: 0.141]\n",
      "3973 [D loss: (0.600)(R 0.638, F 0.563)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.207] [G acc: 0.094]\n",
      "3974 [D loss: (0.584)(R 0.595, F 0.574)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.129] [G acc: 0.172]\n",
      "3975 [D loss: (0.577)(R 0.533, F 0.620)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.242] [G acc: 0.109]\n",
      "3976 [D loss: (0.526)(R 0.448, F 0.604)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.170] [G acc: 0.172]\n",
      "3977 [D loss: (0.550)(R 0.557, F 0.543)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.145] [G acc: 0.188]\n",
      "3978 [D loss: (0.517)(R 0.450, F 0.584)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.191] [G acc: 0.109]\n",
      "3979 [D loss: (0.613)(R 0.581, F 0.644)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.283] [G acc: 0.094]\n",
      "3980 [D loss: (0.566)(R 0.622, F 0.510)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.229] [G acc: 0.078]\n",
      "3981 [D loss: (0.504)(R 0.505, F 0.503)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.456] [G acc: 0.125]\n",
      "3982 [D loss: (0.587)(R 0.640, F 0.534)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.291] [G acc: 0.094]\n",
      "3983 [D loss: (0.663)(R 0.613, F 0.713)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.314] [G acc: 0.062]\n",
      "3984 [D loss: (0.575)(R 0.667, F 0.483)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.376] [G acc: 0.062]\n",
      "3985 [D loss: (0.582)(R 0.676, F 0.488)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.411] [G acc: 0.125]\n",
      "3986 [D loss: (0.558)(R 0.550, F 0.566)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.258] [G acc: 0.062]\n",
      "3987 [D loss: (0.539)(R 0.527, F 0.551)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.307] [G acc: 0.031]\n",
      "3988 [D loss: (0.538)(R 0.609, F 0.466)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.284] [G acc: 0.109]\n",
      "3989 [D loss: (0.544)(R 0.521, F 0.567)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.127] [G acc: 0.219]\n",
      "3990 [D loss: (0.584)(R 0.465, F 0.702)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.261] [G acc: 0.094]\n",
      "3991 [D loss: (0.544)(R 0.488, F 0.600)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.293] [G acc: 0.125]\n",
      "3992 [D loss: (0.544)(R 0.567, F 0.522)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.319] [G acc: 0.047]\n",
      "3993 [D loss: (0.553)(R 0.638, F 0.468)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.221] [G acc: 0.172]\n",
      "3994 [D loss: (0.539)(R 0.570, F 0.507)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.220] [G acc: 0.125]\n",
      "3995 [D loss: (0.501)(R 0.551, F 0.450)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.325] [G acc: 0.125]\n",
      "3996 [D loss: (0.630)(R 0.511, F 0.749)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.387] [G acc: 0.125]\n",
      "3997 [D loss: (0.485)(R 0.494, F 0.477)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.311] [G acc: 0.141]\n",
      "3998 [D loss: (0.572)(R 0.637, F 0.507)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.169] [G acc: 0.172]\n",
      "3999 [D loss: (0.604)(R 0.523, F 0.686)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.284] [G acc: 0.094]\n",
      "4000 [D loss: (0.550)(R 0.534, F 0.566)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.247] [G acc: 0.031]\n",
      "4001 [D loss: (0.574)(R 0.565, F 0.582)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.205] [G acc: 0.094]\n",
      "4002 [D loss: (0.562)(R 0.592, F 0.531)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.398] [G acc: 0.062]\n",
      "4003 [D loss: (0.593)(R 0.654, F 0.532)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.255] [G acc: 0.031]\n",
      "4004 [D loss: (0.514)(R 0.614, F 0.415)] [D acc: (0.766)(0.625, 0.906)] [G loss: 1.289] [G acc: 0.125]\n",
      "4005 [D loss: (0.565)(R 0.516, F 0.614)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.217] [G acc: 0.125]\n",
      "4006 [D loss: (0.660)(R 0.622, F 0.699)] [D acc: (0.609)(0.547, 0.672)] [G loss: 1.238] [G acc: 0.094]\n",
      "4007 [D loss: (0.548)(R 0.610, F 0.485)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.389] [G acc: 0.094]\n",
      "4008 [D loss: (0.647)(R 0.741, F 0.553)] [D acc: (0.609)(0.469, 0.750)] [G loss: 1.367] [G acc: 0.109]\n",
      "4009 [D loss: (0.578)(R 0.510, F 0.646)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.224] [G acc: 0.125]\n",
      "4010 [D loss: (0.530)(R 0.643, F 0.417)] [D acc: (0.758)(0.594, 0.922)] [G loss: 1.244] [G acc: 0.094]\n",
      "4011 [D loss: (0.489)(R 0.463, F 0.514)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.210] [G acc: 0.031]\n",
      "4012 [D loss: (0.524)(R 0.447, F 0.600)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.233] [G acc: 0.141]\n",
      "4013 [D loss: (0.545)(R 0.515, F 0.575)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.281] [G acc: 0.094]\n",
      "4014 [D loss: (0.517)(R 0.541, F 0.493)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.325] [G acc: 0.094]\n",
      "4015 [D loss: (0.525)(R 0.547, F 0.504)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.330] [G acc: 0.094]\n",
      "4016 [D loss: (0.578)(R 0.488, F 0.669)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.398] [G acc: 0.031]\n",
      "4017 [D loss: (0.508)(R 0.504, F 0.511)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.314] [G acc: 0.094]\n",
      "4018 [D loss: (0.593)(R 0.556, F 0.630)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.336] [G acc: 0.078]\n",
      "4019 [D loss: (0.586)(R 0.629, F 0.543)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.216] [G acc: 0.125]\n",
      "4020 [D loss: (0.533)(R 0.541, F 0.525)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.317] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4021 [D loss: (0.528)(R 0.544, F 0.511)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.503] [G acc: 0.125]\n",
      "4022 [D loss: (0.697)(R 0.673, F 0.722)] [D acc: (0.609)(0.531, 0.688)] [G loss: 1.313] [G acc: 0.062]\n",
      "4023 [D loss: (0.554)(R 0.606, F 0.502)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.290] [G acc: 0.062]\n",
      "4024 [D loss: (0.623)(R 0.580, F 0.666)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.181] [G acc: 0.172]\n",
      "4025 [D loss: (0.546)(R 0.613, F 0.479)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.244] [G acc: 0.141]\n",
      "4026 [D loss: (0.639)(R 0.648, F 0.630)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.258] [G acc: 0.094]\n",
      "4027 [D loss: (0.674)(R 0.782, F 0.567)] [D acc: (0.633)(0.469, 0.797)] [G loss: 1.088] [G acc: 0.203]\n",
      "4028 [D loss: (0.608)(R 0.505, F 0.712)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.169] [G acc: 0.109]\n",
      "4029 [D loss: (0.551)(R 0.571, F 0.531)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.185] [G acc: 0.062]\n",
      "4030 [D loss: (0.566)(R 0.539, F 0.594)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.243] [G acc: 0.094]\n",
      "4031 [D loss: (0.605)(R 0.562, F 0.648)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.333] [G acc: 0.078]\n",
      "4032 [D loss: (0.527)(R 0.593, F 0.461)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.187] [G acc: 0.141]\n",
      "4033 [D loss: (0.642)(R 0.644, F 0.639)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.159] [G acc: 0.156]\n",
      "4034 [D loss: (0.530)(R 0.567, F 0.494)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.141] [G acc: 0.156]\n",
      "4035 [D loss: (0.583)(R 0.558, F 0.608)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.193] [G acc: 0.062]\n",
      "4036 [D loss: (0.519)(R 0.579, F 0.460)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.159] [G acc: 0.125]\n",
      "4037 [D loss: (0.574)(R 0.513, F 0.634)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.328] [G acc: 0.109]\n",
      "4038 [D loss: (0.487)(R 0.521, F 0.452)] [D acc: (0.789)(0.703, 0.875)] [G loss: 1.133] [G acc: 0.219]\n",
      "4039 [D loss: (0.443)(R 0.475, F 0.410)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.266] [G acc: 0.125]\n",
      "4040 [D loss: (0.544)(R 0.486, F 0.603)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.305] [G acc: 0.094]\n",
      "4041 [D loss: (0.543)(R 0.542, F 0.544)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.220] [G acc: 0.141]\n",
      "4042 [D loss: (0.596)(R 0.657, F 0.535)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.249] [G acc: 0.203]\n",
      "4043 [D loss: (0.568)(R 0.571, F 0.565)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.202] [G acc: 0.188]\n",
      "4044 [D loss: (0.611)(R 0.612, F 0.611)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.317] [G acc: 0.094]\n",
      "4045 [D loss: (0.595)(R 0.594, F 0.596)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.221] [G acc: 0.062]\n",
      "4046 [D loss: (0.610)(R 0.607, F 0.612)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.301] [G acc: 0.109]\n",
      "4047 [D loss: (0.552)(R 0.609, F 0.495)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.174] [G acc: 0.141]\n",
      "4048 [D loss: (0.561)(R 0.513, F 0.609)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.207] [G acc: 0.094]\n",
      "4049 [D loss: (0.522)(R 0.460, F 0.584)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.342] [G acc: 0.094]\n",
      "4050 [D loss: (0.509)(R 0.567, F 0.450)] [D acc: (0.758)(0.609, 0.906)] [G loss: 1.216] [G acc: 0.172]\n",
      "4051 [D loss: (0.560)(R 0.556, F 0.565)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.231] [G acc: 0.172]\n",
      "4052 [D loss: (0.565)(R 0.526, F 0.604)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.102] [G acc: 0.234]\n",
      "4053 [D loss: (0.613)(R 0.514, F 0.711)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.303] [G acc: 0.078]\n",
      "4054 [D loss: (0.655)(R 0.811, F 0.499)] [D acc: (0.641)(0.438, 0.844)] [G loss: 1.096] [G acc: 0.141]\n",
      "4055 [D loss: (0.570)(R 0.634, F 0.505)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.263] [G acc: 0.094]\n",
      "4056 [D loss: (0.598)(R 0.490, F 0.706)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.298] [G acc: 0.109]\n",
      "4057 [D loss: (0.627)(R 0.664, F 0.590)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.225] [G acc: 0.078]\n",
      "4058 [D loss: (0.584)(R 0.575, F 0.593)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.184] [G acc: 0.078]\n",
      "4059 [D loss: (0.586)(R 0.541, F 0.631)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.156] [G acc: 0.094]\n",
      "4060 [D loss: (0.530)(R 0.591, F 0.469)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.194] [G acc: 0.109]\n",
      "4061 [D loss: (0.629)(R 0.577, F 0.681)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.275] [G acc: 0.062]\n",
      "4062 [D loss: (0.573)(R 0.598, F 0.548)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.246] [G acc: 0.109]\n",
      "4063 [D loss: (0.543)(R 0.630, F 0.456)] [D acc: (0.742)(0.578, 0.906)] [G loss: 1.249] [G acc: 0.062]\n",
      "4064 [D loss: (0.576)(R 0.603, F 0.548)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.311] [G acc: 0.078]\n",
      "4065 [D loss: (0.507)(R 0.506, F 0.508)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.155] [G acc: 0.203]\n",
      "4066 [D loss: (0.540)(R 0.528, F 0.552)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.363] [G acc: 0.078]\n",
      "4067 [D loss: (0.536)(R 0.509, F 0.563)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.333] [G acc: 0.062]\n",
      "4068 [D loss: (0.574)(R 0.568, F 0.579)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.370] [G acc: 0.109]\n",
      "4069 [D loss: (0.582)(R 0.637, F 0.527)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.281] [G acc: 0.094]\n",
      "4070 [D loss: (0.535)(R 0.586, F 0.484)] [D acc: (0.766)(0.609, 0.922)] [G loss: 1.276] [G acc: 0.125]\n",
      "4071 [D loss: (0.607)(R 0.569, F 0.645)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.145] [G acc: 0.125]\n",
      "4072 [D loss: (0.606)(R 0.695, F 0.516)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.349] [G acc: 0.094]\n",
      "4073 [D loss: (0.591)(R 0.475, F 0.707)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.300] [G acc: 0.125]\n",
      "4074 [D loss: (0.608)(R 0.626, F 0.589)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.275] [G acc: 0.094]\n",
      "4075 [D loss: (0.577)(R 0.666, F 0.487)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.170] [G acc: 0.156]\n",
      "4076 [D loss: (0.571)(R 0.626, F 0.515)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.335] [G acc: 0.047]\n",
      "4077 [D loss: (0.550)(R 0.534, F 0.565)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.369] [G acc: 0.156]\n",
      "4078 [D loss: (0.561)(R 0.578, F 0.543)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.213] [G acc: 0.188]\n",
      "4079 [D loss: (0.523)(R 0.503, F 0.543)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.158] [G acc: 0.109]\n",
      "4080 [D loss: (0.653)(R 0.672, F 0.635)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.324] [G acc: 0.062]\n",
      "4081 [D loss: (0.481)(R 0.541, F 0.420)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.322] [G acc: 0.047]\n",
      "4082 [D loss: (0.698)(R 0.646, F 0.750)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.455] [G acc: 0.062]\n",
      "4083 [D loss: (0.555)(R 0.639, F 0.470)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.180] [G acc: 0.156]\n",
      "4084 [D loss: (0.577)(R 0.530, F 0.624)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.241] [G acc: 0.109]\n",
      "4085 [D loss: (0.497)(R 0.520, F 0.475)] [D acc: (0.781)(0.688, 0.875)] [G loss: 1.325] [G acc: 0.141]\n",
      "4086 [D loss: (0.553)(R 0.591, F 0.516)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.277] [G acc: 0.078]\n",
      "4087 [D loss: (0.604)(R 0.640, F 0.568)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.298] [G acc: 0.047]\n",
      "4088 [D loss: (0.675)(R 0.733, F 0.618)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.238] [G acc: 0.141]\n",
      "4089 [D loss: (0.542)(R 0.563, F 0.521)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.321] [G acc: 0.078]\n",
      "4090 [D loss: (0.493)(R 0.537, F 0.450)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.318] [G acc: 0.078]\n",
      "4091 [D loss: (0.595)(R 0.620, F 0.569)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.202] [G acc: 0.156]\n",
      "4092 [D loss: (0.582)(R 0.586, F 0.578)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.291] [G acc: 0.125]\n",
      "4093 [D loss: (0.656)(R 0.721, F 0.591)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.332] [G acc: 0.062]\n",
      "4094 [D loss: (0.587)(R 0.624, F 0.550)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.258] [G acc: 0.047]\n",
      "4095 [D loss: (0.603)(R 0.712, F 0.494)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.209] [G acc: 0.094]\n",
      "4096 [D loss: (0.565)(R 0.564, F 0.566)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.272] [G acc: 0.156]\n",
      "4097 [D loss: (0.553)(R 0.532, F 0.575)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.202] [G acc: 0.109]\n",
      "4098 [D loss: (0.629)(R 0.628, F 0.629)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.211] [G acc: 0.125]\n",
      "4099 [D loss: (0.617)(R 0.666, F 0.567)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.229] [G acc: 0.078]\n",
      "4100 [D loss: (0.593)(R 0.669, F 0.516)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.172] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101 [D loss: (0.626)(R 0.730, F 0.522)] [D acc: (0.586)(0.422, 0.750)] [G loss: 1.282] [G acc: 0.094]\n",
      "4102 [D loss: (0.590)(R 0.600, F 0.579)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.244] [G acc: 0.125]\n",
      "4103 [D loss: (0.620)(R 0.625, F 0.615)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.115] [G acc: 0.141]\n",
      "4104 [D loss: (0.560)(R 0.600, F 0.519)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.153] [G acc: 0.109]\n",
      "4105 [D loss: (0.576)(R 0.563, F 0.589)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.243] [G acc: 0.125]\n",
      "4106 [D loss: (0.550)(R 0.527, F 0.573)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.169] [G acc: 0.125]\n",
      "4107 [D loss: (0.522)(R 0.532, F 0.513)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.218] [G acc: 0.109]\n",
      "4108 [D loss: (0.579)(R 0.509, F 0.649)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.269] [G acc: 0.141]\n",
      "4109 [D loss: (0.549)(R 0.546, F 0.552)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.263] [G acc: 0.156]\n",
      "4110 [D loss: (0.684)(R 0.615, F 0.752)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.230] [G acc: 0.109]\n",
      "4111 [D loss: (0.630)(R 0.740, F 0.520)] [D acc: (0.648)(0.484, 0.812)] [G loss: 1.224] [G acc: 0.062]\n",
      "4112 [D loss: (0.605)(R 0.594, F 0.616)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.144] [G acc: 0.188]\n",
      "4113 [D loss: (0.553)(R 0.590, F 0.517)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.165] [G acc: 0.109]\n",
      "4114 [D loss: (0.661)(R 0.679, F 0.643)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.119] [G acc: 0.172]\n",
      "4115 [D loss: (0.576)(R 0.616, F 0.536)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.153] [G acc: 0.156]\n",
      "4116 [D loss: (0.637)(R 0.650, F 0.625)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.237] [G acc: 0.141]\n",
      "4117 [D loss: (0.605)(R 0.551, F 0.658)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.261] [G acc: 0.125]\n",
      "4118 [D loss: (0.539)(R 0.483, F 0.596)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.201] [G acc: 0.125]\n",
      "4119 [D loss: (0.577)(R 0.638, F 0.516)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.307] [G acc: 0.109]\n",
      "4120 [D loss: (0.568)(R 0.512, F 0.624)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.358] [G acc: 0.078]\n",
      "4121 [D loss: (0.529)(R 0.580, F 0.477)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.267] [G acc: 0.141]\n",
      "4122 [D loss: (0.518)(R 0.478, F 0.559)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.303] [G acc: 0.125]\n",
      "4123 [D loss: (0.518)(R 0.579, F 0.457)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.271] [G acc: 0.156]\n",
      "4124 [D loss: (0.710)(R 0.540, F 0.879)] [D acc: (0.664)(0.703, 0.625)] [G loss: 1.304] [G acc: 0.078]\n",
      "4125 [D loss: (0.590)(R 0.572, F 0.609)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.160] [G acc: 0.125]\n",
      "4126 [D loss: (0.549)(R 0.582, F 0.515)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.264] [G acc: 0.234]\n",
      "4127 [D loss: (0.651)(R 0.674, F 0.627)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.239] [G acc: 0.094]\n",
      "4128 [D loss: (0.607)(R 0.500, F 0.714)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.202] [G acc: 0.125]\n",
      "4129 [D loss: (0.548)(R 0.533, F 0.564)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.373] [G acc: 0.062]\n",
      "4130 [D loss: (0.564)(R 0.557, F 0.571)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.280] [G acc: 0.125]\n",
      "4131 [D loss: (0.610)(R 0.605, F 0.614)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.165] [G acc: 0.172]\n",
      "4132 [D loss: (0.608)(R 0.598, F 0.618)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.359] [G acc: 0.141]\n",
      "4133 [D loss: (0.539)(R 0.592, F 0.486)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.347] [G acc: 0.062]\n",
      "4134 [D loss: (0.580)(R 0.558, F 0.602)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.394] [G acc: 0.125]\n",
      "4135 [D loss: (0.571)(R 0.622, F 0.520)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.241] [G acc: 0.109]\n",
      "4136 [D loss: (0.601)(R 0.673, F 0.529)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.108] [G acc: 0.188]\n",
      "4137 [D loss: (0.481)(R 0.441, F 0.522)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.266] [G acc: 0.094]\n",
      "4138 [D loss: (0.595)(R 0.585, F 0.605)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.344] [G acc: 0.109]\n",
      "4139 [D loss: (0.549)(R 0.482, F 0.616)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.274] [G acc: 0.062]\n",
      "4140 [D loss: (0.634)(R 0.729, F 0.539)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.318] [G acc: 0.078]\n",
      "4141 [D loss: (0.545)(R 0.633, F 0.458)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.330] [G acc: 0.062]\n",
      "4142 [D loss: (0.592)(R 0.625, F 0.559)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.306] [G acc: 0.109]\n",
      "4143 [D loss: (0.494)(R 0.470, F 0.518)] [D acc: (0.797)(0.734, 0.859)] [G loss: 1.339] [G acc: 0.141]\n",
      "4144 [D loss: (0.505)(R 0.570, F 0.440)] [D acc: (0.828)(0.734, 0.922)] [G loss: 1.556] [G acc: 0.125]\n",
      "4145 [D loss: (0.623)(R 0.533, F 0.714)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.145] [G acc: 0.281]\n",
      "4146 [D loss: (0.616)(R 0.639, F 0.592)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.396] [G acc: 0.078]\n",
      "4147 [D loss: (0.537)(R 0.676, F 0.398)] [D acc: (0.773)(0.594, 0.953)] [G loss: 1.322] [G acc: 0.078]\n",
      "4148 [D loss: (0.616)(R 0.609, F 0.623)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.222] [G acc: 0.141]\n",
      "4149 [D loss: (0.518)(R 0.577, F 0.458)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.252] [G acc: 0.141]\n",
      "4150 [D loss: (0.579)(R 0.588, F 0.571)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.209] [G acc: 0.141]\n",
      "4151 [D loss: (0.672)(R 0.677, F 0.666)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.205] [G acc: 0.156]\n",
      "4152 [D loss: (0.594)(R 0.611, F 0.578)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.259] [G acc: 0.062]\n",
      "4153 [D loss: (0.633)(R 0.650, F 0.616)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.235] [G acc: 0.172]\n",
      "4154 [D loss: (0.508)(R 0.509, F 0.506)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.180] [G acc: 0.156]\n",
      "4155 [D loss: (0.572)(R 0.562, F 0.581)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.269] [G acc: 0.172]\n",
      "4156 [D loss: (0.529)(R 0.530, F 0.527)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.292] [G acc: 0.172]\n",
      "4157 [D loss: (0.555)(R 0.570, F 0.539)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.212] [G acc: 0.141]\n",
      "4158 [D loss: (0.623)(R 0.673, F 0.572)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.163] [G acc: 0.188]\n",
      "4159 [D loss: (0.510)(R 0.459, F 0.561)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.238] [G acc: 0.109]\n",
      "4160 [D loss: (0.603)(R 0.557, F 0.650)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.256] [G acc: 0.156]\n",
      "4161 [D loss: (0.591)(R 0.551, F 0.630)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.122] [G acc: 0.125]\n",
      "4162 [D loss: (0.540)(R 0.588, F 0.492)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.225] [G acc: 0.094]\n",
      "4163 [D loss: (0.571)(R 0.536, F 0.607)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.184] [G acc: 0.156]\n",
      "4164 [D loss: (0.532)(R 0.554, F 0.509)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.208] [G acc: 0.125]\n",
      "4165 [D loss: (0.599)(R 0.560, F 0.638)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.299] [G acc: 0.125]\n",
      "4166 [D loss: (0.497)(R 0.522, F 0.473)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.253] [G acc: 0.141]\n",
      "4167 [D loss: (0.518)(R 0.495, F 0.541)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.260] [G acc: 0.062]\n",
      "4168 [D loss: (0.583)(R 0.606, F 0.560)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.311] [G acc: 0.078]\n",
      "4169 [D loss: (0.626)(R 0.587, F 0.664)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.208] [G acc: 0.188]\n",
      "4170 [D loss: (0.599)(R 0.555, F 0.642)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.109] [G acc: 0.125]\n",
      "4171 [D loss: (0.524)(R 0.483, F 0.564)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.189] [G acc: 0.094]\n",
      "4172 [D loss: (0.592)(R 0.616, F 0.567)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.380] [G acc: 0.016]\n",
      "4173 [D loss: (0.578)(R 0.604, F 0.553)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.185] [G acc: 0.094]\n",
      "4174 [D loss: (0.628)(R 0.608, F 0.647)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.327] [G acc: 0.062]\n",
      "4175 [D loss: (0.533)(R 0.563, F 0.502)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.284] [G acc: 0.125]\n",
      "4176 [D loss: (0.625)(R 0.599, F 0.650)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.174] [G acc: 0.141]\n",
      "4177 [D loss: (0.646)(R 0.759, F 0.533)] [D acc: (0.648)(0.453, 0.844)] [G loss: 1.132] [G acc: 0.125]\n",
      "4178 [D loss: (0.580)(R 0.551, F 0.610)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.149] [G acc: 0.141]\n",
      "4179 [D loss: (0.554)(R 0.576, F 0.532)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.150] [G acc: 0.141]\n",
      "4180 [D loss: (0.511)(R 0.504, F 0.517)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.434] [G acc: 0.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4181 [D loss: (0.576)(R 0.482, F 0.671)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.206] [G acc: 0.109]\n",
      "4182 [D loss: (0.513)(R 0.566, F 0.460)] [D acc: (0.750)(0.609, 0.891)] [G loss: 1.160] [G acc: 0.141]\n",
      "4183 [D loss: (0.659)(R 0.622, F 0.696)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.166] [G acc: 0.125]\n",
      "4184 [D loss: (0.561)(R 0.514, F 0.608)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.192] [G acc: 0.078]\n",
      "4185 [D loss: (0.653)(R 0.593, F 0.712)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.201] [G acc: 0.109]\n",
      "4186 [D loss: (0.572)(R 0.616, F 0.528)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.124] [G acc: 0.125]\n",
      "4187 [D loss: (0.518)(R 0.603, F 0.433)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.173] [G acc: 0.156]\n",
      "4188 [D loss: (0.517)(R 0.542, F 0.491)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.174] [G acc: 0.125]\n",
      "4189 [D loss: (0.536)(R 0.503, F 0.569)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.176] [G acc: 0.094]\n",
      "4190 [D loss: (0.615)(R 0.512, F 0.718)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.172] [G acc: 0.078]\n",
      "4191 [D loss: (0.620)(R 0.696, F 0.544)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.169] [G acc: 0.094]\n",
      "4192 [D loss: (0.556)(R 0.524, F 0.588)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.253] [G acc: 0.109]\n",
      "4193 [D loss: (0.518)(R 0.550, F 0.487)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.259] [G acc: 0.078]\n",
      "4194 [D loss: (0.555)(R 0.505, F 0.606)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.434] [G acc: 0.016]\n",
      "4195 [D loss: (0.692)(R 0.660, F 0.724)] [D acc: (0.594)(0.484, 0.703)] [G loss: 1.256] [G acc: 0.078]\n",
      "4196 [D loss: (0.582)(R 0.680, F 0.483)] [D acc: (0.672)(0.469, 0.875)] [G loss: 1.267] [G acc: 0.109]\n",
      "4197 [D loss: (0.546)(R 0.601, F 0.491)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.218] [G acc: 0.109]\n",
      "4198 [D loss: (0.523)(R 0.528, F 0.518)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.302] [G acc: 0.078]\n",
      "4199 [D loss: (0.433)(R 0.453, F 0.413)] [D acc: (0.828)(0.719, 0.938)] [G loss: 1.266] [G acc: 0.141]\n",
      "4200 [D loss: (0.557)(R 0.423, F 0.691)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.358] [G acc: 0.047]\n",
      "4201 [D loss: (0.581)(R 0.574, F 0.589)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.336] [G acc: 0.062]\n",
      "4202 [D loss: (0.580)(R 0.577, F 0.583)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.182] [G acc: 0.156]\n",
      "4203 [D loss: (0.572)(R 0.653, F 0.491)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.276] [G acc: 0.109]\n",
      "4204 [D loss: (0.570)(R 0.588, F 0.552)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.227] [G acc: 0.094]\n",
      "4205 [D loss: (0.577)(R 0.561, F 0.593)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.257] [G acc: 0.125]\n",
      "4206 [D loss: (0.502)(R 0.475, F 0.529)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.132] [G acc: 0.141]\n",
      "4207 [D loss: (0.630)(R 0.561, F 0.700)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.288] [G acc: 0.109]\n",
      "4208 [D loss: (0.549)(R 0.547, F 0.552)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.267] [G acc: 0.078]\n",
      "4209 [D loss: (0.555)(R 0.480, F 0.629)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.224] [G acc: 0.078]\n",
      "4210 [D loss: (0.536)(R 0.560, F 0.512)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.086] [G acc: 0.125]\n",
      "4211 [D loss: (0.593)(R 0.537, F 0.650)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.202] [G acc: 0.156]\n",
      "4212 [D loss: (0.544)(R 0.563, F 0.526)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.317] [G acc: 0.125]\n",
      "4213 [D loss: (0.601)(R 0.587, F 0.616)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.243] [G acc: 0.078]\n",
      "4214 [D loss: (0.571)(R 0.636, F 0.506)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.272] [G acc: 0.125]\n",
      "4215 [D loss: (0.606)(R 0.641, F 0.571)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.192] [G acc: 0.141]\n",
      "4216 [D loss: (0.552)(R 0.497, F 0.607)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.231] [G acc: 0.062]\n",
      "4217 [D loss: (0.556)(R 0.592, F 0.521)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.187] [G acc: 0.078]\n",
      "4218 [D loss: (0.536)(R 0.484, F 0.588)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.305] [G acc: 0.047]\n",
      "4219 [D loss: (0.611)(R 0.683, F 0.540)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.226] [G acc: 0.156]\n",
      "4220 [D loss: (0.553)(R 0.571, F 0.535)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.293] [G acc: 0.078]\n",
      "4221 [D loss: (0.555)(R 0.547, F 0.563)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.316] [G acc: 0.078]\n",
      "4222 [D loss: (0.561)(R 0.585, F 0.537)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.432] [G acc: 0.078]\n",
      "4223 [D loss: (0.598)(R 0.652, F 0.545)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.249] [G acc: 0.078]\n",
      "4224 [D loss: (0.526)(R 0.561, F 0.492)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.332] [G acc: 0.109]\n",
      "4225 [D loss: (0.453)(R 0.407, F 0.498)] [D acc: (0.812)(0.781, 0.844)] [G loss: 1.296] [G acc: 0.141]\n",
      "4226 [D loss: (0.557)(R 0.546, F 0.568)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.332] [G acc: 0.094]\n",
      "4227 [D loss: (0.508)(R 0.541, F 0.476)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.475] [G acc: 0.078]\n",
      "4228 [D loss: (0.646)(R 0.630, F 0.663)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.420] [G acc: 0.047]\n",
      "4229 [D loss: (0.584)(R 0.601, F 0.567)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.290] [G acc: 0.031]\n",
      "4230 [D loss: (0.582)(R 0.623, F 0.541)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.270] [G acc: 0.156]\n",
      "4231 [D loss: (0.516)(R 0.596, F 0.437)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.360] [G acc: 0.094]\n",
      "4232 [D loss: (0.621)(R 0.576, F 0.666)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.296] [G acc: 0.141]\n",
      "4233 [D loss: (0.500)(R 0.477, F 0.522)] [D acc: (0.812)(0.719, 0.906)] [G loss: 1.233] [G acc: 0.125]\n",
      "4234 [D loss: (0.587)(R 0.646, F 0.529)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.346] [G acc: 0.078]\n",
      "4235 [D loss: (0.549)(R 0.532, F 0.566)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.339] [G acc: 0.062]\n",
      "4236 [D loss: (0.550)(R 0.481, F 0.619)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.269] [G acc: 0.125]\n",
      "4237 [D loss: (0.584)(R 0.614, F 0.553)] [D acc: (0.617)(0.516, 0.719)] [G loss: 1.185] [G acc: 0.125]\n",
      "4238 [D loss: (0.536)(R 0.589, F 0.484)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.190] [G acc: 0.109]\n",
      "4239 [D loss: (0.512)(R 0.479, F 0.546)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.255] [G acc: 0.078]\n",
      "4240 [D loss: (0.498)(R 0.545, F 0.452)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.369] [G acc: 0.078]\n",
      "4241 [D loss: (0.595)(R 0.647, F 0.543)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.341] [G acc: 0.062]\n",
      "4242 [D loss: (0.470)(R 0.449, F 0.490)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.356] [G acc: 0.062]\n",
      "4243 [D loss: (0.622)(R 0.597, F 0.647)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.234] [G acc: 0.125]\n",
      "4244 [D loss: (0.580)(R 0.643, F 0.516)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.230] [G acc: 0.141]\n",
      "4245 [D loss: (0.470)(R 0.450, F 0.491)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.188] [G acc: 0.141]\n",
      "4246 [D loss: (0.481)(R 0.417, F 0.545)] [D acc: (0.766)(0.766, 0.766)] [G loss: 1.448] [G acc: 0.062]\n",
      "4247 [D loss: (0.625)(R 0.598, F 0.652)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.300] [G acc: 0.141]\n",
      "4248 [D loss: (0.536)(R 0.558, F 0.514)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.340] [G acc: 0.109]\n",
      "4249 [D loss: (0.573)(R 0.646, F 0.501)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.276] [G acc: 0.078]\n",
      "4250 [D loss: (0.553)(R 0.472, F 0.634)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.320] [G acc: 0.062]\n",
      "4251 [D loss: (0.513)(R 0.548, F 0.477)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.261] [G acc: 0.062]\n",
      "4252 [D loss: (0.476)(R 0.488, F 0.465)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.326] [G acc: 0.109]\n",
      "4253 [D loss: (0.541)(R 0.494, F 0.588)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.401] [G acc: 0.141]\n",
      "4254 [D loss: (0.566)(R 0.548, F 0.583)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.254] [G acc: 0.125]\n",
      "4255 [D loss: (0.548)(R 0.592, F 0.504)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.310] [G acc: 0.078]\n",
      "4256 [D loss: (0.578)(R 0.578, F 0.577)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.368] [G acc: 0.062]\n",
      "4257 [D loss: (0.588)(R 0.670, F 0.506)] [D acc: (0.641)(0.453, 0.828)] [G loss: 1.430] [G acc: 0.047]\n",
      "4258 [D loss: (0.609)(R 0.739, F 0.479)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.488] [G acc: 0.109]\n",
      "4259 [D loss: (0.492)(R 0.484, F 0.499)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.524] [G acc: 0.078]\n",
      "4260 [D loss: (0.610)(R 0.603, F 0.618)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.364] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4261 [D loss: (0.647)(R 0.694, F 0.599)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.178] [G acc: 0.156]\n",
      "4262 [D loss: (0.648)(R 0.629, F 0.667)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.325] [G acc: 0.078]\n",
      "4263 [D loss: (0.645)(R 0.700, F 0.590)] [D acc: (0.664)(0.500, 0.828)] [G loss: 1.202] [G acc: 0.125]\n",
      "4264 [D loss: (0.585)(R 0.669, F 0.502)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.323] [G acc: 0.078]\n",
      "4265 [D loss: (0.657)(R 0.721, F 0.592)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.300] [G acc: 0.094]\n",
      "4266 [D loss: (0.562)(R 0.605, F 0.519)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.343] [G acc: 0.078]\n",
      "4267 [D loss: (0.548)(R 0.630, F 0.465)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.155] [G acc: 0.172]\n",
      "4268 [D loss: (0.554)(R 0.527, F 0.581)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.220] [G acc: 0.125]\n",
      "4269 [D loss: (0.603)(R 0.684, F 0.523)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.191] [G acc: 0.156]\n",
      "4270 [D loss: (0.533)(R 0.554, F 0.512)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.251] [G acc: 0.156]\n",
      "4271 [D loss: (0.629)(R 0.636, F 0.622)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.242] [G acc: 0.094]\n",
      "4272 [D loss: (0.494)(R 0.489, F 0.498)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.310] [G acc: 0.109]\n",
      "4273 [D loss: (0.520)(R 0.625, F 0.415)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.327] [G acc: 0.156]\n",
      "4274 [D loss: (0.597)(R 0.575, F 0.620)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.130] [G acc: 0.219]\n",
      "4275 [D loss: (0.550)(R 0.524, F 0.576)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.277] [G acc: 0.109]\n",
      "4276 [D loss: (0.559)(R 0.561, F 0.556)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.254] [G acc: 0.094]\n",
      "4277 [D loss: (0.560)(R 0.585, F 0.535)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.225] [G acc: 0.156]\n",
      "4278 [D loss: (0.560)(R 0.589, F 0.531)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.427] [G acc: 0.031]\n",
      "4279 [D loss: (0.666)(R 0.692, F 0.639)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.142] [G acc: 0.125]\n",
      "4280 [D loss: (0.564)(R 0.631, F 0.496)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.237] [G acc: 0.125]\n",
      "4281 [D loss: (0.548)(R 0.612, F 0.484)] [D acc: (0.742)(0.594, 0.891)] [G loss: 1.225] [G acc: 0.125]\n",
      "4282 [D loss: (0.507)(R 0.493, F 0.522)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.275] [G acc: 0.141]\n",
      "4283 [D loss: (0.533)(R 0.467, F 0.600)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.235] [G acc: 0.109]\n",
      "4284 [D loss: (0.581)(R 0.666, F 0.495)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.258] [G acc: 0.094]\n",
      "4285 [D loss: (0.570)(R 0.645, F 0.495)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.318] [G acc: 0.094]\n",
      "4286 [D loss: (0.612)(R 0.567, F 0.657)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.284] [G acc: 0.078]\n",
      "4287 [D loss: (0.567)(R 0.599, F 0.535)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.129] [G acc: 0.141]\n",
      "4288 [D loss: (0.582)(R 0.491, F 0.674)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.355] [G acc: 0.016]\n",
      "4289 [D loss: (0.522)(R 0.549, F 0.494)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.174] [G acc: 0.109]\n",
      "4290 [D loss: (0.607)(R 0.573, F 0.641)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.419] [G acc: 0.031]\n",
      "4291 [D loss: (0.586)(R 0.601, F 0.570)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.240] [G acc: 0.172]\n",
      "4292 [D loss: (0.564)(R 0.692, F 0.436)] [D acc: (0.680)(0.484, 0.875)] [G loss: 1.250] [G acc: 0.172]\n",
      "4293 [D loss: (0.564)(R 0.630, F 0.499)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.259] [G acc: 0.125]\n",
      "4294 [D loss: (0.572)(R 0.596, F 0.548)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.398] [G acc: 0.109]\n",
      "4295 [D loss: (0.622)(R 0.605, F 0.639)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.235] [G acc: 0.109]\n",
      "4296 [D loss: (0.611)(R 0.634, F 0.589)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.212] [G acc: 0.078]\n",
      "4297 [D loss: (0.592)(R 0.668, F 0.516)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.126] [G acc: 0.188]\n",
      "4298 [D loss: (0.562)(R 0.597, F 0.528)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.045] [G acc: 0.172]\n",
      "4299 [D loss: (0.545)(R 0.601, F 0.489)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.300] [G acc: 0.141]\n",
      "4300 [D loss: (0.582)(R 0.483, F 0.681)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.332] [G acc: 0.047]\n",
      "4301 [D loss: (0.527)(R 0.473, F 0.582)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.269] [G acc: 0.078]\n",
      "4302 [D loss: (0.553)(R 0.533, F 0.573)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.249] [G acc: 0.078]\n",
      "4303 [D loss: (0.589)(R 0.629, F 0.550)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.349] [G acc: 0.094]\n",
      "4304 [D loss: (0.505)(R 0.485, F 0.524)] [D acc: (0.789)(0.703, 0.875)] [G loss: 1.277] [G acc: 0.078]\n",
      "4305 [D loss: (0.583)(R 0.635, F 0.531)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.175] [G acc: 0.125]\n",
      "4306 [D loss: (0.589)(R 0.519, F 0.659)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.309] [G acc: 0.047]\n",
      "4307 [D loss: (0.561)(R 0.594, F 0.529)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.377] [G acc: 0.141]\n",
      "4308 [D loss: (0.666)(R 0.754, F 0.579)] [D acc: (0.602)(0.453, 0.750)] [G loss: 1.141] [G acc: 0.141]\n",
      "4309 [D loss: (0.526)(R 0.550, F 0.502)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.210] [G acc: 0.062]\n",
      "4310 [D loss: (0.563)(R 0.565, F 0.561)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.213] [G acc: 0.125]\n",
      "4311 [D loss: (0.533)(R 0.543, F 0.523)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.278] [G acc: 0.094]\n",
      "4312 [D loss: (0.601)(R 0.622, F 0.580)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.338] [G acc: 0.109]\n",
      "4313 [D loss: (0.574)(R 0.568, F 0.579)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.257] [G acc: 0.172]\n",
      "4314 [D loss: (0.659)(R 0.689, F 0.629)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.326] [G acc: 0.156]\n",
      "4315 [D loss: (0.498)(R 0.553, F 0.444)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.305] [G acc: 0.156]\n",
      "4316 [D loss: (0.658)(R 0.667, F 0.649)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.154] [G acc: 0.141]\n",
      "4317 [D loss: (0.516)(R 0.552, F 0.479)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.221] [G acc: 0.172]\n",
      "4318 [D loss: (0.507)(R 0.462, F 0.552)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.203] [G acc: 0.047]\n",
      "4319 [D loss: (0.484)(R 0.449, F 0.520)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.233] [G acc: 0.188]\n",
      "4320 [D loss: (0.558)(R 0.488, F 0.628)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.257] [G acc: 0.109]\n",
      "4321 [D loss: (0.592)(R 0.628, F 0.556)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.416] [G acc: 0.047]\n",
      "4322 [D loss: (0.607)(R 0.670, F 0.544)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.311] [G acc: 0.062]\n",
      "4323 [D loss: (0.577)(R 0.570, F 0.584)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.161] [G acc: 0.156]\n",
      "4324 [D loss: (0.572)(R 0.643, F 0.500)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.260] [G acc: 0.125]\n",
      "4325 [D loss: (0.592)(R 0.619, F 0.566)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.258] [G acc: 0.156]\n",
      "4326 [D loss: (0.483)(R 0.354, F 0.612)] [D acc: (0.828)(0.844, 0.812)] [G loss: 1.315] [G acc: 0.141]\n",
      "4327 [D loss: (0.627)(R 0.545, F 0.709)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.333] [G acc: 0.141]\n",
      "4328 [D loss: (0.580)(R 0.654, F 0.506)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.203] [G acc: 0.172]\n",
      "4329 [D loss: (0.603)(R 0.616, F 0.590)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.178] [G acc: 0.188]\n",
      "4330 [D loss: (0.550)(R 0.468, F 0.632)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.309] [G acc: 0.109]\n",
      "4331 [D loss: (0.505)(R 0.514, F 0.497)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.167] [G acc: 0.156]\n",
      "4332 [D loss: (0.494)(R 0.474, F 0.514)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.282] [G acc: 0.078]\n",
      "4333 [D loss: (0.539)(R 0.571, F 0.507)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.409] [G acc: 0.078]\n",
      "4334 [D loss: (0.568)(R 0.500, F 0.637)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.287] [G acc: 0.109]\n",
      "4335 [D loss: (0.549)(R 0.578, F 0.520)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.318] [G acc: 0.109]\n",
      "4336 [D loss: (0.526)(R 0.618, F 0.434)] [D acc: (0.742)(0.594, 0.891)] [G loss: 1.282] [G acc: 0.141]\n",
      "4337 [D loss: (0.607)(R 0.500, F 0.714)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.300] [G acc: 0.078]\n",
      "4338 [D loss: (0.620)(R 0.749, F 0.490)] [D acc: (0.641)(0.453, 0.828)] [G loss: 1.190] [G acc: 0.094]\n",
      "4339 [D loss: (0.617)(R 0.694, F 0.539)] [D acc: (0.633)(0.484, 0.781)] [G loss: 1.346] [G acc: 0.094]\n",
      "4340 [D loss: (0.579)(R 0.671, F 0.488)] [D acc: (0.672)(0.500, 0.844)] [G loss: 1.217] [G acc: 0.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4341 [D loss: (0.537)(R 0.533, F 0.540)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.262] [G acc: 0.094]\n",
      "4342 [D loss: (0.593)(R 0.623, F 0.564)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.345] [G acc: 0.078]\n",
      "4343 [D loss: (0.637)(R 0.692, F 0.581)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.303] [G acc: 0.031]\n",
      "4344 [D loss: (0.518)(R 0.550, F 0.487)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.244] [G acc: 0.109]\n",
      "4345 [D loss: (0.565)(R 0.578, F 0.553)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.270] [G acc: 0.109]\n",
      "4346 [D loss: (0.538)(R 0.587, F 0.488)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.246] [G acc: 0.141]\n",
      "4347 [D loss: (0.588)(R 0.496, F 0.680)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.253] [G acc: 0.078]\n",
      "4348 [D loss: (0.582)(R 0.593, F 0.572)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.217] [G acc: 0.125]\n",
      "4349 [D loss: (0.551)(R 0.587, F 0.515)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.516] [G acc: 0.047]\n",
      "4350 [D loss: (0.558)(R 0.601, F 0.515)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.319] [G acc: 0.078]\n",
      "4351 [D loss: (0.633)(R 0.548, F 0.718)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.203] [G acc: 0.094]\n",
      "4352 [D loss: (0.530)(R 0.629, F 0.430)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.346] [G acc: 0.125]\n",
      "4353 [D loss: (0.523)(R 0.498, F 0.549)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.236] [G acc: 0.125]\n",
      "4354 [D loss: (0.574)(R 0.634, F 0.514)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.345] [G acc: 0.109]\n",
      "4355 [D loss: (0.569)(R 0.589, F 0.549)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.256] [G acc: 0.078]\n",
      "4356 [D loss: (0.464)(R 0.426, F 0.502)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.173] [G acc: 0.172]\n",
      "4357 [D loss: (0.545)(R 0.587, F 0.503)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.323] [G acc: 0.094]\n",
      "4358 [D loss: (0.503)(R 0.574, F 0.433)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.312] [G acc: 0.141]\n",
      "4359 [D loss: (0.571)(R 0.523, F 0.618)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.303] [G acc: 0.078]\n",
      "4360 [D loss: (0.592)(R 0.641, F 0.543)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.132] [G acc: 0.141]\n",
      "4361 [D loss: (0.556)(R 0.553, F 0.559)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.309] [G acc: 0.078]\n",
      "4362 [D loss: (0.585)(R 0.661, F 0.509)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.339] [G acc: 0.141]\n",
      "4363 [D loss: (0.565)(R 0.473, F 0.657)] [D acc: (0.734)(0.766, 0.703)] [G loss: 1.282] [G acc: 0.047]\n",
      "4364 [D loss: (0.594)(R 0.607, F 0.580)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.250] [G acc: 0.125]\n",
      "4365 [D loss: (0.563)(R 0.563, F 0.564)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.256] [G acc: 0.125]\n",
      "4366 [D loss: (0.509)(R 0.558, F 0.460)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.242] [G acc: 0.156]\n",
      "4367 [D loss: (0.557)(R 0.597, F 0.516)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.322] [G acc: 0.125]\n",
      "4368 [D loss: (0.479)(R 0.426, F 0.533)] [D acc: (0.812)(0.766, 0.859)] [G loss: 1.322] [G acc: 0.094]\n",
      "4369 [D loss: (0.600)(R 0.623, F 0.577)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.255] [G acc: 0.125]\n",
      "4370 [D loss: (0.496)(R 0.535, F 0.456)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.318] [G acc: 0.125]\n",
      "4371 [D loss: (0.584)(R 0.536, F 0.633)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.327] [G acc: 0.062]\n",
      "4372 [D loss: (0.580)(R 0.566, F 0.594)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.246] [G acc: 0.078]\n",
      "4373 [D loss: (0.594)(R 0.653, F 0.535)] [D acc: (0.688)(0.516, 0.859)] [G loss: 1.169] [G acc: 0.109]\n",
      "4374 [D loss: (0.523)(R 0.571, F 0.474)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.174] [G acc: 0.141]\n",
      "4375 [D loss: (0.531)(R 0.566, F 0.496)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.213] [G acc: 0.125]\n",
      "4376 [D loss: (0.547)(R 0.532, F 0.561)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.334] [G acc: 0.125]\n",
      "4377 [D loss: (0.570)(R 0.556, F 0.585)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.303] [G acc: 0.062]\n",
      "4378 [D loss: (0.533)(R 0.504, F 0.562)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.272] [G acc: 0.141]\n",
      "4379 [D loss: (0.523)(R 0.473, F 0.573)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.252] [G acc: 0.062]\n",
      "4380 [D loss: (0.575)(R 0.526, F 0.624)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.217] [G acc: 0.125]\n",
      "4381 [D loss: (0.495)(R 0.465, F 0.524)] [D acc: (0.812)(0.781, 0.844)] [G loss: 1.222] [G acc: 0.141]\n",
      "4382 [D loss: (0.553)(R 0.571, F 0.536)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.404] [G acc: 0.062]\n",
      "4383 [D loss: (0.491)(R 0.525, F 0.457)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.398] [G acc: 0.094]\n",
      "4384 [D loss: (0.753)(R 0.555, F 0.950)] [D acc: (0.625)(0.656, 0.594)] [G loss: 1.234] [G acc: 0.047]\n",
      "4385 [D loss: (0.545)(R 0.651, F 0.439)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.189] [G acc: 0.094]\n",
      "4386 [D loss: (0.535)(R 0.558, F 0.511)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.153] [G acc: 0.156]\n",
      "4387 [D loss: (0.570)(R 0.566, F 0.573)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.208] [G acc: 0.125]\n",
      "4388 [D loss: (0.482)(R 0.493, F 0.472)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.378] [G acc: 0.141]\n",
      "4389 [D loss: (0.693)(R 0.577, F 0.810)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.312] [G acc: 0.047]\n",
      "4390 [D loss: (0.608)(R 0.717, F 0.499)] [D acc: (0.648)(0.484, 0.812)] [G loss: 1.172] [G acc: 0.141]\n",
      "4391 [D loss: (0.562)(R 0.608, F 0.516)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.170] [G acc: 0.172]\n",
      "4392 [D loss: (0.492)(R 0.459, F 0.525)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.147] [G acc: 0.156]\n",
      "4393 [D loss: (0.574)(R 0.532, F 0.615)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.277] [G acc: 0.125]\n",
      "4394 [D loss: (0.669)(R 0.689, F 0.649)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.290] [G acc: 0.125]\n",
      "4395 [D loss: (0.568)(R 0.661, F 0.475)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.243] [G acc: 0.078]\n",
      "4396 [D loss: (0.555)(R 0.589, F 0.520)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.325] [G acc: 0.109]\n",
      "4397 [D loss: (0.546)(R 0.479, F 0.613)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.296] [G acc: 0.078]\n",
      "4398 [D loss: (0.564)(R 0.591, F 0.538)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.180] [G acc: 0.188]\n",
      "4399 [D loss: (0.521)(R 0.520, F 0.522)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.388] [G acc: 0.125]\n",
      "4400 [D loss: (0.571)(R 0.610, F 0.532)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.329] [G acc: 0.031]\n",
      "4401 [D loss: (0.554)(R 0.507, F 0.600)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.270] [G acc: 0.156]\n",
      "4402 [D loss: (0.529)(R 0.484, F 0.573)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.266] [G acc: 0.141]\n",
      "4403 [D loss: (0.514)(R 0.522, F 0.507)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.334] [G acc: 0.109]\n",
      "4404 [D loss: (0.519)(R 0.521, F 0.517)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.433] [G acc: 0.156]\n",
      "4405 [D loss: (0.555)(R 0.609, F 0.501)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.239] [G acc: 0.078]\n",
      "4406 [D loss: (0.576)(R 0.514, F 0.638)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.315] [G acc: 0.094]\n",
      "4407 [D loss: (0.588)(R 0.624, F 0.553)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.368] [G acc: 0.047]\n",
      "4408 [D loss: (0.487)(R 0.499, F 0.475)] [D acc: (0.812)(0.734, 0.891)] [G loss: 1.325] [G acc: 0.125]\n",
      "4409 [D loss: (0.501)(R 0.465, F 0.536)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.279] [G acc: 0.125]\n",
      "4410 [D loss: (0.604)(R 0.659, F 0.549)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.295] [G acc: 0.188]\n",
      "4411 [D loss: (0.563)(R 0.602, F 0.524)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.260] [G acc: 0.156]\n",
      "4412 [D loss: (0.608)(R 0.617, F 0.599)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.273] [G acc: 0.094]\n",
      "4413 [D loss: (0.593)(R 0.670, F 0.515)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.244] [G acc: 0.141]\n",
      "4414 [D loss: (0.598)(R 0.590, F 0.605)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.320] [G acc: 0.125]\n",
      "4415 [D loss: (0.521)(R 0.538, F 0.505)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.399] [G acc: 0.109]\n",
      "4416 [D loss: (0.549)(R 0.488, F 0.610)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.313] [G acc: 0.078]\n",
      "4417 [D loss: (0.611)(R 0.745, F 0.478)] [D acc: (0.648)(0.453, 0.844)] [G loss: 1.241] [G acc: 0.125]\n",
      "4418 [D loss: (0.599)(R 0.604, F 0.593)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.300] [G acc: 0.047]\n",
      "4419 [D loss: (0.590)(R 0.623, F 0.557)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.339] [G acc: 0.078]\n",
      "4420 [D loss: (0.652)(R 0.602, F 0.703)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.269] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4421 [D loss: (0.531)(R 0.551, F 0.512)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.153] [G acc: 0.156]\n",
      "4422 [D loss: (0.515)(R 0.487, F 0.544)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.256] [G acc: 0.125]\n",
      "4423 [D loss: (0.551)(R 0.575, F 0.527)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.272] [G acc: 0.094]\n",
      "4424 [D loss: (0.609)(R 0.656, F 0.561)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.195] [G acc: 0.172]\n",
      "4425 [D loss: (0.516)(R 0.559, F 0.473)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.194] [G acc: 0.125]\n",
      "4426 [D loss: (0.684)(R 0.604, F 0.763)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.249] [G acc: 0.078]\n",
      "4427 [D loss: (0.638)(R 0.737, F 0.538)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.219] [G acc: 0.125]\n",
      "4428 [D loss: (0.540)(R 0.519, F 0.561)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.223] [G acc: 0.078]\n",
      "4429 [D loss: (0.566)(R 0.647, F 0.485)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.181] [G acc: 0.141]\n",
      "4430 [D loss: (0.597)(R 0.622, F 0.572)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.160] [G acc: 0.156]\n",
      "4431 [D loss: (0.555)(R 0.591, F 0.519)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.292] [G acc: 0.062]\n",
      "4432 [D loss: (0.538)(R 0.547, F 0.529)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.168] [G acc: 0.172]\n",
      "4433 [D loss: (0.614)(R 0.583, F 0.645)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.084] [G acc: 0.219]\n",
      "4434 [D loss: (0.621)(R 0.722, F 0.519)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.179] [G acc: 0.125]\n",
      "4435 [D loss: (0.655)(R 0.525, F 0.785)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.120] [G acc: 0.062]\n",
      "4436 [D loss: (0.607)(R 0.737, F 0.476)] [D acc: (0.656)(0.453, 0.859)] [G loss: 1.135] [G acc: 0.125]\n",
      "4437 [D loss: (0.525)(R 0.544, F 0.505)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.195] [G acc: 0.109]\n",
      "4438 [D loss: (0.533)(R 0.472, F 0.593)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.091] [G acc: 0.188]\n",
      "4439 [D loss: (0.634)(R 0.585, F 0.684)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.185] [G acc: 0.094]\n",
      "4440 [D loss: (0.565)(R 0.615, F 0.514)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.262] [G acc: 0.062]\n",
      "4441 [D loss: (0.527)(R 0.520, F 0.534)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.328] [G acc: 0.094]\n",
      "4442 [D loss: (0.471)(R 0.501, F 0.442)] [D acc: (0.797)(0.703, 0.891)] [G loss: 1.310] [G acc: 0.109]\n",
      "4443 [D loss: (0.663)(R 0.659, F 0.668)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.218] [G acc: 0.141]\n",
      "4444 [D loss: (0.680)(R 0.568, F 0.791)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.254] [G acc: 0.047]\n",
      "4445 [D loss: (0.570)(R 0.620, F 0.520)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.158] [G acc: 0.125]\n",
      "4446 [D loss: (0.555)(R 0.533, F 0.577)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.319] [G acc: 0.078]\n",
      "4447 [D loss: (0.574)(R 0.608, F 0.540)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.158] [G acc: 0.047]\n",
      "4448 [D loss: (0.511)(R 0.398, F 0.624)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.271] [G acc: 0.062]\n",
      "4449 [D loss: (0.584)(R 0.646, F 0.522)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.323] [G acc: 0.031]\n",
      "4450 [D loss: (0.588)(R 0.694, F 0.481)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.310] [G acc: 0.062]\n",
      "4451 [D loss: (0.550)(R 0.530, F 0.569)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.268] [G acc: 0.109]\n",
      "4452 [D loss: (0.620)(R 0.631, F 0.608)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.207] [G acc: 0.094]\n",
      "4453 [D loss: (0.511)(R 0.535, F 0.487)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.228] [G acc: 0.156]\n",
      "4454 [D loss: (0.597)(R 0.634, F 0.560)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.147] [G acc: 0.156]\n",
      "4455 [D loss: (0.609)(R 0.555, F 0.663)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.258] [G acc: 0.109]\n",
      "4456 [D loss: (0.578)(R 0.645, F 0.512)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.138] [G acc: 0.125]\n",
      "4457 [D loss: (0.611)(R 0.584, F 0.638)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.223] [G acc: 0.172]\n",
      "4458 [D loss: (0.548)(R 0.602, F 0.495)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.119] [G acc: 0.172]\n",
      "4459 [D loss: (0.546)(R 0.612, F 0.480)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.188] [G acc: 0.156]\n",
      "4460 [D loss: (0.567)(R 0.433, F 0.701)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.233] [G acc: 0.125]\n",
      "4461 [D loss: (0.627)(R 0.698, F 0.556)] [D acc: (0.625)(0.484, 0.766)] [G loss: 1.300] [G acc: 0.047]\n",
      "4462 [D loss: (0.652)(R 0.648, F 0.656)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.275] [G acc: 0.078]\n",
      "4463 [D loss: (0.630)(R 0.738, F 0.521)] [D acc: (0.664)(0.453, 0.875)] [G loss: 1.193] [G acc: 0.109]\n",
      "4464 [D loss: (0.587)(R 0.676, F 0.497)] [D acc: (0.672)(0.500, 0.844)] [G loss: 1.154] [G acc: 0.047]\n",
      "4465 [D loss: (0.633)(R 0.553, F 0.712)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.203] [G acc: 0.062]\n",
      "4466 [D loss: (0.565)(R 0.669, F 0.461)] [D acc: (0.688)(0.500, 0.875)] [G loss: 1.172] [G acc: 0.078]\n",
      "4467 [D loss: (0.568)(R 0.564, F 0.571)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.099] [G acc: 0.109]\n",
      "4468 [D loss: (0.529)(R 0.587, F 0.471)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.145] [G acc: 0.141]\n",
      "4469 [D loss: (0.575)(R 0.593, F 0.557)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.320] [G acc: 0.109]\n",
      "4470 [D loss: (0.679)(R 0.586, F 0.771)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.154] [G acc: 0.109]\n",
      "4471 [D loss: (0.549)(R 0.554, F 0.545)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.133] [G acc: 0.109]\n",
      "4472 [D loss: (0.543)(R 0.555, F 0.531)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.252] [G acc: 0.109]\n",
      "4473 [D loss: (0.572)(R 0.573, F 0.572)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.261] [G acc: 0.109]\n",
      "4474 [D loss: (0.558)(R 0.640, F 0.477)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.155] [G acc: 0.156]\n",
      "4475 [D loss: (0.561)(R 0.564, F 0.558)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.152] [G acc: 0.078]\n",
      "4476 [D loss: (0.582)(R 0.571, F 0.593)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.159] [G acc: 0.109]\n",
      "4477 [D loss: (0.617)(R 0.616, F 0.618)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.127] [G acc: 0.078]\n",
      "4478 [D loss: (0.547)(R 0.559, F 0.535)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.213] [G acc: 0.125]\n",
      "4479 [D loss: (0.514)(R 0.481, F 0.547)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.087] [G acc: 0.219]\n",
      "4480 [D loss: (0.555)(R 0.502, F 0.609)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.170] [G acc: 0.047]\n",
      "4481 [D loss: (0.620)(R 0.611, F 0.629)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.238] [G acc: 0.062]\n",
      "4482 [D loss: (0.678)(R 0.688, F 0.668)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.123] [G acc: 0.094]\n",
      "4483 [D loss: (0.600)(R 0.571, F 0.629)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.187] [G acc: 0.109]\n",
      "4484 [D loss: (0.531)(R 0.555, F 0.507)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.192] [G acc: 0.109]\n",
      "4485 [D loss: (0.511)(R 0.505, F 0.518)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.277] [G acc: 0.031]\n",
      "4486 [D loss: (0.487)(R 0.518, F 0.456)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.462] [G acc: 0.031]\n",
      "4487 [D loss: (0.701)(R 0.771, F 0.630)] [D acc: (0.578)(0.469, 0.688)] [G loss: 1.194] [G acc: 0.078]\n",
      "4488 [D loss: (0.571)(R 0.484, F 0.658)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.087] [G acc: 0.156]\n",
      "4489 [D loss: (0.535)(R 0.561, F 0.509)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.151] [G acc: 0.109]\n",
      "4490 [D loss: (0.590)(R 0.600, F 0.580)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.241] [G acc: 0.125]\n",
      "4491 [D loss: (0.575)(R 0.542, F 0.608)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.152] [G acc: 0.109]\n",
      "4492 [D loss: (0.594)(R 0.548, F 0.639)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.190] [G acc: 0.062]\n",
      "4493 [D loss: (0.689)(R 0.576, F 0.802)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.159] [G acc: 0.109]\n",
      "4494 [D loss: (0.588)(R 0.705, F 0.470)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.132] [G acc: 0.141]\n",
      "4495 [D loss: (0.676)(R 0.746, F 0.605)] [D acc: (0.586)(0.422, 0.750)] [G loss: 1.230] [G acc: 0.125]\n",
      "4496 [D loss: (0.669)(R 0.697, F 0.641)] [D acc: (0.641)(0.453, 0.828)] [G loss: 1.231] [G acc: 0.094]\n",
      "4497 [D loss: (0.627)(R 0.605, F 0.648)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.139] [G acc: 0.141]\n",
      "4498 [D loss: (0.548)(R 0.615, F 0.482)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.105] [G acc: 0.109]\n",
      "4499 [D loss: (0.617)(R 0.549, F 0.685)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.171] [G acc: 0.141]\n",
      "4500 [D loss: (0.668)(R 0.816, F 0.520)] [D acc: (0.602)(0.375, 0.828)] [G loss: 1.162] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4501 [D loss: (0.592)(R 0.601, F 0.583)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.175] [G acc: 0.141]\n",
      "4502 [D loss: (0.561)(R 0.621, F 0.501)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.019] [G acc: 0.156]\n",
      "4503 [D loss: (0.611)(R 0.616, F 0.607)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.119] [G acc: 0.125]\n",
      "4504 [D loss: (0.561)(R 0.532, F 0.590)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.134] [G acc: 0.156]\n",
      "4505 [D loss: (0.668)(R 0.622, F 0.714)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.045] [G acc: 0.141]\n",
      "4506 [D loss: (0.599)(R 0.666, F 0.532)] [D acc: (0.672)(0.516, 0.828)] [G loss: 0.981] [G acc: 0.109]\n",
      "4507 [D loss: (0.600)(R 0.639, F 0.561)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.093] [G acc: 0.156]\n",
      "4508 [D loss: (0.563)(R 0.602, F 0.524)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.212] [G acc: 0.156]\n",
      "4509 [D loss: (0.609)(R 0.601, F 0.617)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.211] [G acc: 0.109]\n",
      "4510 [D loss: (0.526)(R 0.530, F 0.522)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.293] [G acc: 0.078]\n",
      "4511 [D loss: (0.599)(R 0.602, F 0.596)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.215] [G acc: 0.125]\n",
      "4512 [D loss: (0.567)(R 0.559, F 0.575)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.272] [G acc: 0.141]\n",
      "4513 [D loss: (0.522)(R 0.565, F 0.479)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.146] [G acc: 0.125]\n",
      "4514 [D loss: (0.754)(R 0.644, F 0.864)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.116] [G acc: 0.094]\n",
      "4515 [D loss: (0.604)(R 0.699, F 0.510)] [D acc: (0.656)(0.453, 0.859)] [G loss: 1.160] [G acc: 0.094]\n",
      "4516 [D loss: (0.536)(R 0.561, F 0.510)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.066] [G acc: 0.188]\n",
      "4517 [D loss: (0.572)(R 0.512, F 0.632)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.122] [G acc: 0.156]\n",
      "4518 [D loss: (0.548)(R 0.593, F 0.503)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.286] [G acc: 0.047]\n",
      "4519 [D loss: (0.572)(R 0.572, F 0.572)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.207] [G acc: 0.141]\n",
      "4520 [D loss: (0.577)(R 0.677, F 0.477)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.222] [G acc: 0.125]\n",
      "4521 [D loss: (0.585)(R 0.577, F 0.594)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.260] [G acc: 0.109]\n",
      "4522 [D loss: (0.522)(R 0.524, F 0.519)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.244] [G acc: 0.141]\n",
      "4523 [D loss: (0.561)(R 0.512, F 0.609)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.052] [G acc: 0.188]\n",
      "4524 [D loss: (0.574)(R 0.527, F 0.622)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.334] [G acc: 0.078]\n",
      "4525 [D loss: (0.636)(R 0.604, F 0.668)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.146] [G acc: 0.188]\n",
      "4526 [D loss: (0.641)(R 0.761, F 0.521)] [D acc: (0.656)(0.484, 0.828)] [G loss: 1.088] [G acc: 0.219]\n",
      "4527 [D loss: (0.577)(R 0.618, F 0.537)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.063] [G acc: 0.125]\n",
      "4528 [D loss: (0.557)(R 0.490, F 0.625)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.313] [G acc: 0.172]\n",
      "4529 [D loss: (0.612)(R 0.631, F 0.592)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.302] [G acc: 0.109]\n",
      "4530 [D loss: (0.501)(R 0.496, F 0.506)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.267] [G acc: 0.125]\n",
      "4531 [D loss: (0.554)(R 0.463, F 0.644)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.207] [G acc: 0.188]\n",
      "4532 [D loss: (0.587)(R 0.637, F 0.537)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.238] [G acc: 0.125]\n",
      "4533 [D loss: (0.619)(R 0.665, F 0.573)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.513] [G acc: 0.062]\n",
      "4534 [D loss: (0.699)(R 0.686, F 0.712)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.227] [G acc: 0.125]\n",
      "4535 [D loss: (0.680)(R 0.709, F 0.651)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.179] [G acc: 0.109]\n",
      "4536 [D loss: (0.543)(R 0.557, F 0.528)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.063] [G acc: 0.141]\n",
      "4537 [D loss: (0.582)(R 0.567, F 0.598)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.157] [G acc: 0.109]\n",
      "4538 [D loss: (0.586)(R 0.598, F 0.573)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.146] [G acc: 0.109]\n",
      "4539 [D loss: (0.557)(R 0.569, F 0.545)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.100] [G acc: 0.141]\n",
      "4540 [D loss: (0.630)(R 0.567, F 0.694)] [D acc: (0.586)(0.562, 0.609)] [G loss: 1.145] [G acc: 0.156]\n",
      "4541 [D loss: (0.521)(R 0.517, F 0.526)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.131] [G acc: 0.156]\n",
      "4542 [D loss: (0.513)(R 0.510, F 0.516)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.315] [G acc: 0.062]\n",
      "4543 [D loss: (0.605)(R 0.557, F 0.652)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.169] [G acc: 0.141]\n",
      "4544 [D loss: (0.580)(R 0.644, F 0.517)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.200] [G acc: 0.188]\n",
      "4545 [D loss: (0.588)(R 0.614, F 0.562)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.084] [G acc: 0.250]\n",
      "4546 [D loss: (0.586)(R 0.575, F 0.597)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.085] [G acc: 0.188]\n",
      "4547 [D loss: (0.547)(R 0.555, F 0.539)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.276] [G acc: 0.078]\n",
      "4548 [D loss: (0.693)(R 0.697, F 0.688)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.159] [G acc: 0.109]\n",
      "4549 [D loss: (0.607)(R 0.674, F 0.539)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.225] [G acc: 0.094]\n",
      "4550 [D loss: (0.521)(R 0.492, F 0.549)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.142] [G acc: 0.188]\n",
      "4551 [D loss: (0.580)(R 0.643, F 0.517)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.159] [G acc: 0.141]\n",
      "4552 [D loss: (0.556)(R 0.530, F 0.583)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.158] [G acc: 0.141]\n",
      "4553 [D loss: (0.660)(R 0.574, F 0.746)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.174] [G acc: 0.125]\n",
      "4554 [D loss: (0.620)(R 0.573, F 0.666)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.209] [G acc: 0.109]\n",
      "4555 [D loss: (0.594)(R 0.672, F 0.516)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.110] [G acc: 0.125]\n",
      "4556 [D loss: (0.599)(R 0.576, F 0.622)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.141] [G acc: 0.141]\n",
      "4557 [D loss: (0.651)(R 0.674, F 0.629)] [D acc: (0.641)(0.469, 0.812)] [G loss: 1.256] [G acc: 0.031]\n",
      "4558 [D loss: (0.588)(R 0.657, F 0.519)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.230] [G acc: 0.062]\n",
      "4559 [D loss: (0.630)(R 0.615, F 0.645)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.225] [G acc: 0.047]\n",
      "4560 [D loss: (0.561)(R 0.641, F 0.481)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.218] [G acc: 0.047]\n",
      "4561 [D loss: (0.586)(R 0.658, F 0.514)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.136] [G acc: 0.172]\n",
      "4562 [D loss: (0.562)(R 0.521, F 0.602)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.156] [G acc: 0.094]\n",
      "4563 [D loss: (0.593)(R 0.618, F 0.569)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.267] [G acc: 0.125]\n",
      "4564 [D loss: (0.573)(R 0.546, F 0.600)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.212] [G acc: 0.125]\n",
      "4565 [D loss: (0.613)(R 0.678, F 0.548)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.102] [G acc: 0.141]\n",
      "4566 [D loss: (0.540)(R 0.569, F 0.512)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.235] [G acc: 0.109]\n",
      "4567 [D loss: (0.599)(R 0.597, F 0.600)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.052] [G acc: 0.156]\n",
      "4568 [D loss: (0.649)(R 0.553, F 0.745)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.228] [G acc: 0.094]\n",
      "4569 [D loss: (0.548)(R 0.621, F 0.475)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.164] [G acc: 0.078]\n",
      "4570 [D loss: (0.499)(R 0.487, F 0.512)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.207] [G acc: 0.125]\n",
      "4571 [D loss: (0.575)(R 0.463, F 0.687)] [D acc: (0.758)(0.781, 0.734)] [G loss: 1.194] [G acc: 0.062]\n",
      "4572 [D loss: (0.628)(R 0.708, F 0.549)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.255] [G acc: 0.156]\n",
      "4573 [D loss: (0.561)(R 0.595, F 0.527)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.227] [G acc: 0.125]\n",
      "4574 [D loss: (0.595)(R 0.614, F 0.575)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.193] [G acc: 0.141]\n",
      "4575 [D loss: (0.602)(R 0.592, F 0.612)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.194] [G acc: 0.109]\n",
      "4576 [D loss: (0.571)(R 0.602, F 0.541)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.287] [G acc: 0.125]\n",
      "4577 [D loss: (0.661)(R 0.606, F 0.717)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.144] [G acc: 0.094]\n",
      "4578 [D loss: (0.566)(R 0.580, F 0.552)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.211] [G acc: 0.172]\n",
      "4579 [D loss: (0.589)(R 0.618, F 0.560)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.210] [G acc: 0.078]\n",
      "4580 [D loss: (0.530)(R 0.551, F 0.509)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.231] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4581 [D loss: (0.562)(R 0.561, F 0.563)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.264] [G acc: 0.109]\n",
      "4582 [D loss: (0.568)(R 0.522, F 0.614)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.385] [G acc: 0.125]\n",
      "4583 [D loss: (0.557)(R 0.595, F 0.519)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.269] [G acc: 0.109]\n",
      "4584 [D loss: (0.642)(R 0.570, F 0.713)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.264] [G acc: 0.078]\n",
      "4585 [D loss: (0.533)(R 0.592, F 0.473)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.248] [G acc: 0.062]\n",
      "4586 [D loss: (0.593)(R 0.634, F 0.552)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.378] [G acc: 0.078]\n",
      "4587 [D loss: (0.624)(R 0.676, F 0.573)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.181] [G acc: 0.188]\n",
      "4588 [D loss: (0.535)(R 0.553, F 0.518)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.158] [G acc: 0.109]\n",
      "4589 [D loss: (0.545)(R 0.575, F 0.516)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.251] [G acc: 0.109]\n",
      "4590 [D loss: (0.571)(R 0.525, F 0.616)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.291] [G acc: 0.109]\n",
      "4591 [D loss: (0.551)(R 0.523, F 0.578)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.381] [G acc: 0.047]\n",
      "4592 [D loss: (0.648)(R 0.687, F 0.608)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.227] [G acc: 0.156]\n",
      "4593 [D loss: (0.588)(R 0.610, F 0.566)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.150] [G acc: 0.141]\n",
      "4594 [D loss: (0.530)(R 0.554, F 0.507)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.225] [G acc: 0.141]\n",
      "4595 [D loss: (0.525)(R 0.485, F 0.564)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.107] [G acc: 0.156]\n",
      "4596 [D loss: (0.540)(R 0.578, F 0.502)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.234] [G acc: 0.172]\n",
      "4597 [D loss: (0.563)(R 0.524, F 0.602)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.232] [G acc: 0.141]\n",
      "4598 [D loss: (0.555)(R 0.508, F 0.602)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.208] [G acc: 0.203]\n",
      "4599 [D loss: (0.594)(R 0.554, F 0.635)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.433] [G acc: 0.109]\n",
      "4600 [D loss: (0.639)(R 0.662, F 0.615)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.162] [G acc: 0.125]\n",
      "4601 [D loss: (0.589)(R 0.648, F 0.530)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.157] [G acc: 0.156]\n",
      "4602 [D loss: (0.564)(R 0.473, F 0.656)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.368] [G acc: 0.062]\n",
      "4603 [D loss: (0.522)(R 0.506, F 0.539)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.231] [G acc: 0.094]\n",
      "4604 [D loss: (0.510)(R 0.560, F 0.461)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.272] [G acc: 0.062]\n",
      "4605 [D loss: (0.653)(R 0.612, F 0.694)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.327] [G acc: 0.062]\n",
      "4606 [D loss: (0.488)(R 0.519, F 0.456)] [D acc: (0.789)(0.703, 0.875)] [G loss: 1.270] [G acc: 0.156]\n",
      "4607 [D loss: (0.526)(R 0.505, F 0.547)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.251] [G acc: 0.156]\n",
      "4608 [D loss: (0.560)(R 0.497, F 0.622)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.323] [G acc: 0.094]\n",
      "4609 [D loss: (0.577)(R 0.639, F 0.515)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.307] [G acc: 0.156]\n",
      "4610 [D loss: (0.517)(R 0.588, F 0.446)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.218] [G acc: 0.109]\n",
      "4611 [D loss: (0.511)(R 0.472, F 0.549)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.255] [G acc: 0.141]\n",
      "4612 [D loss: (0.581)(R 0.508, F 0.654)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.218] [G acc: 0.141]\n",
      "4613 [D loss: (0.563)(R 0.592, F 0.534)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.237] [G acc: 0.094]\n",
      "4614 [D loss: (0.569)(R 0.572, F 0.565)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.251] [G acc: 0.141]\n",
      "4615 [D loss: (0.583)(R 0.591, F 0.574)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.169] [G acc: 0.094]\n",
      "4616 [D loss: (0.565)(R 0.617, F 0.513)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.138] [G acc: 0.250]\n",
      "4617 [D loss: (0.646)(R 0.581, F 0.710)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.171] [G acc: 0.141]\n",
      "4618 [D loss: (0.628)(R 0.624, F 0.632)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.129] [G acc: 0.094]\n",
      "4619 [D loss: (0.538)(R 0.523, F 0.554)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.233] [G acc: 0.078]\n",
      "4620 [D loss: (0.528)(R 0.539, F 0.517)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.203] [G acc: 0.156]\n",
      "4621 [D loss: (0.579)(R 0.555, F 0.602)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.204] [G acc: 0.156]\n",
      "4622 [D loss: (0.647)(R 0.606, F 0.689)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.289] [G acc: 0.125]\n",
      "4623 [D loss: (0.699)(R 0.732, F 0.665)] [D acc: (0.578)(0.484, 0.672)] [G loss: 1.196] [G acc: 0.125]\n",
      "4624 [D loss: (0.519)(R 0.493, F 0.544)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.220] [G acc: 0.109]\n",
      "4625 [D loss: (0.610)(R 0.530, F 0.689)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.182] [G acc: 0.109]\n",
      "4626 [D loss: (0.618)(R 0.670, F 0.566)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.175] [G acc: 0.156]\n",
      "4627 [D loss: (0.588)(R 0.579, F 0.597)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.116] [G acc: 0.125]\n",
      "4628 [D loss: (0.569)(R 0.569, F 0.569)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.212] [G acc: 0.125]\n",
      "4629 [D loss: (0.569)(R 0.441, F 0.697)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.190] [G acc: 0.047]\n",
      "4630 [D loss: (0.517)(R 0.491, F 0.542)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.231] [G acc: 0.141]\n",
      "4631 [D loss: (0.601)(R 0.558, F 0.643)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.179] [G acc: 0.141]\n",
      "4632 [D loss: (0.538)(R 0.469, F 0.606)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.181] [G acc: 0.219]\n",
      "4633 [D loss: (0.592)(R 0.675, F 0.509)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.269] [G acc: 0.125]\n",
      "4634 [D loss: (0.552)(R 0.587, F 0.516)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.266] [G acc: 0.125]\n",
      "4635 [D loss: (0.516)(R 0.510, F 0.523)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.362] [G acc: 0.125]\n",
      "4636 [D loss: (0.491)(R 0.494, F 0.488)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.453] [G acc: 0.078]\n",
      "4637 [D loss: (0.611)(R 0.602, F 0.619)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.294] [G acc: 0.078]\n",
      "4638 [D loss: (0.639)(R 0.701, F 0.577)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.228] [G acc: 0.094]\n",
      "4639 [D loss: (0.588)(R 0.572, F 0.603)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.216] [G acc: 0.094]\n",
      "4640 [D loss: (0.575)(R 0.538, F 0.612)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.143] [G acc: 0.125]\n",
      "4641 [D loss: (0.623)(R 0.745, F 0.502)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.233] [G acc: 0.188]\n",
      "4642 [D loss: (0.646)(R 0.690, F 0.601)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.231] [G acc: 0.125]\n",
      "4643 [D loss: (0.624)(R 0.646, F 0.602)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.182] [G acc: 0.094]\n",
      "4644 [D loss: (0.543)(R 0.579, F 0.508)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.143] [G acc: 0.109]\n",
      "4645 [D loss: (0.555)(R 0.596, F 0.515)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.265] [G acc: 0.125]\n",
      "4646 [D loss: (0.579)(R 0.593, F 0.566)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.130] [G acc: 0.156]\n",
      "4647 [D loss: (0.557)(R 0.560, F 0.554)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.174] [G acc: 0.094]\n",
      "4648 [D loss: (0.638)(R 0.543, F 0.732)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.272] [G acc: 0.062]\n",
      "4649 [D loss: (0.580)(R 0.632, F 0.527)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.181] [G acc: 0.078]\n",
      "4650 [D loss: (0.652)(R 0.691, F 0.612)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.357] [G acc: 0.047]\n",
      "4651 [D loss: (0.529)(R 0.552, F 0.506)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.173] [G acc: 0.141]\n",
      "4652 [D loss: (0.614)(R 0.641, F 0.588)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.230] [G acc: 0.125]\n",
      "4653 [D loss: (0.634)(R 0.618, F 0.650)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.262] [G acc: 0.109]\n",
      "4654 [D loss: (0.534)(R 0.603, F 0.464)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.280] [G acc: 0.125]\n",
      "4655 [D loss: (0.557)(R 0.559, F 0.556)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.337] [G acc: 0.078]\n",
      "4656 [D loss: (0.585)(R 0.549, F 0.621)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.299] [G acc: 0.062]\n",
      "4657 [D loss: (0.570)(R 0.593, F 0.547)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.092] [G acc: 0.156]\n",
      "4658 [D loss: (0.578)(R 0.626, F 0.529)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.034] [G acc: 0.203]\n",
      "4659 [D loss: (0.585)(R 0.593, F 0.578)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.209] [G acc: 0.078]\n",
      "4660 [D loss: (0.536)(R 0.525, F 0.547)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.210] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4661 [D loss: (0.559)(R 0.516, F 0.602)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.238] [G acc: 0.125]\n",
      "4662 [D loss: (0.517)(R 0.495, F 0.539)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.298] [G acc: 0.109]\n",
      "4663 [D loss: (0.652)(R 0.652, F 0.652)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.265] [G acc: 0.125]\n",
      "4664 [D loss: (0.604)(R 0.621, F 0.587)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.122] [G acc: 0.266]\n",
      "4665 [D loss: (0.580)(R 0.626, F 0.535)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.187] [G acc: 0.109]\n",
      "4666 [D loss: (0.534)(R 0.517, F 0.551)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.175] [G acc: 0.188]\n",
      "4667 [D loss: (0.562)(R 0.478, F 0.645)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.165] [G acc: 0.109]\n",
      "4668 [D loss: (0.583)(R 0.668, F 0.497)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.167] [G acc: 0.125]\n",
      "4669 [D loss: (0.645)(R 0.703, F 0.587)] [D acc: (0.641)(0.484, 0.797)] [G loss: 1.183] [G acc: 0.156]\n",
      "4670 [D loss: (0.604)(R 0.577, F 0.632)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.142] [G acc: 0.125]\n",
      "4671 [D loss: (0.579)(R 0.604, F 0.553)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.154] [G acc: 0.141]\n",
      "4672 [D loss: (0.581)(R 0.654, F 0.507)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.059] [G acc: 0.234]\n",
      "4673 [D loss: (0.536)(R 0.521, F 0.551)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.059] [G acc: 0.172]\n",
      "4674 [D loss: (0.646)(R 0.655, F 0.637)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.112] [G acc: 0.188]\n",
      "4675 [D loss: (0.643)(R 0.727, F 0.559)] [D acc: (0.625)(0.484, 0.766)] [G loss: 1.081] [G acc: 0.234]\n",
      "4676 [D loss: (0.585)(R 0.553, F 0.617)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.174] [G acc: 0.141]\n",
      "4677 [D loss: (0.574)(R 0.613, F 0.536)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.129] [G acc: 0.109]\n",
      "4678 [D loss: (0.617)(R 0.670, F 0.564)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.098] [G acc: 0.141]\n",
      "4679 [D loss: (0.603)(R 0.592, F 0.614)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.186] [G acc: 0.125]\n",
      "4680 [D loss: (0.602)(R 0.670, F 0.534)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.123] [G acc: 0.078]\n",
      "4681 [D loss: (0.670)(R 0.569, F 0.771)] [D acc: (0.625)(0.641, 0.609)] [G loss: 1.051] [G acc: 0.141]\n",
      "4682 [D loss: (0.613)(R 0.595, F 0.632)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.084] [G acc: 0.125]\n",
      "4683 [D loss: (0.583)(R 0.587, F 0.580)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.079] [G acc: 0.141]\n",
      "4684 [D loss: (0.591)(R 0.585, F 0.596)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.087] [G acc: 0.109]\n",
      "4685 [D loss: (0.550)(R 0.527, F 0.573)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.205] [G acc: 0.094]\n",
      "4686 [D loss: (0.621)(R 0.592, F 0.651)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.134] [G acc: 0.109]\n",
      "4687 [D loss: (0.559)(R 0.536, F 0.582)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.086] [G acc: 0.172]\n",
      "4688 [D loss: (0.594)(R 0.599, F 0.590)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.209] [G acc: 0.125]\n",
      "4689 [D loss: (0.569)(R 0.542, F 0.595)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.249] [G acc: 0.156]\n",
      "4690 [D loss: (0.600)(R 0.646, F 0.553)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.228] [G acc: 0.109]\n",
      "4691 [D loss: (0.586)(R 0.571, F 0.601)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.163] [G acc: 0.141]\n",
      "4692 [D loss: (0.620)(R 0.643, F 0.597)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.208] [G acc: 0.109]\n",
      "4693 [D loss: (0.614)(R 0.589, F 0.640)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.182] [G acc: 0.062]\n",
      "4694 [D loss: (0.565)(R 0.619, F 0.512)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.171] [G acc: 0.125]\n",
      "4695 [D loss: (0.625)(R 0.575, F 0.675)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.237] [G acc: 0.062]\n",
      "4696 [D loss: (0.519)(R 0.543, F 0.496)] [D acc: (0.781)(0.688, 0.875)] [G loss: 1.245] [G acc: 0.141]\n",
      "4697 [D loss: (0.563)(R 0.543, F 0.583)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.303] [G acc: 0.062]\n",
      "4698 [D loss: (0.540)(R 0.515, F 0.564)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.154] [G acc: 0.125]\n",
      "4699 [D loss: (0.654)(R 0.630, F 0.678)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.149] [G acc: 0.094]\n",
      "4700 [D loss: (0.542)(R 0.545, F 0.539)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.188] [G acc: 0.078]\n",
      "4701 [D loss: (0.535)(R 0.581, F 0.490)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.360] [G acc: 0.062]\n",
      "4702 [D loss: (0.645)(R 0.523, F 0.768)] [D acc: (0.625)(0.656, 0.594)] [G loss: 1.281] [G acc: 0.094]\n",
      "4703 [D loss: (0.598)(R 0.732, F 0.463)] [D acc: (0.688)(0.469, 0.906)] [G loss: 1.156] [G acc: 0.078]\n",
      "4704 [D loss: (0.553)(R 0.561, F 0.545)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.176] [G acc: 0.125]\n",
      "4705 [D loss: (0.536)(R 0.597, F 0.474)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.285] [G acc: 0.125]\n",
      "4706 [D loss: (0.635)(R 0.727, F 0.544)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.298] [G acc: 0.125]\n",
      "4707 [D loss: (0.489)(R 0.417, F 0.562)] [D acc: (0.789)(0.828, 0.750)] [G loss: 1.255] [G acc: 0.172]\n",
      "4708 [D loss: (0.510)(R 0.481, F 0.539)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.404] [G acc: 0.078]\n",
      "4709 [D loss: (0.562)(R 0.583, F 0.541)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.251] [G acc: 0.109]\n",
      "4710 [D loss: (0.663)(R 0.470, F 0.855)] [D acc: (0.688)(0.734, 0.641)] [G loss: 1.418] [G acc: 0.047]\n",
      "4711 [D loss: (0.615)(R 0.787, F 0.444)] [D acc: (0.648)(0.406, 0.891)] [G loss: 1.231] [G acc: 0.094]\n",
      "4712 [D loss: (0.599)(R 0.675, F 0.524)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.128] [G acc: 0.188]\n",
      "4713 [D loss: (0.571)(R 0.602, F 0.541)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.192] [G acc: 0.109]\n",
      "4714 [D loss: (0.612)(R 0.555, F 0.669)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.140] [G acc: 0.188]\n",
      "4715 [D loss: (0.615)(R 0.624, F 0.606)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.175] [G acc: 0.109]\n",
      "4716 [D loss: (0.555)(R 0.562, F 0.549)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.166] [G acc: 0.203]\n",
      "4717 [D loss: (0.523)(R 0.525, F 0.521)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.245] [G acc: 0.156]\n",
      "4718 [D loss: (0.526)(R 0.539, F 0.514)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.227] [G acc: 0.203]\n",
      "4719 [D loss: (0.499)(R 0.461, F 0.536)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.215] [G acc: 0.172]\n",
      "4720 [D loss: (0.558)(R 0.424, F 0.692)] [D acc: (0.758)(0.797, 0.719)] [G loss: 1.353] [G acc: 0.094]\n",
      "4721 [D loss: (0.574)(R 0.590, F 0.557)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.282] [G acc: 0.094]\n",
      "4722 [D loss: (0.559)(R 0.617, F 0.500)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.315] [G acc: 0.125]\n",
      "4723 [D loss: (0.543)(R 0.590, F 0.496)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.304] [G acc: 0.188]\n",
      "4724 [D loss: (0.530)(R 0.596, F 0.463)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.266] [G acc: 0.141]\n",
      "4725 [D loss: (0.586)(R 0.592, F 0.580)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.253] [G acc: 0.094]\n",
      "4726 [D loss: (0.576)(R 0.647, F 0.506)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.102] [G acc: 0.188]\n",
      "4727 [D loss: (0.559)(R 0.540, F 0.579)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.147] [G acc: 0.156]\n",
      "4728 [D loss: (0.542)(R 0.524, F 0.561)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.353] [G acc: 0.125]\n",
      "4729 [D loss: (0.582)(R 0.600, F 0.565)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.206] [G acc: 0.125]\n",
      "4730 [D loss: (0.528)(R 0.542, F 0.513)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.329] [G acc: 0.141]\n",
      "4731 [D loss: (0.556)(R 0.524, F 0.588)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.246] [G acc: 0.141]\n",
      "4732 [D loss: (0.540)(R 0.547, F 0.533)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.325] [G acc: 0.172]\n",
      "4733 [D loss: (0.658)(R 0.582, F 0.733)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.295] [G acc: 0.078]\n",
      "4734 [D loss: (0.547)(R 0.652, F 0.442)] [D acc: (0.703)(0.531, 0.875)] [G loss: 1.241] [G acc: 0.047]\n",
      "4735 [D loss: (0.579)(R 0.655, F 0.503)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.131] [G acc: 0.172]\n",
      "4736 [D loss: (0.585)(R 0.490, F 0.680)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.418] [G acc: 0.094]\n",
      "4737 [D loss: (0.640)(R 0.723, F 0.557)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.168] [G acc: 0.156]\n",
      "4738 [D loss: (0.605)(R 0.583, F 0.627)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.267] [G acc: 0.109]\n",
      "4739 [D loss: (0.508)(R 0.471, F 0.546)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.352] [G acc: 0.109]\n",
      "4740 [D loss: (0.534)(R 0.558, F 0.510)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.328] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4741 [D loss: (0.545)(R 0.586, F 0.505)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.390] [G acc: 0.094]\n",
      "4742 [D loss: (0.580)(R 0.667, F 0.493)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.169] [G acc: 0.094]\n",
      "4743 [D loss: (0.742)(R 0.728, F 0.757)] [D acc: (0.562)(0.516, 0.609)] [G loss: 1.125] [G acc: 0.078]\n",
      "4744 [D loss: (0.620)(R 0.669, F 0.572)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.157] [G acc: 0.078]\n",
      "4745 [D loss: (0.588)(R 0.576, F 0.601)] [D acc: (0.734)(0.672, 0.797)] [G loss: 0.957] [G acc: 0.219]\n",
      "4746 [D loss: (0.578)(R 0.596, F 0.560)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.109] [G acc: 0.125]\n",
      "4747 [D loss: (0.585)(R 0.598, F 0.572)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.184] [G acc: 0.125]\n",
      "4748 [D loss: (0.565)(R 0.577, F 0.552)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.104] [G acc: 0.094]\n",
      "4749 [D loss: (0.533)(R 0.527, F 0.538)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.096] [G acc: 0.141]\n",
      "4750 [D loss: (0.505)(R 0.484, F 0.526)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.189] [G acc: 0.156]\n",
      "4751 [D loss: (0.561)(R 0.614, F 0.509)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.249] [G acc: 0.125]\n",
      "4752 [D loss: (0.647)(R 0.640, F 0.655)] [D acc: (0.602)(0.562, 0.641)] [G loss: 1.119] [G acc: 0.156]\n",
      "4753 [D loss: (0.621)(R 0.619, F 0.623)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.089] [G acc: 0.156]\n",
      "4754 [D loss: (0.562)(R 0.538, F 0.586)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.178] [G acc: 0.141]\n",
      "4755 [D loss: (0.554)(R 0.554, F 0.554)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.113] [G acc: 0.109]\n",
      "4756 [D loss: (0.559)(R 0.526, F 0.592)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.315] [G acc: 0.031]\n",
      "4757 [D loss: (0.581)(R 0.604, F 0.558)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.118] [G acc: 0.078]\n",
      "4758 [D loss: (0.540)(R 0.605, F 0.475)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.290] [G acc: 0.125]\n",
      "4759 [D loss: (0.585)(R 0.499, F 0.671)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.337] [G acc: 0.031]\n",
      "4760 [D loss: (0.608)(R 0.586, F 0.629)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.167] [G acc: 0.078]\n",
      "4761 [D loss: (0.543)(R 0.511, F 0.576)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.247] [G acc: 0.062]\n",
      "4762 [D loss: (0.534)(R 0.587, F 0.481)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.278] [G acc: 0.109]\n",
      "4763 [D loss: (0.618)(R 0.577, F 0.659)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.228] [G acc: 0.172]\n",
      "4764 [D loss: (0.558)(R 0.569, F 0.547)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.068] [G acc: 0.188]\n",
      "4765 [D loss: (0.550)(R 0.534, F 0.565)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.374] [G acc: 0.078]\n",
      "4766 [D loss: (0.575)(R 0.621, F 0.528)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.167] [G acc: 0.094]\n",
      "4767 [D loss: (0.566)(R 0.593, F 0.539)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.287] [G acc: 0.125]\n",
      "4768 [D loss: (0.607)(R 0.594, F 0.620)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.217] [G acc: 0.141]\n",
      "4769 [D loss: (0.584)(R 0.697, F 0.471)] [D acc: (0.672)(0.484, 0.859)] [G loss: 1.187] [G acc: 0.078]\n",
      "4770 [D loss: (0.485)(R 0.499, F 0.472)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.163] [G acc: 0.172]\n",
      "4771 [D loss: (0.667)(R 0.597, F 0.737)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.208] [G acc: 0.094]\n",
      "4772 [D loss: (0.584)(R 0.574, F 0.595)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.178] [G acc: 0.109]\n",
      "4773 [D loss: (0.658)(R 0.639, F 0.677)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.162] [G acc: 0.125]\n",
      "4774 [D loss: (0.582)(R 0.659, F 0.505)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.208] [G acc: 0.094]\n",
      "4775 [D loss: (0.541)(R 0.530, F 0.551)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.170] [G acc: 0.094]\n",
      "4776 [D loss: (0.628)(R 0.668, F 0.587)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.202] [G acc: 0.109]\n",
      "4777 [D loss: (0.549)(R 0.566, F 0.532)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.093] [G acc: 0.125]\n",
      "4778 [D loss: (0.632)(R 0.571, F 0.693)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.294] [G acc: 0.062]\n",
      "4779 [D loss: (0.664)(R 0.669, F 0.659)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.194] [G acc: 0.078]\n",
      "4780 [D loss: (0.576)(R 0.623, F 0.529)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.102] [G acc: 0.188]\n",
      "4781 [D loss: (0.664)(R 0.591, F 0.736)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.154] [G acc: 0.125]\n",
      "4782 [D loss: (0.631)(R 0.745, F 0.518)] [D acc: (0.625)(0.438, 0.812)] [G loss: 1.141] [G acc: 0.094]\n",
      "4783 [D loss: (0.535)(R 0.520, F 0.549)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.120] [G acc: 0.125]\n",
      "4784 [D loss: (0.631)(R 0.633, F 0.629)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.123] [G acc: 0.125]\n",
      "4785 [D loss: (0.590)(R 0.688, F 0.493)] [D acc: (0.648)(0.453, 0.844)] [G loss: 1.166] [G acc: 0.125]\n",
      "4786 [D loss: (0.541)(R 0.543, F 0.540)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.025] [G acc: 0.234]\n",
      "4787 [D loss: (0.491)(R 0.468, F 0.514)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.164] [G acc: 0.109]\n",
      "4788 [D loss: (0.612)(R 0.494, F 0.730)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.117] [G acc: 0.156]\n",
      "4789 [D loss: (0.635)(R 0.717, F 0.554)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.358] [G acc: 0.047]\n",
      "4790 [D loss: (0.598)(R 0.673, F 0.523)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.090] [G acc: 0.203]\n",
      "4791 [D loss: (0.619)(R 0.539, F 0.699)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.311] [G acc: 0.094]\n",
      "4792 [D loss: (0.559)(R 0.565, F 0.552)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.276] [G acc: 0.062]\n",
      "4793 [D loss: (0.595)(R 0.651, F 0.538)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.209] [G acc: 0.109]\n",
      "4794 [D loss: (0.602)(R 0.540, F 0.665)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.272] [G acc: 0.094]\n",
      "4795 [D loss: (0.534)(R 0.544, F 0.524)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.261] [G acc: 0.125]\n",
      "4796 [D loss: (0.592)(R 0.544, F 0.641)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.151] [G acc: 0.094]\n",
      "4797 [D loss: (0.596)(R 0.682, F 0.511)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.311] [G acc: 0.078]\n",
      "4798 [D loss: (0.672)(R 0.742, F 0.601)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.169] [G acc: 0.078]\n",
      "4799 [D loss: (0.694)(R 0.664, F 0.723)] [D acc: (0.594)(0.453, 0.734)] [G loss: 1.121] [G acc: 0.141]\n",
      "4800 [D loss: (0.651)(R 0.738, F 0.565)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.074] [G acc: 0.109]\n",
      "4801 [D loss: (0.572)(R 0.597, F 0.546)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.069] [G acc: 0.172]\n",
      "4802 [D loss: (0.499)(R 0.492, F 0.506)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.268] [G acc: 0.078]\n",
      "4803 [D loss: (0.579)(R 0.585, F 0.573)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.207] [G acc: 0.078]\n",
      "4804 [D loss: (0.584)(R 0.567, F 0.601)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.175] [G acc: 0.141]\n",
      "4805 [D loss: (0.632)(R 0.613, F 0.651)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.148] [G acc: 0.141]\n",
      "4806 [D loss: (0.602)(R 0.676, F 0.527)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.108] [G acc: 0.141]\n",
      "4807 [D loss: (0.564)(R 0.574, F 0.553)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.144] [G acc: 0.125]\n",
      "4808 [D loss: (0.554)(R 0.627, F 0.481)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.209] [G acc: 0.109]\n",
      "4809 [D loss: (0.543)(R 0.490, F 0.595)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.239] [G acc: 0.094]\n",
      "4810 [D loss: (0.521)(R 0.580, F 0.463)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.307] [G acc: 0.109]\n",
      "4811 [D loss: (0.562)(R 0.454, F 0.670)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.190] [G acc: 0.203]\n",
      "4812 [D loss: (0.601)(R 0.596, F 0.607)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.218] [G acc: 0.156]\n",
      "4813 [D loss: (0.610)(R 0.588, F 0.632)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.220] [G acc: 0.094]\n",
      "4814 [D loss: (0.598)(R 0.655, F 0.541)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.192] [G acc: 0.141]\n",
      "4815 [D loss: (0.610)(R 0.671, F 0.548)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.221] [G acc: 0.062]\n",
      "4816 [D loss: (0.595)(R 0.642, F 0.548)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.083] [G acc: 0.188]\n",
      "4817 [D loss: (0.594)(R 0.648, F 0.540)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.183] [G acc: 0.125]\n",
      "4818 [D loss: (0.506)(R 0.479, F 0.533)] [D acc: (0.766)(0.766, 0.766)] [G loss: 1.201] [G acc: 0.109]\n",
      "4819 [D loss: (0.594)(R 0.603, F 0.585)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.055] [G acc: 0.188]\n",
      "4820 [D loss: (0.550)(R 0.541, F 0.559)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.171] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4821 [D loss: (0.543)(R 0.555, F 0.532)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.398] [G acc: 0.078]\n",
      "4822 [D loss: (0.594)(R 0.680, F 0.508)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.235] [G acc: 0.141]\n",
      "4823 [D loss: (0.640)(R 0.650, F 0.630)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.164] [G acc: 0.109]\n",
      "4824 [D loss: (0.560)(R 0.605, F 0.516)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.365] [G acc: 0.094]\n",
      "4825 [D loss: (0.579)(R 0.536, F 0.622)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.229] [G acc: 0.094]\n",
      "4826 [D loss: (0.599)(R 0.680, F 0.518)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.169] [G acc: 0.156]\n",
      "4827 [D loss: (0.520)(R 0.516, F 0.524)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.199] [G acc: 0.109]\n",
      "4828 [D loss: (0.605)(R 0.654, F 0.556)] [D acc: (0.602)(0.500, 0.703)] [G loss: 1.245] [G acc: 0.125]\n",
      "4829 [D loss: (0.561)(R 0.594, F 0.527)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.217] [G acc: 0.078]\n",
      "4830 [D loss: (0.583)(R 0.535, F 0.631)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.170] [G acc: 0.156]\n",
      "4831 [D loss: (0.564)(R 0.600, F 0.528)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.233] [G acc: 0.172]\n",
      "4832 [D loss: (0.599)(R 0.671, F 0.528)] [D acc: (0.625)(0.438, 0.812)] [G loss: 1.198] [G acc: 0.109]\n",
      "4833 [D loss: (0.537)(R 0.453, F 0.622)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.345] [G acc: 0.078]\n",
      "4834 [D loss: (0.582)(R 0.675, F 0.488)] [D acc: (0.688)(0.516, 0.859)] [G loss: 1.217] [G acc: 0.047]\n",
      "4835 [D loss: (0.683)(R 0.739, F 0.627)] [D acc: (0.656)(0.484, 0.828)] [G loss: 1.134] [G acc: 0.125]\n",
      "4836 [D loss: (0.515)(R 0.497, F 0.533)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.170] [G acc: 0.203]\n",
      "4837 [D loss: (0.520)(R 0.512, F 0.529)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.228] [G acc: 0.125]\n",
      "4838 [D loss: (0.736)(R 0.532, F 0.941)] [D acc: (0.664)(0.688, 0.641)] [G loss: 1.348] [G acc: 0.078]\n",
      "4839 [D loss: (0.596)(R 0.707, F 0.485)] [D acc: (0.648)(0.469, 0.828)] [G loss: 1.279] [G acc: 0.141]\n",
      "4840 [D loss: (0.538)(R 0.533, F 0.542)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.218] [G acc: 0.156]\n",
      "4841 [D loss: (0.505)(R 0.551, F 0.459)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.234] [G acc: 0.219]\n",
      "4842 [D loss: (0.518)(R 0.551, F 0.486)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.129] [G acc: 0.156]\n",
      "4843 [D loss: (0.626)(R 0.579, F 0.673)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.241] [G acc: 0.141]\n",
      "4844 [D loss: (0.523)(R 0.565, F 0.481)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.297] [G acc: 0.109]\n",
      "4845 [D loss: (0.631)(R 0.491, F 0.772)] [D acc: (0.727)(0.781, 0.672)] [G loss: 1.288] [G acc: 0.109]\n",
      "4846 [D loss: (0.500)(R 0.492, F 0.508)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.373] [G acc: 0.078]\n",
      "4847 [D loss: (0.616)(R 0.579, F 0.653)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.361] [G acc: 0.047]\n",
      "4848 [D loss: (0.641)(R 0.738, F 0.545)] [D acc: (0.664)(0.500, 0.828)] [G loss: 1.267] [G acc: 0.047]\n",
      "4849 [D loss: (0.612)(R 0.636, F 0.588)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.225] [G acc: 0.125]\n",
      "4850 [D loss: (0.648)(R 0.684, F 0.612)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.201] [G acc: 0.078]\n",
      "4851 [D loss: (0.561)(R 0.581, F 0.540)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.317] [G acc: 0.078]\n",
      "4852 [D loss: (0.585)(R 0.553, F 0.618)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.138] [G acc: 0.172]\n",
      "4853 [D loss: (0.610)(R 0.665, F 0.555)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.210] [G acc: 0.156]\n",
      "4854 [D loss: (0.580)(R 0.606, F 0.553)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.197] [G acc: 0.109]\n",
      "4855 [D loss: (0.582)(R 0.567, F 0.596)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.279] [G acc: 0.078]\n",
      "4856 [D loss: (0.597)(R 0.697, F 0.498)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.206] [G acc: 0.141]\n",
      "4857 [D loss: (0.570)(R 0.532, F 0.608)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.192] [G acc: 0.125]\n",
      "4858 [D loss: (0.610)(R 0.544, F 0.676)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.219] [G acc: 0.141]\n",
      "4859 [D loss: (0.548)(R 0.616, F 0.480)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.201] [G acc: 0.141]\n",
      "4860 [D loss: (0.495)(R 0.469, F 0.521)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.311] [G acc: 0.094]\n",
      "4861 [D loss: (0.642)(R 0.519, F 0.765)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.320] [G acc: 0.062]\n",
      "4862 [D loss: (0.598)(R 0.740, F 0.457)] [D acc: (0.688)(0.500, 0.875)] [G loss: 1.282] [G acc: 0.109]\n",
      "4863 [D loss: (0.577)(R 0.508, F 0.646)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.207] [G acc: 0.125]\n",
      "4864 [D loss: (0.612)(R 0.669, F 0.555)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.310] [G acc: 0.047]\n",
      "4865 [D loss: (0.515)(R 0.540, F 0.490)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.189] [G acc: 0.141]\n",
      "4866 [D loss: (0.545)(R 0.602, F 0.489)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.270] [G acc: 0.109]\n",
      "4867 [D loss: (0.549)(R 0.554, F 0.544)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.380] [G acc: 0.109]\n",
      "4868 [D loss: (0.578)(R 0.590, F 0.567)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.124] [G acc: 0.156]\n",
      "4869 [D loss: (0.534)(R 0.504, F 0.564)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.307] [G acc: 0.109]\n",
      "4870 [D loss: (0.483)(R 0.511, F 0.455)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.397] [G acc: 0.094]\n",
      "4871 [D loss: (0.557)(R 0.466, F 0.649)] [D acc: (0.688)(0.734, 0.641)] [G loss: 1.447] [G acc: 0.047]\n",
      "4872 [D loss: (0.621)(R 0.638, F 0.605)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.245] [G acc: 0.172]\n",
      "4873 [D loss: (0.675)(R 0.726, F 0.624)] [D acc: (0.586)(0.484, 0.688)] [G loss: 1.245] [G acc: 0.078]\n",
      "4874 [D loss: (0.544)(R 0.548, F 0.541)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.150] [G acc: 0.141]\n",
      "4875 [D loss: (0.527)(R 0.503, F 0.550)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.287] [G acc: 0.109]\n",
      "4876 [D loss: (0.547)(R 0.644, F 0.450)] [D acc: (0.734)(0.562, 0.906)] [G loss: 1.240] [G acc: 0.203]\n",
      "4877 [D loss: (0.612)(R 0.514, F 0.709)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.259] [G acc: 0.125]\n",
      "4878 [D loss: (0.703)(R 0.804, F 0.601)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.207] [G acc: 0.078]\n",
      "4879 [D loss: (0.548)(R 0.584, F 0.512)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.186] [G acc: 0.172]\n",
      "4880 [D loss: (0.492)(R 0.530, F 0.454)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.074] [G acc: 0.328]\n",
      "4881 [D loss: (0.608)(R 0.603, F 0.613)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.358] [G acc: 0.125]\n",
      "4882 [D loss: (0.581)(R 0.538, F 0.624)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.274] [G acc: 0.109]\n",
      "4883 [D loss: (0.588)(R 0.563, F 0.613)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.268] [G acc: 0.125]\n",
      "4884 [D loss: (0.563)(R 0.607, F 0.520)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.289] [G acc: 0.109]\n",
      "4885 [D loss: (0.604)(R 0.573, F 0.636)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.278] [G acc: 0.062]\n",
      "4886 [D loss: (0.606)(R 0.669, F 0.544)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.311] [G acc: 0.141]\n",
      "4887 [D loss: (0.568)(R 0.515, F 0.621)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.257] [G acc: 0.078]\n",
      "4888 [D loss: (0.552)(R 0.566, F 0.539)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.269] [G acc: 0.125]\n",
      "4889 [D loss: (0.516)(R 0.557, F 0.474)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.230] [G acc: 0.047]\n",
      "4890 [D loss: (0.590)(R 0.608, F 0.572)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.150] [G acc: 0.125]\n",
      "4891 [D loss: (0.497)(R 0.445, F 0.548)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.188] [G acc: 0.141]\n",
      "4892 [D loss: (0.551)(R 0.611, F 0.492)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.278] [G acc: 0.156]\n",
      "4893 [D loss: (0.606)(R 0.566, F 0.645)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.239] [G acc: 0.141]\n",
      "4894 [D loss: (0.528)(R 0.576, F 0.480)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.343] [G acc: 0.078]\n",
      "4895 [D loss: (0.513)(R 0.503, F 0.523)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.168] [G acc: 0.156]\n",
      "4896 [D loss: (0.553)(R 0.491, F 0.616)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.264] [G acc: 0.125]\n",
      "4897 [D loss: (0.593)(R 0.631, F 0.556)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.221] [G acc: 0.109]\n",
      "4898 [D loss: (0.628)(R 0.681, F 0.574)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.203] [G acc: 0.109]\n",
      "4899 [D loss: (0.479)(R 0.515, F 0.443)] [D acc: (0.797)(0.734, 0.859)] [G loss: 1.245] [G acc: 0.125]\n",
      "4900 [D loss: (0.575)(R 0.543, F 0.608)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.310] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4901 [D loss: (0.556)(R 0.551, F 0.561)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.392] [G acc: 0.125]\n",
      "4902 [D loss: (0.460)(R 0.452, F 0.469)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.357] [G acc: 0.156]\n",
      "4903 [D loss: (0.554)(R 0.554, F 0.554)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.496] [G acc: 0.062]\n",
      "4904 [D loss: (0.544)(R 0.526, F 0.562)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.314] [G acc: 0.078]\n",
      "4905 [D loss: (0.596)(R 0.658, F 0.533)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.289] [G acc: 0.109]\n",
      "4906 [D loss: (0.572)(R 0.668, F 0.475)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.292] [G acc: 0.062]\n",
      "4907 [D loss: (0.653)(R 0.667, F 0.639)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.160] [G acc: 0.109]\n",
      "4908 [D loss: (0.472)(R 0.482, F 0.462)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.243] [G acc: 0.141]\n",
      "4909 [D loss: (0.574)(R 0.537, F 0.611)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.145] [G acc: 0.188]\n",
      "4910 [D loss: (0.555)(R 0.538, F 0.572)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.092] [G acc: 0.219]\n",
      "4911 [D loss: (0.547)(R 0.566, F 0.527)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.235] [G acc: 0.188]\n",
      "4912 [D loss: (0.526)(R 0.566, F 0.486)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.206] [G acc: 0.125]\n",
      "4913 [D loss: (0.616)(R 0.574, F 0.657)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.175] [G acc: 0.156]\n",
      "4914 [D loss: (0.603)(R 0.619, F 0.586)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.046] [G acc: 0.141]\n",
      "4915 [D loss: (0.540)(R 0.506, F 0.575)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.174] [G acc: 0.062]\n",
      "4916 [D loss: (0.537)(R 0.476, F 0.597)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.231] [G acc: 0.078]\n",
      "4917 [D loss: (0.602)(R 0.604, F 0.600)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.203] [G acc: 0.172]\n",
      "4918 [D loss: (0.566)(R 0.564, F 0.568)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.311] [G acc: 0.109]\n",
      "4919 [D loss: (0.603)(R 0.612, F 0.594)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.150] [G acc: 0.109]\n",
      "4920 [D loss: (0.609)(R 0.655, F 0.563)] [D acc: (0.617)(0.516, 0.719)] [G loss: 1.135] [G acc: 0.172]\n",
      "4921 [D loss: (0.528)(R 0.537, F 0.519)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.268] [G acc: 0.109]\n",
      "4922 [D loss: (0.648)(R 0.584, F 0.713)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.392] [G acc: 0.016]\n",
      "4923 [D loss: (0.570)(R 0.669, F 0.470)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.162] [G acc: 0.125]\n",
      "4924 [D loss: (0.559)(R 0.539, F 0.580)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.152] [G acc: 0.156]\n",
      "4925 [D loss: (0.512)(R 0.550, F 0.474)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.087] [G acc: 0.234]\n",
      "4926 [D loss: (0.521)(R 0.473, F 0.570)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.391] [G acc: 0.125]\n",
      "4927 [D loss: (0.574)(R 0.477, F 0.670)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.255] [G acc: 0.109]\n",
      "4928 [D loss: (0.522)(R 0.560, F 0.484)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.325] [G acc: 0.172]\n",
      "4929 [D loss: (0.537)(R 0.581, F 0.494)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.394] [G acc: 0.125]\n",
      "4930 [D loss: (0.646)(R 0.485, F 0.808)] [D acc: (0.703)(0.734, 0.672)] [G loss: 1.414] [G acc: 0.047]\n",
      "4931 [D loss: (0.667)(R 0.738, F 0.596)] [D acc: (0.641)(0.469, 0.812)] [G loss: 1.244] [G acc: 0.062]\n",
      "4932 [D loss: (0.554)(R 0.653, F 0.455)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.193] [G acc: 0.156]\n",
      "4933 [D loss: (0.506)(R 0.537, F 0.474)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.136] [G acc: 0.078]\n",
      "4934 [D loss: (0.437)(R 0.431, F 0.442)] [D acc: (0.828)(0.766, 0.891)] [G loss: 1.405] [G acc: 0.078]\n",
      "4935 [D loss: (0.564)(R 0.596, F 0.531)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.328] [G acc: 0.125]\n",
      "4936 [D loss: (0.571)(R 0.562, F 0.580)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.386] [G acc: 0.109]\n",
      "4937 [D loss: (0.518)(R 0.524, F 0.513)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.355] [G acc: 0.047]\n",
      "4938 [D loss: (0.591)(R 0.693, F 0.489)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.196] [G acc: 0.203]\n",
      "4939 [D loss: (0.648)(R 0.654, F 0.643)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.269] [G acc: 0.094]\n",
      "4940 [D loss: (0.582)(R 0.663, F 0.502)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.320] [G acc: 0.062]\n",
      "4941 [D loss: (0.651)(R 0.665, F 0.637)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.198] [G acc: 0.109]\n",
      "4942 [D loss: (0.568)(R 0.465, F 0.672)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.279] [G acc: 0.062]\n",
      "4943 [D loss: (0.594)(R 0.648, F 0.540)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.290] [G acc: 0.141]\n",
      "4944 [D loss: (0.526)(R 0.582, F 0.470)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.242] [G acc: 0.141]\n",
      "4945 [D loss: (0.683)(R 0.627, F 0.738)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.131] [G acc: 0.172]\n",
      "4946 [D loss: (0.524)(R 0.578, F 0.469)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.186] [G acc: 0.094]\n",
      "4947 [D loss: (0.608)(R 0.665, F 0.551)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.250] [G acc: 0.047]\n",
      "4948 [D loss: (0.550)(R 0.576, F 0.523)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.254] [G acc: 0.188]\n",
      "4949 [D loss: (0.593)(R 0.553, F 0.634)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.215] [G acc: 0.125]\n",
      "4950 [D loss: (0.504)(R 0.507, F 0.502)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.149] [G acc: 0.156]\n",
      "4951 [D loss: (0.580)(R 0.541, F 0.620)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.126] [G acc: 0.156]\n",
      "4952 [D loss: (0.568)(R 0.561, F 0.575)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.269] [G acc: 0.094]\n",
      "4953 [D loss: (0.616)(R 0.659, F 0.572)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.263] [G acc: 0.078]\n",
      "4954 [D loss: (0.585)(R 0.587, F 0.583)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.191] [G acc: 0.141]\n",
      "4955 [D loss: (0.488)(R 0.471, F 0.504)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.206] [G acc: 0.125]\n",
      "4956 [D loss: (0.501)(R 0.491, F 0.510)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.343] [G acc: 0.062]\n",
      "4957 [D loss: (0.507)(R 0.541, F 0.472)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.088] [G acc: 0.219]\n",
      "4958 [D loss: (0.593)(R 0.623, F 0.562)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.392] [G acc: 0.078]\n",
      "4959 [D loss: (0.678)(R 0.623, F 0.734)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.258] [G acc: 0.109]\n",
      "4960 [D loss: (0.628)(R 0.661, F 0.595)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.166] [G acc: 0.156]\n",
      "4961 [D loss: (0.608)(R 0.631, F 0.585)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.173] [G acc: 0.172]\n",
      "4962 [D loss: (0.568)(R 0.652, F 0.484)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.415] [G acc: 0.078]\n",
      "4963 [D loss: (0.622)(R 0.673, F 0.571)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.160] [G acc: 0.141]\n",
      "4964 [D loss: (0.562)(R 0.535, F 0.590)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.209] [G acc: 0.094]\n",
      "4965 [D loss: (0.551)(R 0.544, F 0.558)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.199] [G acc: 0.109]\n",
      "4966 [D loss: (0.600)(R 0.665, F 0.534)] [D acc: (0.688)(0.500, 0.875)] [G loss: 1.258] [G acc: 0.109]\n",
      "4967 [D loss: (0.657)(R 0.742, F 0.572)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.234] [G acc: 0.141]\n",
      "4968 [D loss: (0.687)(R 0.803, F 0.571)] [D acc: (0.602)(0.438, 0.766)] [G loss: 1.106] [G acc: 0.078]\n",
      "4969 [D loss: (0.571)(R 0.541, F 0.601)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.065] [G acc: 0.125]\n",
      "4970 [D loss: (0.590)(R 0.665, F 0.514)] [D acc: (0.656)(0.484, 0.828)] [G loss: 1.201] [G acc: 0.062]\n",
      "4971 [D loss: (0.566)(R 0.537, F 0.596)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.135] [G acc: 0.203]\n",
      "4972 [D loss: (0.529)(R 0.551, F 0.506)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.176] [G acc: 0.172]\n",
      "4973 [D loss: (0.541)(R 0.517, F 0.564)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.190] [G acc: 0.078]\n",
      "4974 [D loss: (0.603)(R 0.652, F 0.554)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.232] [G acc: 0.109]\n",
      "4975 [D loss: (0.608)(R 0.525, F 0.691)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.245] [G acc: 0.078]\n",
      "4976 [D loss: (0.507)(R 0.494, F 0.520)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.183] [G acc: 0.125]\n",
      "4977 [D loss: (0.577)(R 0.563, F 0.590)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.241] [G acc: 0.141]\n",
      "4978 [D loss: (0.562)(R 0.564, F 0.561)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.263] [G acc: 0.078]\n",
      "4979 [D loss: (0.562)(R 0.535, F 0.589)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.239] [G acc: 0.109]\n",
      "4980 [D loss: (0.557)(R 0.536, F 0.578)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.245] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4981 [D loss: (0.684)(R 0.603, F 0.765)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.247] [G acc: 0.078]\n",
      "4982 [D loss: (0.535)(R 0.635, F 0.435)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.290] [G acc: 0.109]\n",
      "4983 [D loss: (0.619)(R 0.674, F 0.564)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.323] [G acc: 0.094]\n",
      "4984 [D loss: (0.504)(R 0.451, F 0.557)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.168] [G acc: 0.141]\n",
      "4985 [D loss: (0.656)(R 0.522, F 0.790)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.207] [G acc: 0.062]\n",
      "4986 [D loss: (0.587)(R 0.616, F 0.558)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.140] [G acc: 0.094]\n",
      "4987 [D loss: (0.635)(R 0.760, F 0.509)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.197] [G acc: 0.062]\n",
      "4988 [D loss: (0.581)(R 0.592, F 0.570)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.194] [G acc: 0.188]\n",
      "4989 [D loss: (0.587)(R 0.625, F 0.550)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.251] [G acc: 0.094]\n",
      "4990 [D loss: (0.531)(R 0.524, F 0.538)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.201] [G acc: 0.125]\n",
      "4991 [D loss: (0.642)(R 0.580, F 0.704)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.211] [G acc: 0.094]\n",
      "4992 [D loss: (0.583)(R 0.679, F 0.486)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.217] [G acc: 0.109]\n",
      "4993 [D loss: (0.560)(R 0.541, F 0.579)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.231] [G acc: 0.109]\n",
      "4994 [D loss: (0.571)(R 0.597, F 0.544)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.251] [G acc: 0.094]\n",
      "4995 [D loss: (0.594)(R 0.689, F 0.499)] [D acc: (0.656)(0.469, 0.844)] [G loss: 1.117] [G acc: 0.141]\n",
      "4996 [D loss: (0.515)(R 0.545, F 0.485)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.160] [G acc: 0.094]\n",
      "4997 [D loss: (0.605)(R 0.541, F 0.669)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.236] [G acc: 0.078]\n",
      "4998 [D loss: (0.600)(R 0.654, F 0.545)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.267] [G acc: 0.109]\n",
      "4999 [D loss: (0.531)(R 0.573, F 0.488)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.332] [G acc: 0.047]\n",
      "5000 [D loss: (0.584)(R 0.599, F 0.568)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.143] [G acc: 0.203]\n",
      "5001 [D loss: (0.526)(R 0.458, F 0.594)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.288] [G acc: 0.078]\n",
      "5002 [D loss: (0.674)(R 0.656, F 0.692)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.234] [G acc: 0.031]\n",
      "5003 [D loss: (0.572)(R 0.698, F 0.445)] [D acc: (0.680)(0.500, 0.859)] [G loss: 1.227] [G acc: 0.062]\n",
      "5004 [D loss: (0.545)(R 0.564, F 0.526)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.312] [G acc: 0.078]\n",
      "5005 [D loss: (0.557)(R 0.651, F 0.464)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.209] [G acc: 0.141]\n",
      "5006 [D loss: (0.595)(R 0.589, F 0.600)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.118] [G acc: 0.141]\n",
      "5007 [D loss: (0.636)(R 0.599, F 0.673)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.286] [G acc: 0.078]\n",
      "5008 [D loss: (0.604)(R 0.597, F 0.610)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.297] [G acc: 0.094]\n",
      "5009 [D loss: (0.593)(R 0.644, F 0.543)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.162] [G acc: 0.188]\n",
      "5010 [D loss: (0.573)(R 0.677, F 0.469)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.408] [G acc: 0.031]\n",
      "5011 [D loss: (0.614)(R 0.522, F 0.705)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.230] [G acc: 0.062]\n",
      "5012 [D loss: (0.524)(R 0.542, F 0.505)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.138] [G acc: 0.125]\n",
      "5013 [D loss: (0.631)(R 0.641, F 0.622)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.126] [G acc: 0.125]\n",
      "5014 [D loss: (0.584)(R 0.652, F 0.517)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.158] [G acc: 0.125]\n",
      "5015 [D loss: (0.510)(R 0.556, F 0.463)] [D acc: (0.820)(0.750, 0.891)] [G loss: 1.201] [G acc: 0.219]\n",
      "5016 [D loss: (0.633)(R 0.536, F 0.729)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.316] [G acc: 0.062]\n",
      "5017 [D loss: (0.523)(R 0.493, F 0.554)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.222] [G acc: 0.078]\n",
      "5018 [D loss: (0.507)(R 0.515, F 0.499)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.310] [G acc: 0.109]\n",
      "5019 [D loss: (0.536)(R 0.567, F 0.505)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.314] [G acc: 0.094]\n",
      "5020 [D loss: (0.660)(R 0.587, F 0.733)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.221] [G acc: 0.109]\n",
      "5021 [D loss: (0.599)(R 0.642, F 0.556)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.101] [G acc: 0.125]\n",
      "5022 [D loss: (0.530)(R 0.585, F 0.475)] [D acc: (0.781)(0.688, 0.875)] [G loss: 1.276] [G acc: 0.062]\n",
      "5023 [D loss: (0.561)(R 0.573, F 0.549)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.441] [G acc: 0.125]\n",
      "5024 [D loss: (0.589)(R 0.492, F 0.686)] [D acc: (0.734)(0.766, 0.703)] [G loss: 1.367] [G acc: 0.109]\n",
      "5025 [D loss: (0.614)(R 0.666, F 0.562)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.264] [G acc: 0.109]\n",
      "5026 [D loss: (0.602)(R 0.558, F 0.646)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.187] [G acc: 0.125]\n",
      "5027 [D loss: (0.546)(R 0.533, F 0.558)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.170] [G acc: 0.125]\n",
      "5028 [D loss: (0.546)(R 0.553, F 0.539)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.241] [G acc: 0.109]\n",
      "5029 [D loss: (0.585)(R 0.546, F 0.624)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.247] [G acc: 0.125]\n",
      "5030 [D loss: (0.617)(R 0.627, F 0.607)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.180] [G acc: 0.125]\n",
      "5031 [D loss: (0.551)(R 0.478, F 0.624)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.323] [G acc: 0.094]\n",
      "5032 [D loss: (0.532)(R 0.580, F 0.484)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.270] [G acc: 0.047]\n",
      "5033 [D loss: (0.608)(R 0.606, F 0.610)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.298] [G acc: 0.094]\n",
      "5034 [D loss: (0.499)(R 0.534, F 0.464)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.270] [G acc: 0.125]\n",
      "5035 [D loss: (0.602)(R 0.550, F 0.655)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.288] [G acc: 0.094]\n",
      "5036 [D loss: (0.634)(R 0.681, F 0.588)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.281] [G acc: 0.047]\n",
      "5037 [D loss: (0.608)(R 0.696, F 0.520)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.144] [G acc: 0.125]\n",
      "5038 [D loss: (0.532)(R 0.541, F 0.522)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.251] [G acc: 0.094]\n",
      "5039 [D loss: (0.623)(R 0.658, F 0.587)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.172] [G acc: 0.172]\n",
      "5040 [D loss: (0.614)(R 0.579, F 0.649)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.094] [G acc: 0.109]\n",
      "5041 [D loss: (0.627)(R 0.569, F 0.685)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.186] [G acc: 0.078]\n",
      "5042 [D loss: (0.576)(R 0.695, F 0.456)] [D acc: (0.688)(0.516, 0.859)] [G loss: 1.195] [G acc: 0.156]\n",
      "5043 [D loss: (0.575)(R 0.517, F 0.633)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.282] [G acc: 0.047]\n",
      "5044 [D loss: (0.527)(R 0.567, F 0.487)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.257] [G acc: 0.078]\n",
      "5045 [D loss: (0.613)(R 0.689, F 0.536)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.122] [G acc: 0.109]\n",
      "5046 [D loss: (0.546)(R 0.525, F 0.568)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.137] [G acc: 0.219]\n",
      "5047 [D loss: (0.559)(R 0.575, F 0.542)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.232] [G acc: 0.141]\n",
      "5048 [D loss: (0.677)(R 0.502, F 0.851)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.142] [G acc: 0.094]\n",
      "5049 [D loss: (0.552)(R 0.661, F 0.444)] [D acc: (0.703)(0.531, 0.875)] [G loss: 1.186] [G acc: 0.078]\n",
      "5050 [D loss: (0.665)(R 0.704, F 0.627)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.079] [G acc: 0.188]\n",
      "5051 [D loss: (0.589)(R 0.627, F 0.552)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.184] [G acc: 0.078]\n",
      "5052 [D loss: (0.529)(R 0.496, F 0.562)] [D acc: (0.797)(0.719, 0.875)] [G loss: 1.169] [G acc: 0.109]\n",
      "5053 [D loss: (0.531)(R 0.558, F 0.504)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.186] [G acc: 0.156]\n",
      "5054 [D loss: (0.644)(R 0.549, F 0.740)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.194] [G acc: 0.156]\n",
      "5055 [D loss: (0.562)(R 0.594, F 0.530)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.112] [G acc: 0.141]\n",
      "5056 [D loss: (0.536)(R 0.523, F 0.550)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.112] [G acc: 0.172]\n",
      "5057 [D loss: (0.517)(R 0.511, F 0.522)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.251] [G acc: 0.094]\n",
      "5058 [D loss: (0.443)(R 0.450, F 0.437)] [D acc: (0.805)(0.703, 0.906)] [G loss: 1.280] [G acc: 0.094]\n",
      "5059 [D loss: (0.608)(R 0.565, F 0.651)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.231] [G acc: 0.078]\n",
      "5060 [D loss: (0.557)(R 0.620, F 0.494)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.162] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5061 [D loss: (0.547)(R 0.591, F 0.503)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.149] [G acc: 0.156]\n",
      "5062 [D loss: (0.594)(R 0.633, F 0.555)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.191] [G acc: 0.094]\n",
      "5063 [D loss: (0.602)(R 0.578, F 0.626)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.283] [G acc: 0.031]\n",
      "5064 [D loss: (0.600)(R 0.649, F 0.551)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.370] [G acc: 0.047]\n",
      "5065 [D loss: (0.580)(R 0.572, F 0.588)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.207] [G acc: 0.109]\n",
      "5066 [D loss: (0.621)(R 0.570, F 0.672)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.216] [G acc: 0.078]\n",
      "5067 [D loss: (0.607)(R 0.603, F 0.610)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.142] [G acc: 0.062]\n",
      "5068 [D loss: (0.612)(R 0.689, F 0.535)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.153] [G acc: 0.031]\n",
      "5069 [D loss: (0.555)(R 0.640, F 0.471)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.235] [G acc: 0.094]\n",
      "5070 [D loss: (0.622)(R 0.625, F 0.619)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.179] [G acc: 0.094]\n",
      "5071 [D loss: (0.523)(R 0.565, F 0.481)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.108] [G acc: 0.141]\n",
      "5072 [D loss: (0.592)(R 0.594, F 0.590)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.165] [G acc: 0.062]\n",
      "5073 [D loss: (0.504)(R 0.541, F 0.467)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.143] [G acc: 0.141]\n",
      "5074 [D loss: (0.666)(R 0.652, F 0.680)] [D acc: (0.578)(0.484, 0.672)] [G loss: 1.198] [G acc: 0.047]\n",
      "5075 [D loss: (0.608)(R 0.703, F 0.514)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.111] [G acc: 0.125]\n",
      "5076 [D loss: (0.565)(R 0.556, F 0.573)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.046] [G acc: 0.203]\n",
      "5077 [D loss: (0.535)(R 0.493, F 0.578)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.234] [G acc: 0.047]\n",
      "5078 [D loss: (0.529)(R 0.528, F 0.529)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.409] [G acc: 0.078]\n",
      "5079 [D loss: (0.596)(R 0.665, F 0.528)] [D acc: (0.641)(0.531, 0.750)] [G loss: 1.347] [G acc: 0.047]\n",
      "5080 [D loss: (0.496)(R 0.512, F 0.480)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.236] [G acc: 0.172]\n",
      "5081 [D loss: (0.707)(R 0.523, F 0.891)] [D acc: (0.609)(0.609, 0.609)] [G loss: 1.240] [G acc: 0.047]\n",
      "5082 [D loss: (0.652)(R 0.711, F 0.593)] [D acc: (0.594)(0.453, 0.734)] [G loss: 1.235] [G acc: 0.094]\n",
      "5083 [D loss: (0.597)(R 0.619, F 0.574)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.262] [G acc: 0.094]\n",
      "5084 [D loss: (0.625)(R 0.651, F 0.600)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.232] [G acc: 0.078]\n",
      "5085 [D loss: (0.540)(R 0.593, F 0.488)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.125] [G acc: 0.219]\n",
      "5086 [D loss: (0.528)(R 0.485, F 0.570)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.175] [G acc: 0.094]\n",
      "5087 [D loss: (0.554)(R 0.614, F 0.494)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.427] [G acc: 0.062]\n",
      "5088 [D loss: (0.678)(R 0.610, F 0.747)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.282] [G acc: 0.078]\n",
      "5089 [D loss: (0.557)(R 0.678, F 0.436)] [D acc: (0.742)(0.547, 0.938)] [G loss: 1.220] [G acc: 0.109]\n",
      "5090 [D loss: (0.579)(R 0.643, F 0.515)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.116] [G acc: 0.109]\n",
      "5091 [D loss: (0.649)(R 0.574, F 0.724)] [D acc: (0.609)(0.625, 0.594)] [G loss: 1.210] [G acc: 0.062]\n",
      "5092 [D loss: (0.628)(R 0.727, F 0.529)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.174] [G acc: 0.125]\n",
      "5093 [D loss: (0.493)(R 0.484, F 0.501)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.231] [G acc: 0.109]\n",
      "5094 [D loss: (0.635)(R 0.488, F 0.782)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.137] [G acc: 0.094]\n",
      "5095 [D loss: (0.550)(R 0.581, F 0.519)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.155] [G acc: 0.188]\n",
      "5096 [D loss: (0.485)(R 0.458, F 0.513)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.213] [G acc: 0.172]\n",
      "5097 [D loss: (0.528)(R 0.446, F 0.610)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.233] [G acc: 0.141]\n",
      "5098 [D loss: (0.623)(R 0.635, F 0.611)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.349] [G acc: 0.031]\n",
      "5099 [D loss: (0.565)(R 0.645, F 0.486)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.324] [G acc: 0.016]\n",
      "5100 [D loss: (0.555)(R 0.669, F 0.442)] [D acc: (0.750)(0.578, 0.922)] [G loss: 1.257] [G acc: 0.125]\n",
      "5101 [D loss: (0.628)(R 0.635, F 0.622)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.148] [G acc: 0.109]\n",
      "5102 [D loss: (0.537)(R 0.544, F 0.530)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.206] [G acc: 0.125]\n",
      "5103 [D loss: (0.639)(R 0.498, F 0.780)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.269] [G acc: 0.125]\n",
      "5104 [D loss: (0.556)(R 0.603, F 0.508)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.204] [G acc: 0.094]\n",
      "5105 [D loss: (0.569)(R 0.569, F 0.570)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.242] [G acc: 0.125]\n",
      "5106 [D loss: (0.572)(R 0.573, F 0.571)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.216] [G acc: 0.109]\n",
      "5107 [D loss: (0.611)(R 0.631, F 0.590)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.183] [G acc: 0.109]\n",
      "5108 [D loss: (0.733)(R 0.760, F 0.706)] [D acc: (0.602)(0.531, 0.672)] [G loss: 1.339] [G acc: 0.094]\n",
      "5109 [D loss: (0.578)(R 0.630, F 0.526)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.197] [G acc: 0.125]\n",
      "5110 [D loss: (0.584)(R 0.581, F 0.588)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.067] [G acc: 0.156]\n",
      "5111 [D loss: (0.627)(R 0.616, F 0.638)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.153] [G acc: 0.141]\n",
      "5112 [D loss: (0.569)(R 0.570, F 0.568)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.023] [G acc: 0.203]\n",
      "5113 [D loss: (0.600)(R 0.660, F 0.541)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.164] [G acc: 0.062]\n",
      "5114 [D loss: (0.596)(R 0.637, F 0.556)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.084] [G acc: 0.156]\n",
      "5115 [D loss: (0.590)(R 0.582, F 0.599)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.144] [G acc: 0.109]\n",
      "5116 [D loss: (0.575)(R 0.571, F 0.579)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.069] [G acc: 0.156]\n",
      "5117 [D loss: (0.596)(R 0.571, F 0.621)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.105] [G acc: 0.125]\n",
      "5118 [D loss: (0.589)(R 0.619, F 0.560)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.150] [G acc: 0.141]\n",
      "5119 [D loss: (0.534)(R 0.589, F 0.479)] [D acc: (0.742)(0.578, 0.906)] [G loss: 1.043] [G acc: 0.188]\n",
      "5120 [D loss: (0.640)(R 0.565, F 0.715)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.178] [G acc: 0.062]\n",
      "5121 [D loss: (0.574)(R 0.559, F 0.589)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.209] [G acc: 0.109]\n",
      "5122 [D loss: (0.582)(R 0.683, F 0.481)] [D acc: (0.703)(0.531, 0.875)] [G loss: 1.211] [G acc: 0.125]\n",
      "5123 [D loss: (0.517)(R 0.507, F 0.527)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.146] [G acc: 0.203]\n",
      "5124 [D loss: (0.568)(R 0.566, F 0.569)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.140] [G acc: 0.094]\n",
      "5125 [D loss: (0.513)(R 0.526, F 0.500)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.258] [G acc: 0.125]\n",
      "5126 [D loss: (0.543)(R 0.469, F 0.616)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.195] [G acc: 0.125]\n",
      "5127 [D loss: (0.528)(R 0.517, F 0.540)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.282] [G acc: 0.141]\n",
      "5128 [D loss: (0.559)(R 0.562, F 0.557)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.325] [G acc: 0.125]\n",
      "5129 [D loss: (0.548)(R 0.526, F 0.570)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.308] [G acc: 0.109]\n",
      "5130 [D loss: (0.607)(R 0.518, F 0.696)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.492] [G acc: 0.109]\n",
      "5131 [D loss: (0.638)(R 0.659, F 0.617)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.372] [G acc: 0.047]\n",
      "5132 [D loss: (0.623)(R 0.674, F 0.572)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.176] [G acc: 0.141]\n",
      "5133 [D loss: (0.514)(R 0.516, F 0.513)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.235] [G acc: 0.047]\n",
      "5134 [D loss: (0.515)(R 0.480, F 0.549)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.176] [G acc: 0.188]\n",
      "5135 [D loss: (0.557)(R 0.596, F 0.518)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.196] [G acc: 0.125]\n",
      "5136 [D loss: (0.614)(R 0.587, F 0.641)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.233] [G acc: 0.078]\n",
      "5137 [D loss: (0.561)(R 0.609, F 0.512)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.192] [G acc: 0.172]\n",
      "5138 [D loss: (0.579)(R 0.566, F 0.591)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.203] [G acc: 0.109]\n",
      "5139 [D loss: (0.567)(R 0.530, F 0.603)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.235] [G acc: 0.094]\n",
      "5140 [D loss: (0.583)(R 0.615, F 0.551)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.304] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5141 [D loss: (0.616)(R 0.589, F 0.644)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.263] [G acc: 0.094]\n",
      "5142 [D loss: (0.530)(R 0.556, F 0.504)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.211] [G acc: 0.172]\n",
      "5143 [D loss: (0.548)(R 0.568, F 0.528)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.182] [G acc: 0.172]\n",
      "5144 [D loss: (0.643)(R 0.576, F 0.710)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.260] [G acc: 0.109]\n",
      "5145 [D loss: (0.512)(R 0.506, F 0.518)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.355] [G acc: 0.125]\n",
      "5146 [D loss: (0.566)(R 0.541, F 0.592)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.221] [G acc: 0.109]\n",
      "5147 [D loss: (0.598)(R 0.594, F 0.603)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.268] [G acc: 0.047]\n",
      "5148 [D loss: (0.548)(R 0.577, F 0.519)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.223] [G acc: 0.125]\n",
      "5149 [D loss: (0.639)(R 0.665, F 0.613)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.242] [G acc: 0.109]\n",
      "5150 [D loss: (0.633)(R 0.662, F 0.604)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.159] [G acc: 0.141]\n",
      "5151 [D loss: (0.596)(R 0.652, F 0.541)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.221] [G acc: 0.047]\n",
      "5152 [D loss: (0.529)(R 0.515, F 0.544)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.267] [G acc: 0.094]\n",
      "5153 [D loss: (0.553)(R 0.565, F 0.541)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.171] [G acc: 0.109]\n",
      "5154 [D loss: (0.624)(R 0.651, F 0.596)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.150] [G acc: 0.078]\n",
      "5155 [D loss: (0.548)(R 0.546, F 0.551)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.265] [G acc: 0.078]\n",
      "5156 [D loss: (0.561)(R 0.668, F 0.453)] [D acc: (0.727)(0.516, 0.938)] [G loss: 1.161] [G acc: 0.156]\n",
      "5157 [D loss: (0.528)(R 0.522, F 0.534)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.082] [G acc: 0.188]\n",
      "5158 [D loss: (0.542)(R 0.574, F 0.509)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.299] [G acc: 0.031]\n",
      "5159 [D loss: (0.637)(R 0.568, F 0.705)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.199] [G acc: 0.078]\n",
      "5160 [D loss: (0.557)(R 0.588, F 0.525)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.268] [G acc: 0.062]\n",
      "5161 [D loss: (0.604)(R 0.548, F 0.660)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.243] [G acc: 0.156]\n",
      "5162 [D loss: (0.524)(R 0.497, F 0.551)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.311] [G acc: 0.141]\n",
      "5163 [D loss: (0.617)(R 0.688, F 0.545)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.183] [G acc: 0.141]\n",
      "5164 [D loss: (0.548)(R 0.472, F 0.623)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.334] [G acc: 0.047]\n",
      "5165 [D loss: (0.665)(R 0.604, F 0.727)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.253] [G acc: 0.062]\n",
      "5166 [D loss: (0.621)(R 0.619, F 0.623)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.130] [G acc: 0.125]\n",
      "5167 [D loss: (0.582)(R 0.664, F 0.501)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.041] [G acc: 0.141]\n",
      "5168 [D loss: (0.545)(R 0.535, F 0.555)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.244] [G acc: 0.109]\n",
      "5169 [D loss: (0.599)(R 0.650, F 0.548)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.203] [G acc: 0.062]\n",
      "5170 [D loss: (0.503)(R 0.584, F 0.422)] [D acc: (0.734)(0.578, 0.891)] [G loss: 1.108] [G acc: 0.141]\n",
      "5171 [D loss: (0.555)(R 0.564, F 0.545)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.252] [G acc: 0.109]\n",
      "5172 [D loss: (0.481)(R 0.491, F 0.470)] [D acc: (0.805)(0.719, 0.891)] [G loss: 1.257] [G acc: 0.062]\n",
      "5173 [D loss: (0.653)(R 0.634, F 0.672)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.242] [G acc: 0.078]\n",
      "5174 [D loss: (0.640)(R 0.645, F 0.636)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.193] [G acc: 0.125]\n",
      "5175 [D loss: (0.636)(R 0.749, F 0.522)] [D acc: (0.602)(0.453, 0.750)] [G loss: 1.220] [G acc: 0.141]\n",
      "5176 [D loss: (0.638)(R 0.576, F 0.701)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.133] [G acc: 0.125]\n",
      "5177 [D loss: (0.573)(R 0.553, F 0.593)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.259] [G acc: 0.062]\n",
      "5178 [D loss: (0.569)(R 0.558, F 0.581)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.210] [G acc: 0.141]\n",
      "5179 [D loss: (0.570)(R 0.599, F 0.541)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.078] [G acc: 0.172]\n",
      "5180 [D loss: (0.589)(R 0.545, F 0.634)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.102] [G acc: 0.188]\n",
      "5181 [D loss: (0.571)(R 0.573, F 0.570)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.131] [G acc: 0.188]\n",
      "5182 [D loss: (0.512)(R 0.418, F 0.607)] [D acc: (0.742)(0.766, 0.719)] [G loss: 1.237] [G acc: 0.094]\n",
      "5183 [D loss: (0.596)(R 0.574, F 0.618)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.233] [G acc: 0.156]\n",
      "5184 [D loss: (0.596)(R 0.591, F 0.601)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.259] [G acc: 0.125]\n",
      "5185 [D loss: (0.575)(R 0.581, F 0.569)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.233] [G acc: 0.109]\n",
      "5186 [D loss: (0.557)(R 0.553, F 0.560)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.209] [G acc: 0.078]\n",
      "5187 [D loss: (0.573)(R 0.518, F 0.627)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.237] [G acc: 0.094]\n",
      "5188 [D loss: (0.520)(R 0.536, F 0.503)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.340] [G acc: 0.109]\n",
      "5189 [D loss: (0.557)(R 0.526, F 0.588)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.283] [G acc: 0.078]\n",
      "5190 [D loss: (0.593)(R 0.614, F 0.572)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.245] [G acc: 0.047]\n",
      "5191 [D loss: (0.499)(R 0.547, F 0.452)] [D acc: (0.750)(0.578, 0.922)] [G loss: 1.248] [G acc: 0.141]\n",
      "5192 [D loss: (0.585)(R 0.579, F 0.591)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.241] [G acc: 0.141]\n",
      "5193 [D loss: (0.554)(R 0.619, F 0.489)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.368] [G acc: 0.047]\n",
      "5194 [D loss: (0.611)(R 0.542, F 0.680)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.373] [G acc: 0.078]\n",
      "5195 [D loss: (0.561)(R 0.603, F 0.519)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.233] [G acc: 0.094]\n",
      "5196 [D loss: (0.516)(R 0.512, F 0.520)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.179] [G acc: 0.078]\n",
      "5197 [D loss: (0.647)(R 0.675, F 0.618)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.317] [G acc: 0.047]\n",
      "5198 [D loss: (0.502)(R 0.534, F 0.470)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.290] [G acc: 0.109]\n",
      "5199 [D loss: (0.538)(R 0.470, F 0.607)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.223] [G acc: 0.125]\n",
      "5200 [D loss: (0.519)(R 0.534, F 0.504)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.386] [G acc: 0.094]\n",
      "5201 [D loss: (0.607)(R 0.561, F 0.654)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.365] [G acc: 0.078]\n",
      "5202 [D loss: (0.651)(R 0.700, F 0.602)] [D acc: (0.711)(0.531, 0.891)] [G loss: 1.433] [G acc: 0.094]\n",
      "5203 [D loss: (0.640)(R 0.662, F 0.618)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.269] [G acc: 0.078]\n",
      "5204 [D loss: (0.666)(R 0.729, F 0.604)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.178] [G acc: 0.125]\n",
      "5205 [D loss: (0.594)(R 0.679, F 0.508)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.250] [G acc: 0.062]\n",
      "5206 [D loss: (0.553)(R 0.677, F 0.430)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.182] [G acc: 0.188]\n",
      "5207 [D loss: (0.598)(R 0.596, F 0.599)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.164] [G acc: 0.109]\n",
      "5208 [D loss: (0.592)(R 0.552, F 0.631)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.364] [G acc: 0.047]\n",
      "5209 [D loss: (0.522)(R 0.592, F 0.453)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.179] [G acc: 0.156]\n",
      "5210 [D loss: (0.566)(R 0.539, F 0.593)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.214] [G acc: 0.031]\n",
      "5211 [D loss: (0.517)(R 0.538, F 0.495)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.226] [G acc: 0.172]\n",
      "5212 [D loss: (0.548)(R 0.578, F 0.518)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.228] [G acc: 0.125]\n",
      "5213 [D loss: (0.541)(R 0.504, F 0.579)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.370] [G acc: 0.094]\n",
      "5214 [D loss: (0.678)(R 0.671, F 0.685)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.156] [G acc: 0.156]\n",
      "5215 [D loss: (0.586)(R 0.580, F 0.592)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.223] [G acc: 0.109]\n",
      "5216 [D loss: (0.588)(R 0.633, F 0.543)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.260] [G acc: 0.047]\n",
      "5217 [D loss: (0.545)(R 0.595, F 0.496)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.263] [G acc: 0.031]\n",
      "5218 [D loss: (0.484)(R 0.466, F 0.501)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.272] [G acc: 0.141]\n",
      "5219 [D loss: (0.594)(R 0.654, F 0.533)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.295] [G acc: 0.156]\n",
      "5220 [D loss: (0.642)(R 0.519, F 0.765)] [D acc: (0.648)(0.688, 0.609)] [G loss: 1.384] [G acc: 0.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5221 [D loss: (0.570)(R 0.562, F 0.578)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.330] [G acc: 0.031]\n",
      "5222 [D loss: (0.629)(R 0.672, F 0.586)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.214] [G acc: 0.094]\n",
      "5223 [D loss: (0.623)(R 0.683, F 0.563)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.189] [G acc: 0.094]\n",
      "5224 [D loss: (0.495)(R 0.529, F 0.460)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.266] [G acc: 0.109]\n",
      "5225 [D loss: (0.618)(R 0.633, F 0.602)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.260] [G acc: 0.125]\n",
      "5226 [D loss: (0.533)(R 0.536, F 0.531)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.341] [G acc: 0.094]\n",
      "5227 [D loss: (0.621)(R 0.493, F 0.749)] [D acc: (0.727)(0.781, 0.672)] [G loss: 1.274] [G acc: 0.125]\n",
      "5228 [D loss: (0.584)(R 0.689, F 0.479)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.236] [G acc: 0.094]\n",
      "5229 [D loss: (0.641)(R 0.710, F 0.573)] [D acc: (0.586)(0.422, 0.750)] [G loss: 1.229] [G acc: 0.062]\n",
      "5230 [D loss: (0.617)(R 0.620, F 0.614)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.293] [G acc: 0.078]\n",
      "5231 [D loss: (0.575)(R 0.526, F 0.623)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.318] [G acc: 0.094]\n",
      "5232 [D loss: (0.553)(R 0.593, F 0.512)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.225] [G acc: 0.125]\n",
      "5233 [D loss: (0.534)(R 0.574, F 0.493)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.243] [G acc: 0.109]\n",
      "5234 [D loss: (0.559)(R 0.440, F 0.678)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.250] [G acc: 0.109]\n",
      "5235 [D loss: (0.574)(R 0.644, F 0.504)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.482] [G acc: 0.078]\n",
      "5236 [D loss: (0.566)(R 0.527, F 0.606)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.276] [G acc: 0.062]\n",
      "5237 [D loss: (0.652)(R 0.648, F 0.655)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.310] [G acc: 0.031]\n",
      "5238 [D loss: (0.630)(R 0.752, F 0.508)] [D acc: (0.656)(0.438, 0.875)] [G loss: 1.144] [G acc: 0.078]\n",
      "5239 [D loss: (0.525)(R 0.593, F 0.458)] [D acc: (0.734)(0.562, 0.906)] [G loss: 1.153] [G acc: 0.109]\n",
      "5240 [D loss: (0.581)(R 0.569, F 0.592)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.225] [G acc: 0.109]\n",
      "5241 [D loss: (0.533)(R 0.503, F 0.563)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.224] [G acc: 0.141]\n",
      "5242 [D loss: (0.614)(R 0.640, F 0.588)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.197] [G acc: 0.125]\n",
      "5243 [D loss: (0.602)(R 0.605, F 0.600)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.213] [G acc: 0.109]\n",
      "5244 [D loss: (0.462)(R 0.465, F 0.458)] [D acc: (0.836)(0.766, 0.906)] [G loss: 1.357] [G acc: 0.109]\n",
      "5245 [D loss: (0.634)(R 0.651, F 0.616)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.257] [G acc: 0.109]\n",
      "5246 [D loss: (0.581)(R 0.550, F 0.612)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.020] [G acc: 0.141]\n",
      "5247 [D loss: (0.578)(R 0.605, F 0.550)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.160] [G acc: 0.125]\n",
      "5248 [D loss: (0.534)(R 0.461, F 0.607)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.317] [G acc: 0.141]\n",
      "5249 [D loss: (0.568)(R 0.573, F 0.564)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.188] [G acc: 0.141]\n",
      "5250 [D loss: (0.566)(R 0.550, F 0.583)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.345] [G acc: 0.047]\n",
      "5251 [D loss: (0.627)(R 0.643, F 0.610)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.231] [G acc: 0.125]\n",
      "5252 [D loss: (0.521)(R 0.523, F 0.518)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.295] [G acc: 0.094]\n",
      "5253 [D loss: (0.566)(R 0.536, F 0.596)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.205] [G acc: 0.109]\n",
      "5254 [D loss: (0.535)(R 0.548, F 0.522)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.084] [G acc: 0.125]\n",
      "5255 [D loss: (0.581)(R 0.609, F 0.554)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.277] [G acc: 0.078]\n",
      "5256 [D loss: (0.674)(R 0.470, F 0.879)] [D acc: (0.703)(0.750, 0.656)] [G loss: 1.266] [G acc: 0.031]\n",
      "5257 [D loss: (0.634)(R 0.779, F 0.489)] [D acc: (0.664)(0.484, 0.844)] [G loss: 1.336] [G acc: 0.047]\n",
      "5258 [D loss: (0.556)(R 0.623, F 0.489)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.113] [G acc: 0.172]\n",
      "5259 [D loss: (0.636)(R 0.599, F 0.673)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.190] [G acc: 0.031]\n",
      "5260 [D loss: (0.545)(R 0.625, F 0.465)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.073] [G acc: 0.141]\n",
      "5261 [D loss: (0.610)(R 0.654, F 0.566)] [D acc: (0.641)(0.469, 0.812)] [G loss: 1.168] [G acc: 0.094]\n",
      "5262 [D loss: (0.540)(R 0.620, F 0.461)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.248] [G acc: 0.094]\n",
      "5263 [D loss: (0.588)(R 0.572, F 0.604)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.166] [G acc: 0.172]\n",
      "5264 [D loss: (0.575)(R 0.577, F 0.573)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.284] [G acc: 0.141]\n",
      "5265 [D loss: (0.487)(R 0.439, F 0.535)] [D acc: (0.781)(0.766, 0.797)] [G loss: 1.260] [G acc: 0.172]\n",
      "5266 [D loss: (0.619)(R 0.471, F 0.767)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.232] [G acc: 0.109]\n",
      "5267 [D loss: (0.590)(R 0.613, F 0.567)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.225] [G acc: 0.062]\n",
      "5268 [D loss: (0.630)(R 0.599, F 0.662)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.112] [G acc: 0.141]\n",
      "5269 [D loss: (0.542)(R 0.667, F 0.418)] [D acc: (0.719)(0.531, 0.906)] [G loss: 1.183] [G acc: 0.109]\n",
      "5270 [D loss: (0.568)(R 0.557, F 0.579)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.289] [G acc: 0.078]\n",
      "5271 [D loss: (0.585)(R 0.568, F 0.601)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.221] [G acc: 0.094]\n",
      "5272 [D loss: (0.625)(R 0.468, F 0.783)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.191] [G acc: 0.094]\n",
      "5273 [D loss: (0.555)(R 0.573, F 0.537)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.206] [G acc: 0.125]\n",
      "5274 [D loss: (0.631)(R 0.551, F 0.712)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.219] [G acc: 0.125]\n",
      "5275 [D loss: (0.554)(R 0.636, F 0.472)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.246] [G acc: 0.109]\n",
      "5276 [D loss: (0.619)(R 0.700, F 0.538)] [D acc: (0.633)(0.438, 0.828)] [G loss: 1.295] [G acc: 0.094]\n",
      "5277 [D loss: (0.560)(R 0.577, F 0.542)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.117] [G acc: 0.141]\n",
      "5278 [D loss: (0.596)(R 0.579, F 0.612)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.269] [G acc: 0.125]\n",
      "5279 [D loss: (0.641)(R 0.624, F 0.658)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.140] [G acc: 0.156]\n",
      "5280 [D loss: (0.577)(R 0.592, F 0.563)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.113] [G acc: 0.156]\n",
      "5281 [D loss: (0.589)(R 0.637, F 0.541)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.128] [G acc: 0.141]\n",
      "5282 [D loss: (0.532)(R 0.530, F 0.534)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.170] [G acc: 0.125]\n",
      "5283 [D loss: (0.603)(R 0.553, F 0.654)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.232] [G acc: 0.109]\n",
      "5284 [D loss: (0.604)(R 0.620, F 0.589)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.256] [G acc: 0.109]\n",
      "5285 [D loss: (0.590)(R 0.609, F 0.570)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.240] [G acc: 0.125]\n",
      "5286 [D loss: (0.513)(R 0.486, F 0.540)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.292] [G acc: 0.125]\n",
      "5287 [D loss: (0.444)(R 0.404, F 0.484)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.393] [G acc: 0.156]\n",
      "5288 [D loss: (0.577)(R 0.603, F 0.552)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.084] [G acc: 0.250]\n",
      "5289 [D loss: (0.578)(R 0.489, F 0.667)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.426] [G acc: 0.078]\n",
      "5290 [D loss: (0.575)(R 0.623, F 0.527)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.176] [G acc: 0.062]\n",
      "5291 [D loss: (0.558)(R 0.561, F 0.556)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.222] [G acc: 0.094]\n",
      "5292 [D loss: (0.506)(R 0.500, F 0.512)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.270] [G acc: 0.141]\n",
      "5293 [D loss: (0.527)(R 0.574, F 0.479)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.350] [G acc: 0.016]\n",
      "5294 [D loss: (0.524)(R 0.461, F 0.586)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.384] [G acc: 0.062]\n",
      "5295 [D loss: (0.551)(R 0.559, F 0.543)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.310] [G acc: 0.094]\n",
      "5296 [D loss: (0.585)(R 0.665, F 0.505)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.217] [G acc: 0.203]\n",
      "5297 [D loss: (0.654)(R 0.633, F 0.675)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.252] [G acc: 0.125]\n",
      "5298 [D loss: (0.563)(R 0.485, F 0.640)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.296] [G acc: 0.062]\n",
      "5299 [D loss: (0.615)(R 0.668, F 0.562)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.275] [G acc: 0.094]\n",
      "5300 [D loss: (0.544)(R 0.557, F 0.530)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.254] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5301 [D loss: (0.585)(R 0.568, F 0.602)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.208] [G acc: 0.094]\n",
      "5302 [D loss: (0.481)(R 0.451, F 0.510)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.379] [G acc: 0.188]\n",
      "5303 [D loss: (0.586)(R 0.674, F 0.497)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.372] [G acc: 0.062]\n",
      "5304 [D loss: (0.498)(R 0.491, F 0.505)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.322] [G acc: 0.078]\n",
      "5305 [D loss: (0.558)(R 0.594, F 0.522)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.312] [G acc: 0.062]\n",
      "5306 [D loss: (0.643)(R 0.510, F 0.775)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.387] [G acc: 0.062]\n",
      "5307 [D loss: (0.627)(R 0.754, F 0.499)] [D acc: (0.625)(0.406, 0.844)] [G loss: 1.362] [G acc: 0.125]\n",
      "5308 [D loss: (0.592)(R 0.605, F 0.580)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.326] [G acc: 0.109]\n",
      "5309 [D loss: (0.663)(R 0.572, F 0.755)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.264] [G acc: 0.094]\n",
      "5310 [D loss: (0.569)(R 0.700, F 0.439)] [D acc: (0.672)(0.484, 0.859)] [G loss: 1.184] [G acc: 0.094]\n",
      "5311 [D loss: (0.548)(R 0.666, F 0.431)] [D acc: (0.742)(0.531, 0.953)] [G loss: 1.256] [G acc: 0.125]\n",
      "5312 [D loss: (0.522)(R 0.583, F 0.460)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.166] [G acc: 0.156]\n",
      "5313 [D loss: (0.644)(R 0.572, F 0.715)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.220] [G acc: 0.078]\n",
      "5314 [D loss: (0.563)(R 0.646, F 0.480)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.189] [G acc: 0.125]\n",
      "5315 [D loss: (0.625)(R 0.526, F 0.723)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.116] [G acc: 0.109]\n",
      "5316 [D loss: (0.550)(R 0.536, F 0.564)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.171] [G acc: 0.141]\n",
      "5317 [D loss: (0.652)(R 0.635, F 0.670)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.107] [G acc: 0.203]\n",
      "5318 [D loss: (0.592)(R 0.638, F 0.547)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.068] [G acc: 0.141]\n",
      "5319 [D loss: (0.620)(R 0.655, F 0.585)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.147] [G acc: 0.188]\n",
      "5320 [D loss: (0.503)(R 0.512, F 0.495)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.209] [G acc: 0.156]\n",
      "5321 [D loss: (0.573)(R 0.541, F 0.604)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.359] [G acc: 0.078]\n",
      "5322 [D loss: (0.545)(R 0.631, F 0.458)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.265] [G acc: 0.141]\n",
      "5323 [D loss: (0.623)(R 0.583, F 0.663)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.328] [G acc: 0.094]\n",
      "5324 [D loss: (0.634)(R 0.624, F 0.645)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.397] [G acc: 0.078]\n",
      "5325 [D loss: (0.575)(R 0.550, F 0.601)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.268] [G acc: 0.078]\n",
      "5326 [D loss: (0.564)(R 0.577, F 0.550)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.336] [G acc: 0.094]\n",
      "5327 [D loss: (0.553)(R 0.567, F 0.540)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.269] [G acc: 0.141]\n",
      "5328 [D loss: (0.533)(R 0.492, F 0.575)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.278] [G acc: 0.125]\n",
      "5329 [D loss: (0.615)(R 0.706, F 0.524)] [D acc: (0.625)(0.469, 0.781)] [G loss: 1.228] [G acc: 0.125]\n",
      "5330 [D loss: (0.623)(R 0.515, F 0.732)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.307] [G acc: 0.156]\n",
      "5331 [D loss: (0.580)(R 0.636, F 0.525)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.287] [G acc: 0.062]\n",
      "5332 [D loss: (0.584)(R 0.581, F 0.588)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.247] [G acc: 0.156]\n",
      "5333 [D loss: (0.584)(R 0.650, F 0.517)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.279] [G acc: 0.156]\n",
      "5334 [D loss: (0.566)(R 0.574, F 0.558)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.261] [G acc: 0.156]\n",
      "5335 [D loss: (0.498)(R 0.516, F 0.480)] [D acc: (0.781)(0.688, 0.875)] [G loss: 1.488] [G acc: 0.141]\n",
      "5336 [D loss: (0.503)(R 0.509, F 0.496)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.395] [G acc: 0.219]\n",
      "5337 [D loss: (0.537)(R 0.439, F 0.635)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.384] [G acc: 0.109]\n",
      "5338 [D loss: (0.589)(R 0.496, F 0.682)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.389] [G acc: 0.078]\n",
      "5339 [D loss: (0.469)(R 0.450, F 0.489)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.399] [G acc: 0.078]\n",
      "5340 [D loss: (0.599)(R 0.643, F 0.555)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.203] [G acc: 0.062]\n",
      "5341 [D loss: (0.525)(R 0.579, F 0.470)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.279] [G acc: 0.109]\n",
      "5342 [D loss: (0.554)(R 0.587, F 0.520)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.173] [G acc: 0.156]\n",
      "5343 [D loss: (0.675)(R 0.653, F 0.697)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.305] [G acc: 0.078]\n",
      "5344 [D loss: (0.488)(R 0.491, F 0.486)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.261] [G acc: 0.047]\n",
      "5345 [D loss: (0.635)(R 0.664, F 0.607)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.195] [G acc: 0.172]\n",
      "5346 [D loss: (0.625)(R 0.685, F 0.565)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.239] [G acc: 0.141]\n",
      "5347 [D loss: (0.520)(R 0.505, F 0.536)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.258] [G acc: 0.078]\n",
      "5348 [D loss: (0.535)(R 0.552, F 0.519)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.342] [G acc: 0.094]\n",
      "5349 [D loss: (0.615)(R 0.564, F 0.666)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.381] [G acc: 0.078]\n",
      "5350 [D loss: (0.539)(R 0.610, F 0.469)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.267] [G acc: 0.078]\n",
      "5351 [D loss: (0.553)(R 0.536, F 0.570)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.213] [G acc: 0.156]\n",
      "5352 [D loss: (0.615)(R 0.576, F 0.654)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.260] [G acc: 0.062]\n",
      "5353 [D loss: (0.628)(R 0.623, F 0.633)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.322] [G acc: 0.109]\n",
      "5354 [D loss: (0.603)(R 0.680, F 0.526)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.252] [G acc: 0.094]\n",
      "5355 [D loss: (0.569)(R 0.568, F 0.570)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.104] [G acc: 0.203]\n",
      "5356 [D loss: (0.540)(R 0.545, F 0.536)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.219] [G acc: 0.172]\n",
      "5357 [D loss: (0.559)(R 0.571, F 0.546)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.223] [G acc: 0.078]\n",
      "5358 [D loss: (0.569)(R 0.496, F 0.642)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.320] [G acc: 0.078]\n",
      "5359 [D loss: (0.609)(R 0.671, F 0.547)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.255] [G acc: 0.094]\n",
      "5360 [D loss: (0.601)(R 0.649, F 0.554)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.260] [G acc: 0.062]\n",
      "5361 [D loss: (0.601)(R 0.522, F 0.679)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.324] [G acc: 0.094]\n",
      "5362 [D loss: (0.632)(R 0.726, F 0.539)] [D acc: (0.641)(0.453, 0.828)] [G loss: 1.251] [G acc: 0.031]\n",
      "5363 [D loss: (0.595)(R 0.581, F 0.609)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.191] [G acc: 0.047]\n",
      "5364 [D loss: (0.593)(R 0.602, F 0.585)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.083] [G acc: 0.172]\n",
      "5365 [D loss: (0.565)(R 0.660, F 0.470)] [D acc: (0.727)(0.547, 0.906)] [G loss: 1.241] [G acc: 0.109]\n",
      "5366 [D loss: (0.573)(R 0.534, F 0.612)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.199] [G acc: 0.125]\n",
      "5367 [D loss: (0.618)(R 0.631, F 0.605)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.174] [G acc: 0.141]\n",
      "5368 [D loss: (0.563)(R 0.525, F 0.601)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.267] [G acc: 0.062]\n",
      "5369 [D loss: (0.578)(R 0.668, F 0.488)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.138] [G acc: 0.156]\n",
      "5370 [D loss: (0.529)(R 0.551, F 0.508)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.268] [G acc: 0.141]\n",
      "5371 [D loss: (0.550)(R 0.571, F 0.528)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.265] [G acc: 0.125]\n",
      "5372 [D loss: (0.662)(R 0.689, F 0.636)] [D acc: (0.617)(0.516, 0.719)] [G loss: 1.187] [G acc: 0.094]\n",
      "5373 [D loss: (0.584)(R 0.567, F 0.600)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.291] [G acc: 0.078]\n",
      "5374 [D loss: (0.606)(R 0.679, F 0.532)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.205] [G acc: 0.109]\n",
      "5375 [D loss: (0.538)(R 0.503, F 0.573)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.259] [G acc: 0.078]\n",
      "5376 [D loss: (0.569)(R 0.532, F 0.607)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.210] [G acc: 0.156]\n",
      "5377 [D loss: (0.547)(R 0.562, F 0.533)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.301] [G acc: 0.094]\n",
      "5378 [D loss: (0.576)(R 0.689, F 0.463)] [D acc: (0.742)(0.578, 0.906)] [G loss: 1.232] [G acc: 0.141]\n",
      "5379 [D loss: (0.599)(R 0.741, F 0.457)] [D acc: (0.719)(0.516, 0.922)] [G loss: 1.216] [G acc: 0.094]\n",
      "5380 [D loss: (0.541)(R 0.521, F 0.561)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.411] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5381 [D loss: (0.522)(R 0.573, F 0.470)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.186] [G acc: 0.156]\n",
      "5382 [D loss: (0.620)(R 0.557, F 0.682)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.235] [G acc: 0.172]\n",
      "5383 [D loss: (0.628)(R 0.783, F 0.473)] [D acc: (0.688)(0.484, 0.891)] [G loss: 1.230] [G acc: 0.141]\n",
      "5384 [D loss: (0.596)(R 0.567, F 0.624)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.157] [G acc: 0.125]\n",
      "5385 [D loss: (0.558)(R 0.554, F 0.563)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.416] [G acc: 0.078]\n",
      "5386 [D loss: (0.589)(R 0.593, F 0.585)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.340] [G acc: 0.109]\n",
      "5387 [D loss: (0.635)(R 0.741, F 0.529)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.172] [G acc: 0.125]\n",
      "5388 [D loss: (0.590)(R 0.502, F 0.678)] [D acc: (0.633)(0.672, 0.594)] [G loss: 1.236] [G acc: 0.141]\n",
      "5389 [D loss: (0.600)(R 0.547, F 0.654)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.236] [G acc: 0.062]\n",
      "5390 [D loss: (0.526)(R 0.551, F 0.500)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.218] [G acc: 0.125]\n",
      "5391 [D loss: (0.524)(R 0.538, F 0.510)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.187] [G acc: 0.125]\n",
      "5392 [D loss: (0.590)(R 0.621, F 0.559)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.247] [G acc: 0.125]\n",
      "5393 [D loss: (0.561)(R 0.499, F 0.622)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.086] [G acc: 0.203]\n",
      "5394 [D loss: (0.589)(R 0.586, F 0.591)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.211] [G acc: 0.125]\n",
      "5395 [D loss: (0.597)(R 0.663, F 0.531)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.230] [G acc: 0.141]\n",
      "5396 [D loss: (0.549)(R 0.557, F 0.542)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.195] [G acc: 0.156]\n",
      "5397 [D loss: (0.611)(R 0.597, F 0.624)] [D acc: (0.625)(0.500, 0.750)] [G loss: 1.290] [G acc: 0.109]\n",
      "5398 [D loss: (0.628)(R 0.580, F 0.676)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.287] [G acc: 0.031]\n",
      "5399 [D loss: (0.584)(R 0.591, F 0.578)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.180] [G acc: 0.109]\n",
      "5400 [D loss: (0.545)(R 0.529, F 0.560)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.268] [G acc: 0.125]\n",
      "5401 [D loss: (0.515)(R 0.554, F 0.476)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.276] [G acc: 0.078]\n",
      "5402 [D loss: (0.626)(R 0.619, F 0.633)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.198] [G acc: 0.109]\n",
      "5403 [D loss: (0.585)(R 0.633, F 0.537)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.344] [G acc: 0.047]\n",
      "5404 [D loss: (0.623)(R 0.587, F 0.659)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.337] [G acc: 0.062]\n",
      "5405 [D loss: (0.581)(R 0.562, F 0.600)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.297] [G acc: 0.125]\n",
      "5406 [D loss: (0.556)(R 0.538, F 0.574)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.355] [G acc: 0.094]\n",
      "5407 [D loss: (0.576)(R 0.647, F 0.505)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.213] [G acc: 0.109]\n",
      "5408 [D loss: (0.582)(R 0.591, F 0.574)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.120] [G acc: 0.141]\n",
      "5409 [D loss: (0.632)(R 0.541, F 0.724)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.226] [G acc: 0.062]\n",
      "5410 [D loss: (0.571)(R 0.648, F 0.494)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.269] [G acc: 0.094]\n",
      "5411 [D loss: (0.556)(R 0.524, F 0.588)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.171] [G acc: 0.141]\n",
      "5412 [D loss: (0.550)(R 0.581, F 0.520)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.054] [G acc: 0.156]\n",
      "5413 [D loss: (0.543)(R 0.552, F 0.534)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.127] [G acc: 0.203]\n",
      "5414 [D loss: (0.505)(R 0.484, F 0.526)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.356] [G acc: 0.125]\n",
      "5415 [D loss: (0.590)(R 0.647, F 0.532)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.355] [G acc: 0.141]\n",
      "5416 [D loss: (0.614)(R 0.601, F 0.627)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.330] [G acc: 0.078]\n",
      "5417 [D loss: (0.582)(R 0.591, F 0.572)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.291] [G acc: 0.156]\n",
      "5418 [D loss: (0.721)(R 0.667, F 0.774)] [D acc: (0.586)(0.469, 0.703)] [G loss: 1.077] [G acc: 0.156]\n",
      "5419 [D loss: (0.567)(R 0.627, F 0.507)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.129] [G acc: 0.203]\n",
      "5420 [D loss: (0.507)(R 0.495, F 0.519)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.260] [G acc: 0.109]\n",
      "5421 [D loss: (0.588)(R 0.602, F 0.574)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.143] [G acc: 0.141]\n",
      "5422 [D loss: (0.581)(R 0.507, F 0.654)] [D acc: (0.664)(0.719, 0.609)] [G loss: 1.311] [G acc: 0.125]\n",
      "5423 [D loss: (0.622)(R 0.551, F 0.692)] [D acc: (0.625)(0.672, 0.578)] [G loss: 1.265] [G acc: 0.109]\n",
      "5424 [D loss: (0.525)(R 0.611, F 0.438)] [D acc: (0.727)(0.547, 0.906)] [G loss: 1.223] [G acc: 0.094]\n",
      "5425 [D loss: (0.500)(R 0.536, F 0.464)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.244] [G acc: 0.125]\n",
      "5426 [D loss: (0.538)(R 0.588, F 0.489)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.266] [G acc: 0.141]\n",
      "5427 [D loss: (0.544)(R 0.512, F 0.577)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.230] [G acc: 0.094]\n",
      "5428 [D loss: (0.551)(R 0.469, F 0.634)] [D acc: (0.703)(0.734, 0.672)] [G loss: 1.207] [G acc: 0.094]\n",
      "5429 [D loss: (0.639)(R 0.727, F 0.551)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.174] [G acc: 0.156]\n",
      "5430 [D loss: (0.556)(R 0.544, F 0.567)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.187] [G acc: 0.125]\n",
      "5431 [D loss: (0.609)(R 0.516, F 0.702)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.218] [G acc: 0.109]\n",
      "5432 [D loss: (0.568)(R 0.619, F 0.517)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.311] [G acc: 0.109]\n",
      "5433 [D loss: (0.557)(R 0.574, F 0.541)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.264] [G acc: 0.094]\n",
      "5434 [D loss: (0.579)(R 0.597, F 0.561)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.216] [G acc: 0.141]\n",
      "5435 [D loss: (0.527)(R 0.570, F 0.485)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.294] [G acc: 0.109]\n",
      "5436 [D loss: (0.544)(R 0.525, F 0.564)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.248] [G acc: 0.109]\n",
      "5437 [D loss: (0.491)(R 0.370, F 0.612)] [D acc: (0.812)(0.828, 0.797)] [G loss: 1.293] [G acc: 0.094]\n",
      "5438 [D loss: (0.600)(R 0.545, F 0.655)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.391] [G acc: 0.047]\n",
      "5439 [D loss: (0.709)(R 0.732, F 0.686)] [D acc: (0.641)(0.516, 0.766)] [G loss: 1.323] [G acc: 0.094]\n",
      "5440 [D loss: (0.560)(R 0.638, F 0.483)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.178] [G acc: 0.125]\n",
      "5441 [D loss: (0.526)(R 0.545, F 0.506)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.342] [G acc: 0.094]\n",
      "5442 [D loss: (0.548)(R 0.503, F 0.593)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.201] [G acc: 0.078]\n",
      "5443 [D loss: (0.664)(R 0.589, F 0.739)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.320] [G acc: 0.000]\n",
      "5444 [D loss: (0.695)(R 0.777, F 0.614)] [D acc: (0.633)(0.453, 0.812)] [G loss: 1.298] [G acc: 0.047]\n",
      "5445 [D loss: (0.572)(R 0.671, F 0.472)] [D acc: (0.688)(0.469, 0.906)] [G loss: 1.264] [G acc: 0.125]\n",
      "5446 [D loss: (0.553)(R 0.549, F 0.557)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.214] [G acc: 0.141]\n",
      "5447 [D loss: (0.501)(R 0.558, F 0.445)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.356] [G acc: 0.062]\n",
      "5448 [D loss: (0.582)(R 0.581, F 0.583)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.258] [G acc: 0.078]\n",
      "5449 [D loss: (0.559)(R 0.524, F 0.594)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.213] [G acc: 0.109]\n",
      "5450 [D loss: (0.593)(R 0.668, F 0.518)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.190] [G acc: 0.172]\n",
      "5451 [D loss: (0.528)(R 0.499, F 0.557)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.455] [G acc: 0.109]\n",
      "5452 [D loss: (0.527)(R 0.414, F 0.639)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.307] [G acc: 0.094]\n",
      "5453 [D loss: (0.537)(R 0.603, F 0.472)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.393] [G acc: 0.062]\n",
      "5454 [D loss: (0.502)(R 0.461, F 0.544)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.315] [G acc: 0.125]\n",
      "5455 [D loss: (0.602)(R 0.502, F 0.702)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.335] [G acc: 0.078]\n",
      "5456 [D loss: (0.586)(R 0.670, F 0.503)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.332] [G acc: 0.109]\n",
      "5457 [D loss: (0.567)(R 0.543, F 0.592)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.339] [G acc: 0.094]\n",
      "5458 [D loss: (0.573)(R 0.551, F 0.596)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.314] [G acc: 0.125]\n",
      "5459 [D loss: (0.515)(R 0.564, F 0.466)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.278] [G acc: 0.172]\n",
      "5460 [D loss: (0.671)(R 0.719, F 0.623)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.249] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5461 [D loss: (0.649)(R 0.761, F 0.537)] [D acc: (0.641)(0.438, 0.844)] [G loss: 1.064] [G acc: 0.172]\n",
      "5462 [D loss: (0.471)(R 0.382, F 0.560)] [D acc: (0.805)(0.812, 0.797)] [G loss: 1.237] [G acc: 0.125]\n",
      "5463 [D loss: (0.561)(R 0.557, F 0.566)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.207] [G acc: 0.094]\n",
      "5464 [D loss: (0.624)(R 0.541, F 0.707)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.227] [G acc: 0.109]\n",
      "5465 [D loss: (0.537)(R 0.615, F 0.459)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.230] [G acc: 0.188]\n",
      "5466 [D loss: (0.618)(R 0.679, F 0.557)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.429] [G acc: 0.094]\n",
      "5467 [D loss: (0.645)(R 0.672, F 0.619)] [D acc: (0.625)(0.500, 0.750)] [G loss: 1.255] [G acc: 0.078]\n",
      "5468 [D loss: (0.594)(R 0.593, F 0.594)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.186] [G acc: 0.125]\n",
      "5469 [D loss: (0.531)(R 0.603, F 0.459)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.353] [G acc: 0.109]\n",
      "5470 [D loss: (0.528)(R 0.501, F 0.555)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.331] [G acc: 0.125]\n",
      "5471 [D loss: (0.499)(R 0.477, F 0.520)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.395] [G acc: 0.094]\n",
      "5472 [D loss: (0.550)(R 0.517, F 0.583)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.328] [G acc: 0.047]\n",
      "5473 [D loss: (0.597)(R 0.645, F 0.548)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.245] [G acc: 0.141]\n",
      "5474 [D loss: (0.691)(R 0.612, F 0.770)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.283] [G acc: 0.141]\n",
      "5475 [D loss: (0.525)(R 0.596, F 0.453)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.218] [G acc: 0.141]\n",
      "5476 [D loss: (0.596)(R 0.506, F 0.686)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.209] [G acc: 0.125]\n",
      "5477 [D loss: (0.629)(R 0.698, F 0.561)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.327] [G acc: 0.172]\n",
      "5478 [D loss: (0.553)(R 0.532, F 0.575)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.184] [G acc: 0.141]\n",
      "5479 [D loss: (0.518)(R 0.567, F 0.469)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.264] [G acc: 0.141]\n",
      "5480 [D loss: (0.578)(R 0.481, F 0.676)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.396] [G acc: 0.062]\n",
      "5481 [D loss: (0.578)(R 0.610, F 0.546)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.278] [G acc: 0.109]\n",
      "5482 [D loss: (0.567)(R 0.533, F 0.602)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.206] [G acc: 0.125]\n",
      "5483 [D loss: (0.575)(R 0.585, F 0.565)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.349] [G acc: 0.031]\n",
      "5484 [D loss: (0.597)(R 0.713, F 0.482)] [D acc: (0.703)(0.531, 0.875)] [G loss: 1.269] [G acc: 0.047]\n",
      "5485 [D loss: (0.618)(R 0.629, F 0.608)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.204] [G acc: 0.125]\n",
      "5486 [D loss: (0.516)(R 0.498, F 0.534)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.161] [G acc: 0.109]\n",
      "5487 [D loss: (0.612)(R 0.642, F 0.582)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.144] [G acc: 0.109]\n",
      "5488 [D loss: (0.466)(R 0.474, F 0.457)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.235] [G acc: 0.125]\n",
      "5489 [D loss: (0.528)(R 0.449, F 0.607)] [D acc: (0.773)(0.781, 0.766)] [G loss: 1.524] [G acc: 0.031]\n",
      "5490 [D loss: (0.635)(R 0.769, F 0.502)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.176] [G acc: 0.250]\n",
      "5491 [D loss: (0.528)(R 0.495, F 0.561)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.269] [G acc: 0.156]\n",
      "5492 [D loss: (0.514)(R 0.480, F 0.547)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.271] [G acc: 0.094]\n",
      "5493 [D loss: (0.493)(R 0.468, F 0.519)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.256] [G acc: 0.078]\n",
      "5494 [D loss: (0.611)(R 0.574, F 0.649)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.309] [G acc: 0.078]\n",
      "5495 [D loss: (0.530)(R 0.600, F 0.460)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.362] [G acc: 0.109]\n",
      "5496 [D loss: (0.718)(R 0.652, F 0.784)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.313] [G acc: 0.094]\n",
      "5497 [D loss: (0.551)(R 0.619, F 0.483)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.191] [G acc: 0.172]\n",
      "5498 [D loss: (0.544)(R 0.561, F 0.528)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.334] [G acc: 0.078]\n",
      "5499 [D loss: (0.603)(R 0.596, F 0.609)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.202] [G acc: 0.172]\n",
      "5500 [D loss: (0.601)(R 0.575, F 0.626)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.156] [G acc: 0.172]\n",
      "5501 [D loss: (0.524)(R 0.563, F 0.486)] [D acc: (0.719)(0.562, 0.875)] [G loss: 1.288] [G acc: 0.094]\n",
      "5502 [D loss: (0.576)(R 0.550, F 0.602)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.206] [G acc: 0.109]\n",
      "5503 [D loss: (0.532)(R 0.521, F 0.542)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.180] [G acc: 0.172]\n",
      "5504 [D loss: (0.603)(R 0.589, F 0.617)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.181] [G acc: 0.094]\n",
      "5505 [D loss: (0.562)(R 0.475, F 0.650)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.259] [G acc: 0.094]\n",
      "5506 [D loss: (0.641)(R 0.756, F 0.525)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.251] [G acc: 0.094]\n",
      "5507 [D loss: (0.580)(R 0.646, F 0.515)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.280] [G acc: 0.062]\n",
      "5508 [D loss: (0.684)(R 0.675, F 0.694)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.224] [G acc: 0.094]\n",
      "5509 [D loss: (0.614)(R 0.695, F 0.532)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.282] [G acc: 0.062]\n",
      "5510 [D loss: (0.547)(R 0.617, F 0.478)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.354] [G acc: 0.016]\n",
      "5511 [D loss: (0.581)(R 0.617, F 0.545)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.328] [G acc: 0.062]\n",
      "5512 [D loss: (0.573)(R 0.561, F 0.586)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.311] [G acc: 0.078]\n",
      "5513 [D loss: (0.546)(R 0.569, F 0.523)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.200] [G acc: 0.141]\n",
      "5514 [D loss: (0.633)(R 0.574, F 0.691)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.192] [G acc: 0.094]\n",
      "5515 [D loss: (0.588)(R 0.610, F 0.566)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.233] [G acc: 0.094]\n",
      "5516 [D loss: (0.535)(R 0.551, F 0.519)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.178] [G acc: 0.094]\n",
      "5517 [D loss: (0.598)(R 0.611, F 0.586)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.163] [G acc: 0.141]\n",
      "5518 [D loss: (0.557)(R 0.493, F 0.621)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.128] [G acc: 0.125]\n",
      "5519 [D loss: (0.567)(R 0.596, F 0.538)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.203] [G acc: 0.094]\n",
      "5520 [D loss: (0.585)(R 0.599, F 0.571)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.263] [G acc: 0.141]\n",
      "5521 [D loss: (0.562)(R 0.604, F 0.519)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.252] [G acc: 0.078]\n",
      "5522 [D loss: (0.558)(R 0.548, F 0.568)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.191] [G acc: 0.172]\n",
      "5523 [D loss: (0.579)(R 0.593, F 0.564)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.309] [G acc: 0.062]\n",
      "5524 [D loss: (0.601)(R 0.566, F 0.637)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.211] [G acc: 0.109]\n",
      "5525 [D loss: (0.547)(R 0.560, F 0.533)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.240] [G acc: 0.125]\n",
      "5526 [D loss: (0.601)(R 0.622, F 0.580)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.311] [G acc: 0.016]\n",
      "5527 [D loss: (0.569)(R 0.591, F 0.546)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.325] [G acc: 0.141]\n",
      "5528 [D loss: (0.678)(R 0.676, F 0.680)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.166] [G acc: 0.047]\n",
      "5529 [D loss: (0.489)(R 0.536, F 0.443)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.108] [G acc: 0.125]\n",
      "5530 [D loss: (0.625)(R 0.575, F 0.675)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.317] [G acc: 0.047]\n",
      "5531 [D loss: (0.590)(R 0.621, F 0.559)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.240] [G acc: 0.109]\n",
      "5532 [D loss: (0.588)(R 0.537, F 0.640)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.219] [G acc: 0.062]\n",
      "5533 [D loss: (0.521)(R 0.569, F 0.472)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.526] [G acc: 0.109]\n",
      "5534 [D loss: (0.537)(R 0.628, F 0.446)] [D acc: (0.750)(0.625, 0.875)] [G loss: 1.277] [G acc: 0.109]\n",
      "5535 [D loss: (0.652)(R 0.595, F 0.710)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.305] [G acc: 0.094]\n",
      "5536 [D loss: (0.538)(R 0.608, F 0.468)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.208] [G acc: 0.156]\n",
      "5537 [D loss: (0.575)(R 0.561, F 0.589)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.193] [G acc: 0.109]\n",
      "5538 [D loss: (0.578)(R 0.565, F 0.591)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.317] [G acc: 0.156]\n",
      "5539 [D loss: (0.693)(R 0.612, F 0.774)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.250] [G acc: 0.078]\n",
      "5540 [D loss: (0.619)(R 0.699, F 0.538)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.344] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5541 [D loss: (0.505)(R 0.542, F 0.467)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.210] [G acc: 0.062]\n",
      "5542 [D loss: (0.731)(R 0.748, F 0.715)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.287] [G acc: 0.078]\n",
      "5543 [D loss: (0.592)(R 0.706, F 0.478)] [D acc: (0.680)(0.484, 0.875)] [G loss: 1.218] [G acc: 0.125]\n",
      "5544 [D loss: (0.576)(R 0.574, F 0.579)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.107] [G acc: 0.172]\n",
      "5545 [D loss: (0.569)(R 0.600, F 0.537)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.263] [G acc: 0.031]\n",
      "5546 [D loss: (0.552)(R 0.529, F 0.574)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.233] [G acc: 0.109]\n",
      "5547 [D loss: (0.665)(R 0.723, F 0.607)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.331] [G acc: 0.078]\n",
      "5548 [D loss: (0.587)(R 0.684, F 0.490)] [D acc: (0.672)(0.484, 0.859)] [G loss: 1.173] [G acc: 0.141]\n",
      "5549 [D loss: (0.662)(R 0.627, F 0.696)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.220] [G acc: 0.047]\n",
      "5550 [D loss: (0.627)(R 0.628, F 0.626)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.088] [G acc: 0.109]\n",
      "5551 [D loss: (0.574)(R 0.577, F 0.570)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.129] [G acc: 0.125]\n",
      "5552 [D loss: (0.584)(R 0.553, F 0.615)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.250] [G acc: 0.078]\n",
      "5553 [D loss: (0.627)(R 0.714, F 0.540)] [D acc: (0.664)(0.500, 0.828)] [G loss: 1.188] [G acc: 0.078]\n",
      "5554 [D loss: (0.604)(R 0.633, F 0.574)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.214] [G acc: 0.172]\n",
      "5555 [D loss: (0.486)(R 0.451, F 0.522)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.144] [G acc: 0.094]\n",
      "5556 [D loss: (0.493)(R 0.399, F 0.588)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.100] [G acc: 0.188]\n",
      "5557 [D loss: (0.612)(R 0.561, F 0.662)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.086] [G acc: 0.172]\n",
      "5558 [D loss: (0.552)(R 0.623, F 0.481)] [D acc: (0.750)(0.594, 0.906)] [G loss: 1.181] [G acc: 0.141]\n",
      "5559 [D loss: (0.545)(R 0.566, F 0.524)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.214] [G acc: 0.141]\n",
      "5560 [D loss: (0.497)(R 0.459, F 0.535)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.201] [G acc: 0.141]\n",
      "5561 [D loss: (0.578)(R 0.565, F 0.591)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.206] [G acc: 0.062]\n",
      "5562 [D loss: (0.575)(R 0.524, F 0.626)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.142] [G acc: 0.156]\n",
      "5563 [D loss: (0.551)(R 0.550, F 0.552)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.304] [G acc: 0.094]\n",
      "5564 [D loss: (0.601)(R 0.594, F 0.608)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.210] [G acc: 0.094]\n",
      "5565 [D loss: (0.581)(R 0.620, F 0.542)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.058] [G acc: 0.172]\n",
      "5566 [D loss: (0.521)(R 0.620, F 0.422)] [D acc: (0.742)(0.578, 0.906)] [G loss: 1.239] [G acc: 0.172]\n",
      "5567 [D loss: (0.467)(R 0.471, F 0.462)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.294] [G acc: 0.188]\n",
      "5568 [D loss: (0.587)(R 0.519, F 0.655)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.335] [G acc: 0.047]\n",
      "5569 [D loss: (0.645)(R 0.720, F 0.569)] [D acc: (0.641)(0.469, 0.812)] [G loss: 1.318] [G acc: 0.078]\n",
      "5570 [D loss: (0.578)(R 0.615, F 0.540)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.204] [G acc: 0.094]\n",
      "5571 [D loss: (0.578)(R 0.556, F 0.600)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.236] [G acc: 0.156]\n",
      "5572 [D loss: (0.550)(R 0.526, F 0.574)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.390] [G acc: 0.078]\n",
      "5573 [D loss: (0.546)(R 0.627, F 0.466)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.225] [G acc: 0.141]\n",
      "5574 [D loss: (0.643)(R 0.570, F 0.716)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.129] [G acc: 0.172]\n",
      "5575 [D loss: (0.629)(R 0.720, F 0.537)] [D acc: (0.648)(0.438, 0.859)] [G loss: 1.188] [G acc: 0.047]\n",
      "5576 [D loss: (0.629)(R 0.680, F 0.579)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.163] [G acc: 0.109]\n",
      "5577 [D loss: (0.543)(R 0.583, F 0.503)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.257] [G acc: 0.094]\n",
      "5578 [D loss: (0.532)(R 0.536, F 0.527)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.266] [G acc: 0.094]\n",
      "5579 [D loss: (0.602)(R 0.582, F 0.622)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.268] [G acc: 0.141]\n",
      "5580 [D loss: (0.574)(R 0.565, F 0.582)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.259] [G acc: 0.031]\n",
      "5581 [D loss: (0.538)(R 0.500, F 0.575)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.301] [G acc: 0.078]\n",
      "5582 [D loss: (0.545)(R 0.467, F 0.624)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.198] [G acc: 0.125]\n",
      "5583 [D loss: (0.662)(R 0.673, F 0.651)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.310] [G acc: 0.094]\n",
      "5584 [D loss: (0.595)(R 0.728, F 0.462)] [D acc: (0.703)(0.484, 0.922)] [G loss: 1.238] [G acc: 0.141]\n",
      "5585 [D loss: (0.542)(R 0.495, F 0.588)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.174] [G acc: 0.109]\n",
      "5586 [D loss: (0.568)(R 0.574, F 0.562)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.260] [G acc: 0.078]\n",
      "5587 [D loss: (0.611)(R 0.617, F 0.605)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.198] [G acc: 0.141]\n",
      "5588 [D loss: (0.663)(R 0.678, F 0.648)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.185] [G acc: 0.062]\n",
      "5589 [D loss: (0.573)(R 0.619, F 0.528)] [D acc: (0.680)(0.516, 0.844)] [G loss: 1.327] [G acc: 0.172]\n",
      "5590 [D loss: (0.572)(R 0.619, F 0.526)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.265] [G acc: 0.094]\n",
      "5591 [D loss: (0.519)(R 0.551, F 0.487)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.326] [G acc: 0.141]\n",
      "5592 [D loss: (0.559)(R 0.569, F 0.549)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.272] [G acc: 0.078]\n",
      "5593 [D loss: (0.545)(R 0.582, F 0.509)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.317] [G acc: 0.078]\n",
      "5594 [D loss: (0.574)(R 0.579, F 0.568)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.161] [G acc: 0.219]\n",
      "5595 [D loss: (0.637)(R 0.591, F 0.684)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.305] [G acc: 0.078]\n",
      "5596 [D loss: (0.546)(R 0.681, F 0.412)] [D acc: (0.742)(0.578, 0.906)] [G loss: 1.210] [G acc: 0.109]\n",
      "5597 [D loss: (0.607)(R 0.554, F 0.661)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.325] [G acc: 0.047]\n",
      "5598 [D loss: (0.595)(R 0.673, F 0.516)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.257] [G acc: 0.031]\n",
      "5599 [D loss: (0.562)(R 0.588, F 0.535)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.087] [G acc: 0.172]\n",
      "5600 [D loss: (0.568)(R 0.606, F 0.529)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.158] [G acc: 0.141]\n",
      "5601 [D loss: (0.575)(R 0.610, F 0.541)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.160] [G acc: 0.172]\n",
      "5602 [D loss: (0.560)(R 0.552, F 0.568)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.325] [G acc: 0.078]\n",
      "5603 [D loss: (0.620)(R 0.593, F 0.647)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.077] [G acc: 0.188]\n",
      "5604 [D loss: (0.582)(R 0.598, F 0.566)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.263] [G acc: 0.094]\n",
      "5605 [D loss: (0.596)(R 0.608, F 0.585)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.132] [G acc: 0.109]\n",
      "5606 [D loss: (0.618)(R 0.594, F 0.642)] [D acc: (0.609)(0.531, 0.688)] [G loss: 1.065] [G acc: 0.125]\n",
      "5607 [D loss: (0.562)(R 0.598, F 0.526)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.138] [G acc: 0.234]\n",
      "5608 [D loss: (0.616)(R 0.532, F 0.699)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.084] [G acc: 0.156]\n",
      "5609 [D loss: (0.516)(R 0.548, F 0.484)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.186] [G acc: 0.141]\n",
      "5610 [D loss: (0.562)(R 0.593, F 0.531)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.092] [G acc: 0.094]\n",
      "5611 [D loss: (0.564)(R 0.573, F 0.554)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.116] [G acc: 0.234]\n",
      "5612 [D loss: (0.483)(R 0.442, F 0.524)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.270] [G acc: 0.156]\n",
      "5613 [D loss: (0.587)(R 0.510, F 0.664)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.198] [G acc: 0.125]\n",
      "5614 [D loss: (0.594)(R 0.619, F 0.568)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.213] [G acc: 0.109]\n",
      "5615 [D loss: (0.626)(R 0.704, F 0.548)] [D acc: (0.656)(0.484, 0.828)] [G loss: 1.302] [G acc: 0.047]\n",
      "5616 [D loss: (0.617)(R 0.694, F 0.541)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.128] [G acc: 0.109]\n",
      "5617 [D loss: (0.595)(R 0.599, F 0.591)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.119] [G acc: 0.125]\n",
      "5618 [D loss: (0.641)(R 0.714, F 0.568)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.227] [G acc: 0.078]\n",
      "5619 [D loss: (0.523)(R 0.617, F 0.429)] [D acc: (0.711)(0.547, 0.875)] [G loss: 1.196] [G acc: 0.141]\n",
      "5620 [D loss: (0.528)(R 0.539, F 0.518)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.296] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5621 [D loss: (0.562)(R 0.581, F 0.544)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.322] [G acc: 0.062]\n",
      "5622 [D loss: (0.630)(R 0.614, F 0.645)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.131] [G acc: 0.156]\n",
      "5623 [D loss: (0.581)(R 0.649, F 0.513)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.180] [G acc: 0.094]\n",
      "5624 [D loss: (0.543)(R 0.510, F 0.577)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.218] [G acc: 0.125]\n",
      "5625 [D loss: (0.641)(R 0.597, F 0.685)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.169] [G acc: 0.125]\n",
      "5626 [D loss: (0.605)(R 0.573, F 0.636)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.285] [G acc: 0.031]\n",
      "5627 [D loss: (0.556)(R 0.639, F 0.472)] [D acc: (0.734)(0.562, 0.906)] [G loss: 1.202] [G acc: 0.109]\n",
      "5628 [D loss: (0.539)(R 0.530, F 0.548)] [D acc: (0.750)(0.609, 0.891)] [G loss: 1.173] [G acc: 0.172]\n",
      "5629 [D loss: (0.578)(R 0.511, F 0.645)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.163] [G acc: 0.141]\n",
      "5630 [D loss: (0.570)(R 0.555, F 0.586)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.330] [G acc: 0.109]\n",
      "5631 [D loss: (0.566)(R 0.637, F 0.494)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.228] [G acc: 0.219]\n",
      "5632 [D loss: (0.578)(R 0.526, F 0.629)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.272] [G acc: 0.094]\n",
      "5633 [D loss: (0.557)(R 0.556, F 0.558)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.279] [G acc: 0.125]\n",
      "5634 [D loss: (0.578)(R 0.496, F 0.659)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.275] [G acc: 0.109]\n",
      "5635 [D loss: (0.533)(R 0.464, F 0.601)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.310] [G acc: 0.094]\n",
      "5636 [D loss: (0.534)(R 0.587, F 0.481)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.535] [G acc: 0.078]\n",
      "5637 [D loss: (0.506)(R 0.545, F 0.468)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.324] [G acc: 0.078]\n",
      "5638 [D loss: (0.587)(R 0.564, F 0.611)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.208] [G acc: 0.172]\n",
      "5639 [D loss: (0.495)(R 0.506, F 0.484)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.221] [G acc: 0.203]\n",
      "5640 [D loss: (0.576)(R 0.528, F 0.623)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.382] [G acc: 0.172]\n",
      "5641 [D loss: (0.517)(R 0.462, F 0.571)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.233] [G acc: 0.125]\n",
      "5642 [D loss: (0.572)(R 0.560, F 0.585)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.357] [G acc: 0.062]\n",
      "5643 [D loss: (0.521)(R 0.493, F 0.549)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.238] [G acc: 0.141]\n",
      "5644 [D loss: (0.591)(R 0.594, F 0.588)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.320] [G acc: 0.156]\n",
      "5645 [D loss: (0.621)(R 0.680, F 0.562)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.392] [G acc: 0.109]\n",
      "5646 [D loss: (0.644)(R 0.557, F 0.732)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.317] [G acc: 0.062]\n",
      "5647 [D loss: (0.578)(R 0.657, F 0.499)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.418] [G acc: 0.031]\n",
      "5648 [D loss: (0.503)(R 0.551, F 0.455)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.351] [G acc: 0.156]\n",
      "5649 [D loss: (0.453)(R 0.402, F 0.505)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.232] [G acc: 0.125]\n",
      "5650 [D loss: (0.616)(R 0.516, F 0.716)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.225] [G acc: 0.125]\n",
      "5651 [D loss: (0.545)(R 0.552, F 0.537)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.315] [G acc: 0.062]\n",
      "5652 [D loss: (0.570)(R 0.652, F 0.488)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.287] [G acc: 0.062]\n",
      "5653 [D loss: (0.551)(R 0.509, F 0.592)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.324] [G acc: 0.094]\n",
      "5654 [D loss: (0.704)(R 0.592, F 0.817)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.231] [G acc: 0.094]\n",
      "5655 [D loss: (0.529)(R 0.564, F 0.495)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.239] [G acc: 0.094]\n",
      "5656 [D loss: (0.493)(R 0.505, F 0.481)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.282] [G acc: 0.156]\n",
      "5657 [D loss: (0.559)(R 0.560, F 0.557)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.298] [G acc: 0.109]\n",
      "5658 [D loss: (0.632)(R 0.640, F 0.623)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.360] [G acc: 0.094]\n",
      "5659 [D loss: (0.503)(R 0.540, F 0.467)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.383] [G acc: 0.078]\n",
      "5660 [D loss: (0.542)(R 0.490, F 0.593)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.337] [G acc: 0.125]\n",
      "5661 [D loss: (0.566)(R 0.665, F 0.467)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.315] [G acc: 0.125]\n",
      "5662 [D loss: (0.573)(R 0.557, F 0.589)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.225] [G acc: 0.078]\n",
      "5663 [D loss: (0.509)(R 0.574, F 0.444)] [D acc: (0.766)(0.625, 0.906)] [G loss: 1.297] [G acc: 0.062]\n",
      "5664 [D loss: (0.535)(R 0.598, F 0.473)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.274] [G acc: 0.062]\n",
      "5665 [D loss: (0.539)(R 0.546, F 0.531)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.221] [G acc: 0.156]\n",
      "5666 [D loss: (0.533)(R 0.479, F 0.587)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.337] [G acc: 0.094]\n",
      "5667 [D loss: (0.533)(R 0.521, F 0.545)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.239] [G acc: 0.109]\n",
      "5668 [D loss: (0.636)(R 0.629, F 0.644)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.478] [G acc: 0.078]\n",
      "5669 [D loss: (0.632)(R 0.667, F 0.597)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.195] [G acc: 0.094]\n",
      "5670 [D loss: (0.603)(R 0.615, F 0.592)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.290] [G acc: 0.094]\n",
      "5671 [D loss: (0.508)(R 0.587, F 0.429)] [D acc: (0.781)(0.656, 0.906)] [G loss: 1.189] [G acc: 0.156]\n",
      "5672 [D loss: (0.540)(R 0.559, F 0.520)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.161] [G acc: 0.125]\n",
      "5673 [D loss: (0.641)(R 0.619, F 0.663)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.182] [G acc: 0.141]\n",
      "5674 [D loss: (0.623)(R 0.669, F 0.577)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.289] [G acc: 0.078]\n",
      "5675 [D loss: (0.628)(R 0.659, F 0.596)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.263] [G acc: 0.031]\n",
      "5676 [D loss: (0.575)(R 0.577, F 0.573)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.260] [G acc: 0.078]\n",
      "5677 [D loss: (0.568)(R 0.626, F 0.510)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.183] [G acc: 0.094]\n",
      "5678 [D loss: (0.509)(R 0.544, F 0.474)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.182] [G acc: 0.109]\n",
      "5679 [D loss: (0.611)(R 0.641, F 0.582)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.167] [G acc: 0.078]\n",
      "5680 [D loss: (0.525)(R 0.466, F 0.584)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.223] [G acc: 0.125]\n",
      "5681 [D loss: (0.570)(R 0.542, F 0.597)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.365] [G acc: 0.062]\n",
      "5682 [D loss: (0.535)(R 0.489, F 0.582)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.247] [G acc: 0.094]\n",
      "5683 [D loss: (0.644)(R 0.789, F 0.498)] [D acc: (0.664)(0.422, 0.906)] [G loss: 1.203] [G acc: 0.094]\n",
      "5684 [D loss: (0.530)(R 0.561, F 0.500)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.248] [G acc: 0.125]\n",
      "5685 [D loss: (0.567)(R 0.520, F 0.614)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.274] [G acc: 0.062]\n",
      "5686 [D loss: (0.550)(R 0.588, F 0.513)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.203] [G acc: 0.188]\n",
      "5687 [D loss: (0.491)(R 0.502, F 0.481)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.329] [G acc: 0.125]\n",
      "5688 [D loss: (0.679)(R 0.623, F 0.735)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.354] [G acc: 0.078]\n",
      "5689 [D loss: (0.582)(R 0.638, F 0.525)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.259] [G acc: 0.109]\n",
      "5690 [D loss: (0.601)(R 0.606, F 0.597)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.187] [G acc: 0.156]\n",
      "5691 [D loss: (0.555)(R 0.573, F 0.536)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.391] [G acc: 0.078]\n",
      "5692 [D loss: (0.707)(R 0.632, F 0.783)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.255] [G acc: 0.047]\n",
      "5693 [D loss: (0.606)(R 0.742, F 0.471)] [D acc: (0.641)(0.438, 0.844)] [G loss: 1.258] [G acc: 0.031]\n",
      "5694 [D loss: (0.455)(R 0.492, F 0.417)] [D acc: (0.812)(0.750, 0.875)] [G loss: 1.228] [G acc: 0.156]\n",
      "5695 [D loss: (0.504)(R 0.512, F 0.497)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.276] [G acc: 0.141]\n",
      "5696 [D loss: (0.576)(R 0.464, F 0.687)] [D acc: (0.703)(0.750, 0.656)] [G loss: 1.375] [G acc: 0.172]\n",
      "5697 [D loss: (0.575)(R 0.598, F 0.552)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.364] [G acc: 0.047]\n",
      "5698 [D loss: (0.508)(R 0.549, F 0.466)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.338] [G acc: 0.047]\n",
      "5699 [D loss: (0.594)(R 0.659, F 0.529)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.309] [G acc: 0.172]\n",
      "5700 [D loss: (0.559)(R 0.442, F 0.677)] [D acc: (0.773)(0.812, 0.734)] [G loss: 1.427] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5701 [D loss: (0.605)(R 0.572, F 0.638)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.303] [G acc: 0.109]\n",
      "5702 [D loss: (0.531)(R 0.605, F 0.456)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.355] [G acc: 0.141]\n",
      "5703 [D loss: (0.490)(R 0.474, F 0.505)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.329] [G acc: 0.062]\n",
      "5704 [D loss: (0.636)(R 0.656, F 0.616)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.310] [G acc: 0.094]\n",
      "5705 [D loss: (0.572)(R 0.559, F 0.585)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.282] [G acc: 0.156]\n",
      "5706 [D loss: (0.592)(R 0.501, F 0.682)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.361] [G acc: 0.062]\n",
      "5707 [D loss: (0.598)(R 0.730, F 0.466)] [D acc: (0.695)(0.500, 0.891)] [G loss: 1.276] [G acc: 0.094]\n",
      "5708 [D loss: (0.529)(R 0.584, F 0.473)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.355] [G acc: 0.141]\n",
      "5709 [D loss: (0.618)(R 0.616, F 0.621)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.233] [G acc: 0.141]\n",
      "5710 [D loss: (0.605)(R 0.611, F 0.600)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.268] [G acc: 0.156]\n",
      "5711 [D loss: (0.549)(R 0.629, F 0.468)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.285] [G acc: 0.141]\n",
      "5712 [D loss: (0.619)(R 0.558, F 0.680)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.253] [G acc: 0.062]\n",
      "5713 [D loss: (0.632)(R 0.700, F 0.564)] [D acc: (0.586)(0.484, 0.688)] [G loss: 1.228] [G acc: 0.125]\n",
      "5714 [D loss: (0.636)(R 0.610, F 0.662)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.109] [G acc: 0.250]\n",
      "5715 [D loss: (0.560)(R 0.607, F 0.513)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.390] [G acc: 0.109]\n",
      "5716 [D loss: (0.537)(R 0.600, F 0.474)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.339] [G acc: 0.094]\n",
      "5717 [D loss: (0.579)(R 0.566, F 0.592)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.295] [G acc: 0.109]\n",
      "5718 [D loss: (0.498)(R 0.479, F 0.517)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.244] [G acc: 0.188]\n",
      "5719 [D loss: (0.589)(R 0.461, F 0.717)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.189] [G acc: 0.078]\n",
      "5720 [D loss: (0.581)(R 0.592, F 0.569)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.254] [G acc: 0.188]\n",
      "5721 [D loss: (0.537)(R 0.551, F 0.523)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.279] [G acc: 0.141]\n",
      "5722 [D loss: (0.595)(R 0.649, F 0.541)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.198] [G acc: 0.156]\n",
      "5723 [D loss: (0.622)(R 0.568, F 0.677)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.198] [G acc: 0.109]\n",
      "5724 [D loss: (0.548)(R 0.516, F 0.579)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.172] [G acc: 0.156]\n",
      "5725 [D loss: (0.487)(R 0.408, F 0.567)] [D acc: (0.812)(0.797, 0.828)] [G loss: 1.177] [G acc: 0.172]\n",
      "5726 [D loss: (0.542)(R 0.473, F 0.611)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.222] [G acc: 0.094]\n",
      "5727 [D loss: (0.560)(R 0.635, F 0.486)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.256] [G acc: 0.156]\n",
      "5728 [D loss: (0.605)(R 0.539, F 0.670)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.316] [G acc: 0.125]\n",
      "5729 [D loss: (0.616)(R 0.634, F 0.597)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.222] [G acc: 0.094]\n",
      "5730 [D loss: (0.538)(R 0.597, F 0.480)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.175] [G acc: 0.188]\n",
      "5731 [D loss: (0.590)(R 0.622, F 0.559)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.240] [G acc: 0.094]\n",
      "5732 [D loss: (0.481)(R 0.491, F 0.471)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.211] [G acc: 0.047]\n",
      "5733 [D loss: (0.577)(R 0.582, F 0.572)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.210] [G acc: 0.078]\n",
      "5734 [D loss: (0.600)(R 0.599, F 0.602)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.237] [G acc: 0.047]\n",
      "5735 [D loss: (0.571)(R 0.616, F 0.525)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.293] [G acc: 0.141]\n",
      "5736 [D loss: (0.508)(R 0.580, F 0.437)] [D acc: (0.727)(0.547, 0.906)] [G loss: 1.197] [G acc: 0.172]\n",
      "5737 [D loss: (0.640)(R 0.606, F 0.675)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.382] [G acc: 0.031]\n",
      "5738 [D loss: (0.661)(R 0.599, F 0.723)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.201] [G acc: 0.125]\n",
      "5739 [D loss: (0.633)(R 0.659, F 0.608)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.132] [G acc: 0.172]\n",
      "5740 [D loss: (0.520)(R 0.528, F 0.512)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.139] [G acc: 0.125]\n",
      "5741 [D loss: (0.562)(R 0.601, F 0.523)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.131] [G acc: 0.141]\n",
      "5742 [D loss: (0.540)(R 0.595, F 0.485)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.023] [G acc: 0.172]\n",
      "5743 [D loss: (0.599)(R 0.460, F 0.738)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.173] [G acc: 0.094]\n",
      "5744 [D loss: (0.618)(R 0.671, F 0.564)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.206] [G acc: 0.094]\n",
      "5745 [D loss: (0.576)(R 0.576, F 0.576)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.214] [G acc: 0.109]\n",
      "5746 [D loss: (0.583)(R 0.484, F 0.683)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.243] [G acc: 0.078]\n",
      "5747 [D loss: (0.567)(R 0.640, F 0.494)] [D acc: (0.703)(0.531, 0.875)] [G loss: 1.160] [G acc: 0.109]\n",
      "5748 [D loss: (0.581)(R 0.613, F 0.548)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.130] [G acc: 0.141]\n",
      "5749 [D loss: (0.554)(R 0.543, F 0.565)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.127] [G acc: 0.172]\n",
      "5750 [D loss: (0.641)(R 0.583, F 0.700)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.284] [G acc: 0.141]\n",
      "5751 [D loss: (0.536)(R 0.590, F 0.482)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.256] [G acc: 0.125]\n",
      "5752 [D loss: (0.516)(R 0.516, F 0.516)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.352] [G acc: 0.094]\n",
      "5753 [D loss: (0.556)(R 0.473, F 0.640)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.396] [G acc: 0.094]\n",
      "5754 [D loss: (0.665)(R 0.727, F 0.604)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.162] [G acc: 0.125]\n",
      "5755 [D loss: (0.605)(R 0.637, F 0.573)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.247] [G acc: 0.141]\n",
      "5756 [D loss: (0.601)(R 0.656, F 0.545)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.186] [G acc: 0.141]\n",
      "5757 [D loss: (0.520)(R 0.500, F 0.540)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.264] [G acc: 0.141]\n",
      "5758 [D loss: (0.605)(R 0.592, F 0.617)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.217] [G acc: 0.156]\n",
      "5759 [D loss: (0.537)(R 0.538, F 0.537)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.258] [G acc: 0.078]\n",
      "5760 [D loss: (0.655)(R 0.695, F 0.616)] [D acc: (0.641)(0.484, 0.797)] [G loss: 1.243] [G acc: 0.109]\n",
      "5761 [D loss: (0.581)(R 0.631, F 0.532)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.135] [G acc: 0.078]\n",
      "5762 [D loss: (0.574)(R 0.636, F 0.512)] [D acc: (0.703)(0.531, 0.875)] [G loss: 1.127] [G acc: 0.172]\n",
      "5763 [D loss: (0.539)(R 0.588, F 0.490)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.246] [G acc: 0.125]\n",
      "5764 [D loss: (0.531)(R 0.537, F 0.524)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.212] [G acc: 0.094]\n",
      "5765 [D loss: (0.577)(R 0.506, F 0.648)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.214] [G acc: 0.062]\n",
      "5766 [D loss: (0.537)(R 0.561, F 0.514)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.316] [G acc: 0.156]\n",
      "5767 [D loss: (0.538)(R 0.597, F 0.480)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.163] [G acc: 0.141]\n",
      "5768 [D loss: (0.563)(R 0.483, F 0.643)] [D acc: (0.727)(0.734, 0.719)] [G loss: 1.171] [G acc: 0.156]\n",
      "5769 [D loss: (0.607)(R 0.640, F 0.574)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.326] [G acc: 0.047]\n",
      "5770 [D loss: (0.528)(R 0.532, F 0.525)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.300] [G acc: 0.078]\n",
      "5771 [D loss: (0.630)(R 0.483, F 0.777)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.189] [G acc: 0.094]\n",
      "5772 [D loss: (0.511)(R 0.572, F 0.450)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.239] [G acc: 0.172]\n",
      "5773 [D loss: (0.597)(R 0.495, F 0.699)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.329] [G acc: 0.094]\n",
      "5774 [D loss: (0.532)(R 0.622, F 0.443)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.317] [G acc: 0.031]\n",
      "5775 [D loss: (0.559)(R 0.631, F 0.487)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.259] [G acc: 0.109]\n",
      "5776 [D loss: (0.557)(R 0.591, F 0.524)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.306] [G acc: 0.141]\n",
      "5777 [D loss: (0.583)(R 0.568, F 0.597)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.378] [G acc: 0.109]\n",
      "5778 [D loss: (0.572)(R 0.539, F 0.606)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.272] [G acc: 0.078]\n",
      "5779 [D loss: (0.454)(R 0.399, F 0.509)] [D acc: (0.812)(0.797, 0.828)] [G loss: 1.356] [G acc: 0.078]\n",
      "5780 [D loss: (0.628)(R 0.697, F 0.558)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.483] [G acc: 0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5781 [D loss: (0.569)(R 0.553, F 0.585)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.324] [G acc: 0.141]\n",
      "5782 [D loss: (0.549)(R 0.554, F 0.545)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.286] [G acc: 0.094]\n",
      "5783 [D loss: (0.637)(R 0.651, F 0.624)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.151] [G acc: 0.109]\n",
      "5784 [D loss: (0.544)(R 0.615, F 0.473)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.305] [G acc: 0.109]\n",
      "5785 [D loss: (0.600)(R 0.545, F 0.655)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.362] [G acc: 0.062]\n",
      "5786 [D loss: (0.556)(R 0.532, F 0.579)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.431] [G acc: 0.047]\n",
      "5787 [D loss: (0.539)(R 0.525, F 0.552)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.114] [G acc: 0.109]\n",
      "5788 [D loss: (0.513)(R 0.537, F 0.488)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.255] [G acc: 0.078]\n",
      "5789 [D loss: (0.554)(R 0.495, F 0.612)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.150] [G acc: 0.125]\n",
      "5790 [D loss: (0.555)(R 0.588, F 0.521)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.151] [G acc: 0.234]\n",
      "5791 [D loss: (0.591)(R 0.510, F 0.671)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.335] [G acc: 0.078]\n",
      "5792 [D loss: (0.558)(R 0.579, F 0.536)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.190] [G acc: 0.141]\n",
      "5793 [D loss: (0.574)(R 0.555, F 0.594)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.124] [G acc: 0.234]\n",
      "5794 [D loss: (0.524)(R 0.496, F 0.552)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.281] [G acc: 0.047]\n",
      "5795 [D loss: (0.517)(R 0.535, F 0.500)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.266] [G acc: 0.141]\n",
      "5796 [D loss: (0.521)(R 0.550, F 0.492)] [D acc: (0.773)(0.656, 0.891)] [G loss: 1.360] [G acc: 0.172]\n",
      "5797 [D loss: (0.556)(R 0.588, F 0.523)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.338] [G acc: 0.109]\n",
      "5798 [D loss: (0.606)(R 0.605, F 0.607)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.270] [G acc: 0.125]\n",
      "5799 [D loss: (0.524)(R 0.570, F 0.477)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.355] [G acc: 0.094]\n",
      "5800 [D loss: (0.555)(R 0.567, F 0.544)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.294] [G acc: 0.188]\n",
      "5801 [D loss: (0.543)(R 0.580, F 0.506)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.424] [G acc: 0.078]\n",
      "5802 [D loss: (0.584)(R 0.571, F 0.597)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.230] [G acc: 0.125]\n",
      "5803 [D loss: (0.478)(R 0.472, F 0.483)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.113] [G acc: 0.250]\n",
      "5804 [D loss: (0.524)(R 0.520, F 0.529)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.458] [G acc: 0.062]\n",
      "5805 [D loss: (0.592)(R 0.698, F 0.485)] [D acc: (0.641)(0.484, 0.797)] [G loss: 1.247] [G acc: 0.188]\n",
      "5806 [D loss: (0.592)(R 0.504, F 0.681)] [D acc: (0.742)(0.781, 0.703)] [G loss: 1.257] [G acc: 0.156]\n",
      "5807 [D loss: (0.573)(R 0.655, F 0.492)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.242] [G acc: 0.203]\n",
      "5808 [D loss: (0.620)(R 0.603, F 0.638)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.302] [G acc: 0.078]\n",
      "5809 [D loss: (0.529)(R 0.505, F 0.554)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.250] [G acc: 0.125]\n",
      "5810 [D loss: (0.543)(R 0.523, F 0.562)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.249] [G acc: 0.078]\n",
      "5811 [D loss: (0.592)(R 0.544, F 0.640)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.377] [G acc: 0.094]\n",
      "5812 [D loss: (0.674)(R 0.772, F 0.576)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.291] [G acc: 0.078]\n",
      "5813 [D loss: (0.623)(R 0.726, F 0.521)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.295] [G acc: 0.062]\n",
      "5814 [D loss: (0.614)(R 0.644, F 0.583)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.136] [G acc: 0.094]\n",
      "5815 [D loss: (0.623)(R 0.565, F 0.680)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.206] [G acc: 0.062]\n",
      "5816 [D loss: (0.493)(R 0.539, F 0.446)] [D acc: (0.773)(0.656, 0.891)] [G loss: 1.248] [G acc: 0.031]\n",
      "5817 [D loss: (0.633)(R 0.566, F 0.701)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.240] [G acc: 0.062]\n",
      "5818 [D loss: (0.549)(R 0.655, F 0.443)] [D acc: (0.758)(0.594, 0.922)] [G loss: 1.211] [G acc: 0.094]\n",
      "5819 [D loss: (0.525)(R 0.492, F 0.558)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.298] [G acc: 0.078]\n",
      "5820 [D loss: (0.547)(R 0.555, F 0.538)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.143] [G acc: 0.172]\n",
      "5821 [D loss: (0.587)(R 0.602, F 0.572)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.306] [G acc: 0.109]\n",
      "5822 [D loss: (0.590)(R 0.609, F 0.571)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.281] [G acc: 0.078]\n",
      "5823 [D loss: (0.589)(R 0.722, F 0.455)] [D acc: (0.656)(0.469, 0.844)] [G loss: 1.264] [G acc: 0.109]\n",
      "5824 [D loss: (0.599)(R 0.578, F 0.620)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.299] [G acc: 0.125]\n",
      "5825 [D loss: (0.504)(R 0.543, F 0.465)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.138] [G acc: 0.172]\n",
      "5826 [D loss: (0.742)(R 0.557, F 0.926)] [D acc: (0.609)(0.594, 0.625)] [G loss: 1.258] [G acc: 0.125]\n",
      "5827 [D loss: (0.646)(R 0.769, F 0.523)] [D acc: (0.648)(0.469, 0.828)] [G loss: 1.111] [G acc: 0.125]\n",
      "5828 [D loss: (0.562)(R 0.620, F 0.503)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.150] [G acc: 0.125]\n",
      "5829 [D loss: (0.519)(R 0.529, F 0.509)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.189] [G acc: 0.156]\n",
      "5830 [D loss: (0.536)(R 0.538, F 0.534)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.254] [G acc: 0.078]\n",
      "5831 [D loss: (0.556)(R 0.537, F 0.574)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.239] [G acc: 0.047]\n",
      "5832 [D loss: (0.502)(R 0.535, F 0.468)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.211] [G acc: 0.141]\n",
      "5833 [D loss: (0.691)(R 0.521, F 0.861)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.270] [G acc: 0.109]\n",
      "5834 [D loss: (0.568)(R 0.632, F 0.504)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.315] [G acc: 0.094]\n",
      "5835 [D loss: (0.582)(R 0.602, F 0.562)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.266] [G acc: 0.078]\n",
      "5836 [D loss: (0.616)(R 0.675, F 0.557)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.258] [G acc: 0.125]\n",
      "5837 [D loss: (0.599)(R 0.578, F 0.621)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.161] [G acc: 0.062]\n",
      "5838 [D loss: (0.643)(R 0.636, F 0.650)] [D acc: (0.625)(0.500, 0.750)] [G loss: 1.202] [G acc: 0.172]\n",
      "5839 [D loss: (0.597)(R 0.672, F 0.523)] [D acc: (0.719)(0.562, 0.875)] [G loss: 1.289] [G acc: 0.109]\n",
      "5840 [D loss: (0.568)(R 0.611, F 0.525)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.299] [G acc: 0.141]\n",
      "5841 [D loss: (0.564)(R 0.648, F 0.480)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.251] [G acc: 0.141]\n",
      "5842 [D loss: (0.541)(R 0.511, F 0.570)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.303] [G acc: 0.094]\n",
      "5843 [D loss: (0.560)(R 0.491, F 0.630)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.298] [G acc: 0.078]\n",
      "5844 [D loss: (0.553)(R 0.628, F 0.479)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.209] [G acc: 0.172]\n",
      "5845 [D loss: (0.570)(R 0.496, F 0.644)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.323] [G acc: 0.031]\n",
      "5846 [D loss: (0.562)(R 0.618, F 0.506)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.250] [G acc: 0.078]\n",
      "5847 [D loss: (0.582)(R 0.536, F 0.628)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.138] [G acc: 0.188]\n",
      "5848 [D loss: (0.526)(R 0.575, F 0.477)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.265] [G acc: 0.078]\n",
      "5849 [D loss: (0.637)(R 0.548, F 0.726)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.269] [G acc: 0.094]\n",
      "5850 [D loss: (0.528)(R 0.586, F 0.471)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.192] [G acc: 0.125]\n",
      "5851 [D loss: (0.597)(R 0.536, F 0.659)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.357] [G acc: 0.094]\n",
      "5852 [D loss: (0.595)(R 0.664, F 0.525)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.205] [G acc: 0.156]\n",
      "5853 [D loss: (0.624)(R 0.576, F 0.673)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.271] [G acc: 0.109]\n",
      "5854 [D loss: (0.615)(R 0.699, F 0.532)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.195] [G acc: 0.078]\n",
      "5855 [D loss: (0.615)(R 0.640, F 0.590)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.248] [G acc: 0.109]\n",
      "5856 [D loss: (0.501)(R 0.490, F 0.512)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.254] [G acc: 0.047]\n",
      "5857 [D loss: (0.579)(R 0.637, F 0.522)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.338] [G acc: 0.094]\n",
      "5858 [D loss: (0.696)(R 0.651, F 0.741)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.244] [G acc: 0.094]\n",
      "5859 [D loss: (0.543)(R 0.619, F 0.467)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.292] [G acc: 0.062]\n",
      "5860 [D loss: (0.552)(R 0.508, F 0.596)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.344] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5861 [D loss: (0.544)(R 0.582, F 0.506)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.198] [G acc: 0.156]\n",
      "5862 [D loss: (0.497)(R 0.414, F 0.579)] [D acc: (0.797)(0.797, 0.797)] [G loss: 1.344] [G acc: 0.109]\n",
      "5863 [D loss: (0.582)(R 0.584, F 0.581)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.208] [G acc: 0.125]\n",
      "5864 [D loss: (0.578)(R 0.674, F 0.482)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.275] [G acc: 0.156]\n",
      "5865 [D loss: (0.621)(R 0.595, F 0.646)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.391] [G acc: 0.078]\n",
      "5866 [D loss: (0.583)(R 0.611, F 0.556)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.218] [G acc: 0.109]\n",
      "5867 [D loss: (0.598)(R 0.572, F 0.624)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.305] [G acc: 0.047]\n",
      "5868 [D loss: (0.646)(R 0.701, F 0.591)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.236] [G acc: 0.125]\n",
      "5869 [D loss: (0.564)(R 0.606, F 0.523)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.120] [G acc: 0.203]\n",
      "5870 [D loss: (0.587)(R 0.553, F 0.622)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.253] [G acc: 0.094]\n",
      "5871 [D loss: (0.529)(R 0.558, F 0.501)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.204] [G acc: 0.172]\n",
      "5872 [D loss: (0.556)(R 0.452, F 0.661)] [D acc: (0.711)(0.750, 0.672)] [G loss: 1.297] [G acc: 0.094]\n",
      "5873 [D loss: (0.590)(R 0.674, F 0.506)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.341] [G acc: 0.094]\n",
      "5874 [D loss: (0.639)(R 0.620, F 0.659)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.146] [G acc: 0.141]\n",
      "5875 [D loss: (0.559)(R 0.573, F 0.544)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.291] [G acc: 0.078]\n",
      "5876 [D loss: (0.623)(R 0.658, F 0.589)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.200] [G acc: 0.141]\n",
      "5877 [D loss: (0.572)(R 0.626, F 0.519)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.315] [G acc: 0.062]\n",
      "5878 [D loss: (0.563)(R 0.612, F 0.513)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.214] [G acc: 0.219]\n",
      "5879 [D loss: (0.626)(R 0.573, F 0.680)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.133] [G acc: 0.156]\n",
      "5880 [D loss: (0.500)(R 0.442, F 0.557)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.199] [G acc: 0.188]\n",
      "5881 [D loss: (0.597)(R 0.556, F 0.637)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.197] [G acc: 0.062]\n",
      "5882 [D loss: (0.578)(R 0.583, F 0.573)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.401] [G acc: 0.078]\n",
      "5883 [D loss: (0.555)(R 0.706, F 0.404)] [D acc: (0.711)(0.516, 0.906)] [G loss: 1.251] [G acc: 0.141]\n",
      "5884 [D loss: (0.627)(R 0.630, F 0.623)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.261] [G acc: 0.125]\n",
      "5885 [D loss: (0.592)(R 0.666, F 0.519)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.260] [G acc: 0.094]\n",
      "5886 [D loss: (0.532)(R 0.604, F 0.460)] [D acc: (0.766)(0.609, 0.922)] [G loss: 1.150] [G acc: 0.156]\n",
      "5887 [D loss: (0.587)(R 0.547, F 0.626)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.279] [G acc: 0.094]\n",
      "5888 [D loss: (0.667)(R 0.586, F 0.748)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.246] [G acc: 0.078]\n",
      "5889 [D loss: (0.541)(R 0.589, F 0.493)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.262] [G acc: 0.062]\n",
      "5890 [D loss: (0.624)(R 0.608, F 0.639)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.286] [G acc: 0.109]\n",
      "5891 [D loss: (0.552)(R 0.570, F 0.534)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.202] [G acc: 0.078]\n",
      "5892 [D loss: (0.512)(R 0.526, F 0.497)] [D acc: (0.781)(0.688, 0.875)] [G loss: 1.186] [G acc: 0.219]\n",
      "5893 [D loss: (0.619)(R 0.504, F 0.734)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.278] [G acc: 0.078]\n",
      "5894 [D loss: (0.546)(R 0.645, F 0.446)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.381] [G acc: 0.141]\n",
      "5895 [D loss: (0.645)(R 0.664, F 0.625)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.216] [G acc: 0.188]\n",
      "5896 [D loss: (0.562)(R 0.488, F 0.635)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.321] [G acc: 0.062]\n",
      "5897 [D loss: (0.542)(R 0.560, F 0.525)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.255] [G acc: 0.047]\n",
      "5898 [D loss: (0.589)(R 0.574, F 0.605)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.171] [G acc: 0.109]\n",
      "5899 [D loss: (0.582)(R 0.596, F 0.567)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.321] [G acc: 0.125]\n",
      "5900 [D loss: (0.577)(R 0.514, F 0.639)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.280] [G acc: 0.078]\n",
      "5901 [D loss: (0.683)(R 0.724, F 0.642)] [D acc: (0.617)(0.516, 0.719)] [G loss: 1.372] [G acc: 0.094]\n",
      "5902 [D loss: (0.614)(R 0.658, F 0.570)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.288] [G acc: 0.125]\n",
      "5903 [D loss: (0.586)(R 0.615, F 0.556)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.141] [G acc: 0.219]\n",
      "5904 [D loss: (0.574)(R 0.583, F 0.566)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.189] [G acc: 0.109]\n",
      "5905 [D loss: (0.596)(R 0.651, F 0.541)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.134] [G acc: 0.156]\n",
      "5906 [D loss: (0.594)(R 0.586, F 0.602)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.218] [G acc: 0.172]\n",
      "5907 [D loss: (0.598)(R 0.566, F 0.631)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.228] [G acc: 0.094]\n",
      "5908 [D loss: (0.571)(R 0.561, F 0.582)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.270] [G acc: 0.203]\n",
      "5909 [D loss: (0.595)(R 0.645, F 0.544)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.341] [G acc: 0.125]\n",
      "5910 [D loss: (0.590)(R 0.583, F 0.597)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.215] [G acc: 0.094]\n",
      "5911 [D loss: (0.600)(R 0.634, F 0.565)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.298] [G acc: 0.109]\n",
      "5912 [D loss: (0.542)(R 0.608, F 0.476)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.235] [G acc: 0.094]\n",
      "5913 [D loss: (0.620)(R 0.677, F 0.562)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.132] [G acc: 0.094]\n",
      "5914 [D loss: (0.616)(R 0.665, F 0.566)] [D acc: (0.672)(0.531, 0.812)] [G loss: 1.214] [G acc: 0.094]\n",
      "5915 [D loss: (0.560)(R 0.543, F 0.577)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.132] [G acc: 0.188]\n",
      "5916 [D loss: (0.581)(R 0.556, F 0.606)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.282] [G acc: 0.141]\n",
      "5917 [D loss: (0.574)(R 0.604, F 0.544)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.284] [G acc: 0.094]\n",
      "5918 [D loss: (0.579)(R 0.625, F 0.533)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.157] [G acc: 0.109]\n",
      "5919 [D loss: (0.694)(R 0.600, F 0.787)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.223] [G acc: 0.094]\n",
      "5920 [D loss: (0.633)(R 0.587, F 0.679)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.266] [G acc: 0.125]\n",
      "5921 [D loss: (0.593)(R 0.681, F 0.504)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.307] [G acc: 0.078]\n",
      "5922 [D loss: (0.627)(R 0.613, F 0.641)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.143] [G acc: 0.125]\n",
      "5923 [D loss: (0.595)(R 0.638, F 0.553)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.281] [G acc: 0.094]\n",
      "5924 [D loss: (0.642)(R 0.631, F 0.652)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.132] [G acc: 0.125]\n",
      "5925 [D loss: (0.612)(R 0.587, F 0.637)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.223] [G acc: 0.062]\n",
      "5926 [D loss: (0.580)(R 0.612, F 0.548)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.076] [G acc: 0.156]\n",
      "5927 [D loss: (0.606)(R 0.544, F 0.668)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.196] [G acc: 0.125]\n",
      "5928 [D loss: (0.668)(R 0.853, F 0.483)] [D acc: (0.633)(0.422, 0.844)] [G loss: 1.260] [G acc: 0.078]\n",
      "5929 [D loss: (0.612)(R 0.591, F 0.634)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.175] [G acc: 0.109]\n",
      "5930 [D loss: (0.638)(R 0.675, F 0.601)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.179] [G acc: 0.109]\n",
      "5931 [D loss: (0.721)(R 0.598, F 0.844)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.160] [G acc: 0.109]\n",
      "5932 [D loss: (0.655)(R 0.778, F 0.532)] [D acc: (0.609)(0.406, 0.812)] [G loss: 1.056] [G acc: 0.125]\n",
      "5933 [D loss: (0.592)(R 0.648, F 0.537)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.070] [G acc: 0.188]\n",
      "5934 [D loss: (0.541)(R 0.561, F 0.520)] [D acc: (0.758)(0.625, 0.891)] [G loss: 1.170] [G acc: 0.109]\n",
      "5935 [D loss: (0.556)(R 0.553, F 0.558)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.251] [G acc: 0.109]\n",
      "5936 [D loss: (0.575)(R 0.648, F 0.501)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.251] [G acc: 0.109]\n",
      "5937 [D loss: (0.580)(R 0.571, F 0.589)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.096] [G acc: 0.125]\n",
      "5938 [D loss: (0.532)(R 0.521, F 0.543)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.245] [G acc: 0.156]\n",
      "5939 [D loss: (0.581)(R 0.606, F 0.556)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.355] [G acc: 0.062]\n",
      "5940 [D loss: (0.578)(R 0.637, F 0.519)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.183] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5941 [D loss: (0.601)(R 0.578, F 0.625)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.293] [G acc: 0.109]\n",
      "5942 [D loss: (0.628)(R 0.645, F 0.611)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.222] [G acc: 0.125]\n",
      "5943 [D loss: (0.558)(R 0.491, F 0.625)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.210] [G acc: 0.047]\n",
      "5944 [D loss: (0.626)(R 0.450, F 0.801)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.193] [G acc: 0.094]\n",
      "5945 [D loss: (0.613)(R 0.699, F 0.527)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.224] [G acc: 0.062]\n",
      "5946 [D loss: (0.566)(R 0.585, F 0.546)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.304] [G acc: 0.031]\n",
      "5947 [D loss: (0.544)(R 0.573, F 0.515)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.262] [G acc: 0.078]\n",
      "5948 [D loss: (0.539)(R 0.482, F 0.597)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.273] [G acc: 0.109]\n",
      "5949 [D loss: (0.599)(R 0.519, F 0.679)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.224] [G acc: 0.109]\n",
      "5950 [D loss: (0.542)(R 0.490, F 0.594)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.305] [G acc: 0.141]\n",
      "5951 [D loss: (0.604)(R 0.623, F 0.585)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.134] [G acc: 0.109]\n",
      "5952 [D loss: (0.561)(R 0.611, F 0.511)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.269] [G acc: 0.062]\n",
      "5953 [D loss: (0.554)(R 0.524, F 0.584)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.140] [G acc: 0.156]\n",
      "5954 [D loss: (0.547)(R 0.554, F 0.539)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.258] [G acc: 0.062]\n",
      "5955 [D loss: (0.619)(R 0.601, F 0.637)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.154] [G acc: 0.141]\n",
      "5956 [D loss: (0.606)(R 0.603, F 0.609)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.175] [G acc: 0.156]\n",
      "5957 [D loss: (0.567)(R 0.598, F 0.536)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.241] [G acc: 0.125]\n",
      "5958 [D loss: (0.551)(R 0.557, F 0.546)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.223] [G acc: 0.109]\n",
      "5959 [D loss: (0.567)(R 0.558, F 0.576)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.281] [G acc: 0.109]\n",
      "5960 [D loss: (0.607)(R 0.625, F 0.590)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.179] [G acc: 0.109]\n",
      "5961 [D loss: (0.566)(R 0.526, F 0.606)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.117] [G acc: 0.141]\n",
      "5962 [D loss: (0.608)(R 0.654, F 0.562)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.152] [G acc: 0.109]\n",
      "5963 [D loss: (0.594)(R 0.653, F 0.535)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.161] [G acc: 0.078]\n",
      "5964 [D loss: (0.531)(R 0.539, F 0.522)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.183] [G acc: 0.125]\n",
      "5965 [D loss: (0.531)(R 0.523, F 0.540)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.152] [G acc: 0.141]\n",
      "5966 [D loss: (0.602)(R 0.666, F 0.539)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.082] [G acc: 0.234]\n",
      "5967 [D loss: (0.566)(R 0.454, F 0.678)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.209] [G acc: 0.109]\n",
      "5968 [D loss: (0.493)(R 0.492, F 0.495)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.324] [G acc: 0.109]\n",
      "5969 [D loss: (0.516)(R 0.464, F 0.567)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.250] [G acc: 0.078]\n",
      "5970 [D loss: (0.523)(R 0.546, F 0.501)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.214] [G acc: 0.188]\n",
      "5971 [D loss: (0.548)(R 0.596, F 0.500)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.372] [G acc: 0.078]\n",
      "5972 [D loss: (0.517)(R 0.506, F 0.528)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.389] [G acc: 0.078]\n",
      "5973 [D loss: (0.577)(R 0.556, F 0.598)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.165] [G acc: 0.156]\n",
      "5974 [D loss: (0.634)(R 0.614, F 0.653)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.348] [G acc: 0.047]\n",
      "5975 [D loss: (0.700)(R 0.719, F 0.680)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.316] [G acc: 0.047]\n",
      "5976 [D loss: (0.553)(R 0.657, F 0.448)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.271] [G acc: 0.078]\n",
      "5977 [D loss: (0.498)(R 0.457, F 0.539)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.344] [G acc: 0.141]\n",
      "5978 [D loss: (0.582)(R 0.588, F 0.576)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.168] [G acc: 0.141]\n",
      "5979 [D loss: (0.548)(R 0.502, F 0.593)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.218] [G acc: 0.125]\n",
      "5980 [D loss: (0.552)(R 0.525, F 0.580)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.301] [G acc: 0.125]\n",
      "5981 [D loss: (0.643)(R 0.654, F 0.633)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.228] [G acc: 0.203]\n",
      "5982 [D loss: (0.590)(R 0.511, F 0.669)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.340] [G acc: 0.109]\n",
      "5983 [D loss: (0.591)(R 0.719, F 0.464)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.279] [G acc: 0.078]\n",
      "5984 [D loss: (0.589)(R 0.632, F 0.546)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.212] [G acc: 0.047]\n",
      "5985 [D loss: (0.600)(R 0.632, F 0.567)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.290] [G acc: 0.125]\n",
      "5986 [D loss: (0.580)(R 0.575, F 0.585)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.347] [G acc: 0.031]\n",
      "5987 [D loss: (0.613)(R 0.631, F 0.596)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.101] [G acc: 0.188]\n",
      "5988 [D loss: (0.521)(R 0.479, F 0.563)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.297] [G acc: 0.109]\n",
      "5989 [D loss: (0.650)(R 0.695, F 0.605)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.267] [G acc: 0.125]\n",
      "5990 [D loss: (0.532)(R 0.568, F 0.497)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.182] [G acc: 0.156]\n",
      "5991 [D loss: (0.540)(R 0.597, F 0.483)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.251] [G acc: 0.094]\n",
      "5992 [D loss: (0.620)(R 0.547, F 0.694)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.154] [G acc: 0.141]\n",
      "5993 [D loss: (0.593)(R 0.628, F 0.557)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.175] [G acc: 0.141]\n",
      "5994 [D loss: (0.599)(R 0.627, F 0.570)] [D acc: (0.625)(0.484, 0.766)] [G loss: 1.119] [G acc: 0.156]\n",
      "5995 [D loss: (0.568)(R 0.564, F 0.573)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.243] [G acc: 0.141]\n",
      "5996 [D loss: (0.633)(R 0.617, F 0.649)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.271] [G acc: 0.078]\n",
      "5997 [D loss: (0.584)(R 0.707, F 0.460)] [D acc: (0.703)(0.516, 0.891)] [G loss: 1.139] [G acc: 0.109]\n",
      "5998 [D loss: (0.616)(R 0.662, F 0.570)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.159] [G acc: 0.125]\n",
      "5999 [D loss: (0.584)(R 0.573, F 0.596)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.168] [G acc: 0.078]\n"
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
